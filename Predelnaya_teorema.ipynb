{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## выбираем для эксперимента гамма распределение, сохраняем его теоретическое среднее и дисперсию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theoretical mean = 2.0 ; theoretical var = 2.0\n"
     ]
    }
   ],
   "source": [
    "dist = sts.gamma(2)\n",
    "mean = dist.stats()[0]\n",
    "var = dist.stats()[1]\n",
    "print('theoretical mean =', mean, '; theoretical var =', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сгенерируем из него выборку объёма 1000, построим гистограмму выборки и нарисуем поверх неё теоретическую плотность распределения случайной величины "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$x$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# генерация вборки\n",
    "sample = dist.rvs(1000)\n",
    "# гистограмма\n",
    "plt.hist(sample, density=True)\n",
    "# график теоретической плотности\n",
    "x = np.linspace(-2,15,1000)\n",
    "pdf = dist.pdf(x)\n",
    "plt.plot(x, pdf, label='theoretical pdf', alpha=1, linewidth=4)\n",
    "plt.legend()\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## сгенерируем 1000 выборок объёма n (5, 10, 50) и построим гистограммы распределений их выборочных средних, поверх каждой гистограммы нарисуем плотность соответствующего нормального распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$x$')"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# генерация выборок, построение гистограммы выборочных средних\n",
    "plt.hist([dist.rvs(5).mean() for i in range(1000)], density=True)\n",
    "# график теоретической плотности нормального распределения\n",
    "x = np.linspace(-1,6,1000)\n",
    "norm_dist = sts.norm(mean, (var/5) ** (1/2)) # подсчет стандартного отклонения (var/5) ** (1/2)\n",
    "pdf = norm_dist.pdf(x)\n",
    "plt.plot(x, pdf, label='norm_dist 5 samples pdf', alpha=1, linewidth=4)\n",
    "plt.legend()\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$x$')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([dist.rvs(10).mean() for i in range(1000)], density=True)\n",
    "x = np.linspace(0,5,1000)\n",
    "norm_dist = sts.norm(mean, (var/10) ** (1/2)) \n",
    "pdf = norm_dist.pdf(x)\n",
    "plt.plot(x, pdf, label='norm_dist 10 samples pdf', alpha=1, linewidth=4)\n",
    "plt.legend()\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$x$')"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([dist.rvs(50).mean() for i in range(1000)], density=True)\n",
    "x = np.linspace(1,4,1000)\n",
    "norm_dist = sts.norm(mean, (var/50) ** (1/2))\n",
    "pdf = norm_dist.pdf(x)\n",
    "plt.plot(x, pdf, label='norm_dist 50 samples pdf', alpha=1, linewidth=4)\n",
    "plt.legend()\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чем больше количество образцов в выборке n, тем больше распределение выборочных средних соответствует нормальному, тем меньше стандартное отклонение распределения выборочных средних, точность аппроксимации повышается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.stats in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.stats - .. _statsrefmanual:\n",
      "\n",
      "DESCRIPTION\n",
      "    ==========================================\n",
      "    Statistical functions (:mod:`scipy.stats`)\n",
      "    ==========================================\n",
      "    \n",
      "    .. currentmodule:: scipy.stats\n",
      "    \n",
      "    This module contains a large number of probability distributions as\n",
      "    well as a growing library of statistical functions.\n",
      "    \n",
      "    Each univariate distribution is an instance of a subclass of `rv_continuous`\n",
      "    (`rv_discrete` for discrete distributions):\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rv_continuous\n",
      "       rv_discrete\n",
      "       rv_histogram\n",
      "    \n",
      "    Continuous distributions\n",
      "    ========================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       alpha             -- Alpha\n",
      "       anglit            -- Anglit\n",
      "       arcsine           -- Arcsine\n",
      "       argus             -- Argus\n",
      "       beta              -- Beta\n",
      "       betaprime         -- Beta Prime\n",
      "       bradford          -- Bradford\n",
      "       burr              -- Burr (Type III)\n",
      "       burr12            -- Burr (Type XII)\n",
      "       cauchy            -- Cauchy\n",
      "       chi               -- Chi\n",
      "       chi2              -- Chi-squared\n",
      "       cosine            -- Cosine\n",
      "       crystalball       -- Crystalball\n",
      "       dgamma            -- Double Gamma\n",
      "       dweibull          -- Double Weibull\n",
      "       erlang            -- Erlang\n",
      "       expon             -- Exponential\n",
      "       exponnorm         -- Exponentially Modified Normal\n",
      "       exponweib         -- Exponentiated Weibull\n",
      "       exponpow          -- Exponential Power\n",
      "       f                 -- F (Snecdor F)\n",
      "       fatiguelife       -- Fatigue Life (Birnbaum-Saunders)\n",
      "       fisk              -- Fisk\n",
      "       foldcauchy        -- Folded Cauchy\n",
      "       foldnorm          -- Folded Normal\n",
      "       frechet_r         -- Deprecated. Alias for weibull_min\n",
      "       frechet_l         -- Deprecated. Alias for weibull_max\n",
      "       genlogistic       -- Generalized Logistic\n",
      "       gennorm           -- Generalized normal\n",
      "       genpareto         -- Generalized Pareto\n",
      "       genexpon          -- Generalized Exponential\n",
      "       genextreme        -- Generalized Extreme Value\n",
      "       gausshyper        -- Gauss Hypergeometric\n",
      "       gamma             -- Gamma\n",
      "       gengamma          -- Generalized gamma\n",
      "       genhalflogistic   -- Generalized Half Logistic\n",
      "       geninvgauss       -- Generalized Inverse Gaussian\n",
      "       gilbrat           -- Gilbrat\n",
      "       gompertz          -- Gompertz (Truncated Gumbel)\n",
      "       gumbel_r          -- Right Sided Gumbel, Log-Weibull, Fisher-Tippett, Extreme Value Type I\n",
      "       gumbel_l          -- Left Sided Gumbel, etc.\n",
      "       halfcauchy        -- Half Cauchy\n",
      "       halflogistic      -- Half Logistic\n",
      "       halfnorm          -- Half Normal\n",
      "       halfgennorm       -- Generalized Half Normal\n",
      "       hypsecant         -- Hyperbolic Secant\n",
      "       invgamma          -- Inverse Gamma\n",
      "       invgauss          -- Inverse Gaussian\n",
      "       invweibull        -- Inverse Weibull\n",
      "       johnsonsb         -- Johnson SB\n",
      "       johnsonsu         -- Johnson SU\n",
      "       kappa4            -- Kappa 4 parameter\n",
      "       kappa3            -- Kappa 3 parameter\n",
      "       ksone             -- Distribution of Kolmogorov-Smirnov one-sided test statistic\n",
      "       kstwo             -- Distribution of Kolmogorov-Smirnov two-sided test statistic\n",
      "       kstwobign         -- Limiting Distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n",
      "       laplace           -- Laplace\n",
      "       levy              -- Levy\n",
      "       levy_l\n",
      "       levy_stable\n",
      "       logistic          -- Logistic\n",
      "       loggamma          -- Log-Gamma\n",
      "       loglaplace        -- Log-Laplace (Log Double Exponential)\n",
      "       lognorm           -- Log-Normal\n",
      "       loguniform        -- Log-Uniform\n",
      "       lomax             -- Lomax (Pareto of the second kind)\n",
      "       maxwell           -- Maxwell\n",
      "       mielke            -- Mielke's Beta-Kappa\n",
      "       moyal             -- Moyal\n",
      "       nakagami          -- Nakagami\n",
      "       ncx2              -- Non-central chi-squared\n",
      "       ncf               -- Non-central F\n",
      "       nct               -- Non-central Student's T\n",
      "       norm              -- Normal (Gaussian)\n",
      "       norminvgauss      -- Normal Inverse Gaussian\n",
      "       pareto            -- Pareto\n",
      "       pearson3          -- Pearson type III\n",
      "       powerlaw          -- Power-function\n",
      "       powerlognorm      -- Power log normal\n",
      "       powernorm         -- Power normal\n",
      "       rdist             -- R-distribution\n",
      "       rayleigh          -- Rayleigh\n",
      "       rice              -- Rice\n",
      "       recipinvgauss     -- Reciprocal Inverse Gaussian\n",
      "       semicircular      -- Semicircular\n",
      "       skewnorm          -- Skew normal\n",
      "       t                 -- Student's T\n",
      "       trapz             -- Trapezoidal\n",
      "       triang            -- Triangular\n",
      "       truncexpon        -- Truncated Exponential\n",
      "       truncnorm         -- Truncated Normal\n",
      "       tukeylambda       -- Tukey-Lambda\n",
      "       uniform           -- Uniform\n",
      "       vonmises          -- Von-Mises (Circular)\n",
      "       vonmises_line     -- Von-Mises (Line)\n",
      "       wald              -- Wald\n",
      "       weibull_min       -- Minimum Weibull (see Frechet)\n",
      "       weibull_max       -- Maximum Weibull (see Frechet)\n",
      "       wrapcauchy        -- Wrapped Cauchy\n",
      "    \n",
      "    Multivariate distributions\n",
      "    ==========================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       multivariate_normal   -- Multivariate normal distribution\n",
      "       matrix_normal         -- Matrix normal distribution\n",
      "       dirichlet             -- Dirichlet\n",
      "       wishart               -- Wishart\n",
      "       invwishart            -- Inverse Wishart\n",
      "       multinomial           -- Multinomial distribution\n",
      "       special_ortho_group   -- SO(N) group\n",
      "       ortho_group           -- O(N) group\n",
      "       unitary_group         -- U(N) group\n",
      "       random_correlation    -- random correlation matrices\n",
      "    \n",
      "    Discrete distributions\n",
      "    ======================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bernoulli         -- Bernoulli\n",
      "       betabinom         -- Beta-Binomial\n",
      "       binom             -- Binomial\n",
      "       boltzmann         -- Boltzmann (Truncated Discrete Exponential)\n",
      "       dlaplace          -- Discrete Laplacian\n",
      "       geom              -- Geometric\n",
      "       hypergeom         -- Hypergeometric\n",
      "       logser            -- Logarithmic (Log-Series, Series)\n",
      "       nbinom            -- Negative Binomial\n",
      "       planck            -- Planck (Discrete Exponential)\n",
      "       poisson           -- Poisson\n",
      "       randint           -- Discrete Uniform\n",
      "       skellam           -- Skellam\n",
      "       zipf              -- Zipf\n",
      "       yulesimon         -- Yule-Simon\n",
      "    \n",
      "    An overview of statistical functions is given below.\n",
      "    Several of these functions have a similar version in\n",
      "    `scipy.stats.mstats` which work for masked arrays.\n",
      "    \n",
      "    Summary statistics\n",
      "    ==================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       describe          -- Descriptive statistics\n",
      "       gmean             -- Geometric mean\n",
      "       hmean             -- Harmonic mean\n",
      "       kurtosis          -- Fisher or Pearson kurtosis\n",
      "       mode              -- Modal value\n",
      "       moment            -- Central moment\n",
      "       skew              -- Skewness\n",
      "       kstat             --\n",
      "       kstatvar          --\n",
      "       tmean             -- Truncated arithmetic mean\n",
      "       tvar              -- Truncated variance\n",
      "       tmin              --\n",
      "       tmax              --\n",
      "       tstd              --\n",
      "       tsem              --\n",
      "       variation         -- Coefficient of variation\n",
      "       find_repeats\n",
      "       trim_mean\n",
      "       gstd              -- Geometric Standard Deviation\n",
      "       iqr\n",
      "       sem\n",
      "       bayes_mvs\n",
      "       mvsdist\n",
      "       entropy\n",
      "       median_absolute_deviation\n",
      "       median_abs_deviation\n",
      "    \n",
      "    Frequency statistics\n",
      "    ====================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       cumfreq\n",
      "       itemfreq\n",
      "       percentileofscore\n",
      "       scoreatpercentile\n",
      "       relfreq\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       binned_statistic     -- Compute a binned statistic for a set of data.\n",
      "       binned_statistic_2d  -- Compute a 2-D binned statistic for a set of data.\n",
      "       binned_statistic_dd  -- Compute a d-D binned statistic for a set of data.\n",
      "    \n",
      "    Correlation functions\n",
      "    =====================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       f_oneway\n",
      "       pearsonr\n",
      "       spearmanr\n",
      "       pointbiserialr\n",
      "       kendalltau\n",
      "       weightedtau\n",
      "       linregress\n",
      "       siegelslopes\n",
      "       theilslopes\n",
      "       multiscale_graphcorr\n",
      "    \n",
      "    Statistical tests\n",
      "    =================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ttest_1samp\n",
      "       ttest_ind\n",
      "       ttest_ind_from_stats\n",
      "       ttest_rel\n",
      "       chisquare\n",
      "       power_divergence\n",
      "       kstest\n",
      "       ks_1samp\n",
      "       ks_2samp\n",
      "       epps_singleton_2samp\n",
      "       mannwhitneyu\n",
      "       tiecorrect\n",
      "       rankdata\n",
      "       ranksums\n",
      "       wilcoxon\n",
      "       kruskal\n",
      "       friedmanchisquare\n",
      "       brunnermunzel\n",
      "       combine_pvalues\n",
      "       jarque_bera\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ansari\n",
      "       bartlett\n",
      "       levene\n",
      "       shapiro\n",
      "       anderson\n",
      "       anderson_ksamp\n",
      "       binom_test\n",
      "       fligner\n",
      "       median_test\n",
      "       mood\n",
      "       skewtest\n",
      "       kurtosistest\n",
      "       normaltest\n",
      "    \n",
      "    Transformations\n",
      "    ===============\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       boxcox\n",
      "       boxcox_normmax\n",
      "       boxcox_llf\n",
      "       yeojohnson\n",
      "       yeojohnson_normmax\n",
      "       yeojohnson_llf\n",
      "       obrientransform\n",
      "       sigmaclip\n",
      "       trimboth\n",
      "       trim1\n",
      "       zmap\n",
      "       zscore\n",
      "    \n",
      "    Statistical distances\n",
      "    =====================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       wasserstein_distance\n",
      "       energy_distance\n",
      "    \n",
      "    Random variate generation\n",
      "    =========================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rvs_ratio_uniforms\n",
      "    \n",
      "    Circular statistical functions\n",
      "    ==============================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       circmean\n",
      "       circvar\n",
      "       circstd\n",
      "    \n",
      "    Contingency table functions\n",
      "    ===========================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       chi2_contingency\n",
      "       contingency.expected_freq\n",
      "       contingency.margins\n",
      "       fisher_exact\n",
      "    \n",
      "    Plot-tests\n",
      "    ==========\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ppcc_max\n",
      "       ppcc_plot\n",
      "       probplot\n",
      "       boxcox_normplot\n",
      "       yeojohnson_normplot\n",
      "    \n",
      "    \n",
      "    Masked statistics functions\n",
      "    ===========================\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       stats.mstats\n",
      "    \n",
      "    \n",
      "    Univariate and multivariate kernel density estimation\n",
      "    =====================================================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       gaussian_kde\n",
      "    \n",
      "    Warnings used in :mod:`scipy.stats`\n",
      "    ===================================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       F_onewayConstantInputWarning\n",
      "       F_onewayBadInputSizesWarning\n",
      "       PearsonRConstantInputWarning\n",
      "       PearsonRNearConstantInputWarning\n",
      "       SpearmanRConstantInputWarning\n",
      "    \n",
      "    For many more stat related functions install the software R and the\n",
      "    interface package rpy.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _binned_statistic\n",
      "    _constants\n",
      "    _continuous_distns\n",
      "    _discrete_distns\n",
      "    _distn_infrastructure\n",
      "    _distr_params\n",
      "    _hypotests\n",
      "    _ksstats\n",
      "    _multivariate\n",
      "    _rvs_sampling\n",
      "    _stats\n",
      "    _stats_mstats_common\n",
      "    _tukeylambda_stats\n",
      "    _wilcoxon_data\n",
      "    contingency\n",
      "    distributions\n",
      "    kde\n",
      "    morestats\n",
      "    mstats\n",
      "    mstats_basic\n",
      "    mstats_extras\n",
      "    mvn\n",
      "    setup\n",
      "    statlib\n",
      "    stats\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.RuntimeWarning(builtins.Warning)\n",
      "        scipy.stats.stats.F_onewayBadInputSizesWarning\n",
      "        scipy.stats.stats.F_onewayConstantInputWarning\n",
      "        scipy.stats.stats.PearsonRConstantInputWarning\n",
      "        scipy.stats.stats.PearsonRNearConstantInputWarning\n",
      "        scipy.stats.stats.SpearmanRConstantInputWarning\n",
      "    builtins.object\n",
      "        scipy.stats.kde.gaussian_kde\n",
      "    scipy.stats._distn_infrastructure.rv_generic(builtins.object)\n",
      "        scipy.stats._distn_infrastructure.rv_continuous\n",
      "            scipy.stats._continuous_distns.rv_histogram\n",
      "        scipy.stats._distn_infrastructure.rv_discrete\n",
      "    \n",
      "    class F_onewayBadInputSizesWarning(builtins.RuntimeWarning)\n",
      "     |  Warning generated by `f_oneway` when an input has length 0,\n",
      "     |  or if all the inputs have length 1.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      F_onewayBadInputSizesWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class F_onewayConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  F_onewayConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `f_oneway` when an input is constant, e.g.\n",
      "     |  each of the samples provided is a constant array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      F_onewayConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class PearsonRConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  PearsonRConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `pearsonr` when an input is constant.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PearsonRConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class PearsonRNearConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  PearsonRNearConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `pearsonr` when an input is nearly constant.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PearsonRNearConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class SpearmanRConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  SpearmanRConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `spearmanr` when an input is constant.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpearmanRConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class gaussian_kde(builtins.object)\n",
      "     |  gaussian_kde(dataset, bw_method=None, weights=None)\n",
      "     |  \n",
      "     |  Representation of a kernel-density estimate using Gaussian kernels.\n",
      "     |  \n",
      "     |  Kernel density estimation is a way to estimate the probability density\n",
      "     |  function (PDF) of a random variable in a non-parametric way.\n",
      "     |  `gaussian_kde` works for both uni-variate and multi-variate data.   It\n",
      "     |  includes automatic bandwidth determination.  The estimation works best for\n",
      "     |  a unimodal distribution; bimodal or multi-modal distributions tend to be\n",
      "     |  oversmoothed.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  dataset : array_like\n",
      "     |      Datapoints to estimate from. In case of univariate data this is a 1-D\n",
      "     |      array, otherwise a 2-D array with shape (# of dims, # of data).\n",
      "     |  bw_method : str, scalar or callable, optional\n",
      "     |      The method used to calculate the estimator bandwidth.  This can be\n",
      "     |      'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n",
      "     |      this will be used directly as `kde.factor`.  If a callable, it should\n",
      "     |      take a `gaussian_kde` instance as only parameter and return a scalar.\n",
      "     |      If None (default), 'scott' is used.  See Notes for more details.\n",
      "     |  weights : array_like, optional\n",
      "     |      weights of datapoints. This must be the same shape as dataset.\n",
      "     |      If None (default), the samples are assumed to be equally weighted\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  dataset : ndarray\n",
      "     |      The dataset with which `gaussian_kde` was initialized.\n",
      "     |  d : int\n",
      "     |      Number of dimensions.\n",
      "     |  n : int\n",
      "     |      Number of datapoints.\n",
      "     |  neff : int\n",
      "     |      Effective number of datapoints.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |  factor : float\n",
      "     |      The bandwidth factor, obtained from `kde.covariance_factor`, with which\n",
      "     |      the covariance matrix is multiplied.\n",
      "     |  covariance : ndarray\n",
      "     |      The covariance matrix of `dataset`, scaled by the calculated bandwidth\n",
      "     |      (`kde.factor`).\n",
      "     |  inv_cov : ndarray\n",
      "     |      The inverse of `covariance`.\n",
      "     |  \n",
      "     |  Methods\n",
      "     |  -------\n",
      "     |  evaluate\n",
      "     |  __call__\n",
      "     |  integrate_gaussian\n",
      "     |  integrate_box_1d\n",
      "     |  integrate_box\n",
      "     |  integrate_kde\n",
      "     |  pdf\n",
      "     |  logpdf\n",
      "     |  resample\n",
      "     |  set_bandwidth\n",
      "     |  covariance_factor\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Bandwidth selection strongly influences the estimate obtained from the KDE\n",
      "     |  (much more so than the actual shape of the kernel).  Bandwidth selection\n",
      "     |  can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n",
      "     |  methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n",
      "     |  uses a rule of thumb, the default is Scott's Rule.\n",
      "     |  \n",
      "     |  Scott's Rule [1]_, implemented as `scotts_factor`, is::\n",
      "     |  \n",
      "     |      n**(-1./(d+4)),\n",
      "     |  \n",
      "     |  with ``n`` the number of data points and ``d`` the number of dimensions.\n",
      "     |  In the case of unequally weighted points, `scotts_factor` becomes::\n",
      "     |  \n",
      "     |      neff**(-1./(d+4)),\n",
      "     |  \n",
      "     |  with ``neff`` the effective number of datapoints.\n",
      "     |  Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n",
      "     |  \n",
      "     |      (n * (d + 2) / 4.)**(-1. / (d + 4)).\n",
      "     |  \n",
      "     |  or in the case of unequally weighted points::\n",
      "     |  \n",
      "     |      (neff * (d + 2) / 4.)**(-1. / (d + 4)).\n",
      "     |  \n",
      "     |  Good general descriptions of kernel density estimation can be found in [1]_\n",
      "     |  and [2]_, the mathematics for this multi-dimensional implementation can be\n",
      "     |  found in [1]_.\n",
      "     |  \n",
      "     |  With a set of weighted samples, the effective number of datapoints ``neff``\n",
      "     |  is defined by::\n",
      "     |  \n",
      "     |      neff = sum(weights)^2 / sum(weights^2)\n",
      "     |  \n",
      "     |  as detailed in [5]_.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n",
      "     |         Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n",
      "     |  .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n",
      "     |         Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n",
      "     |         Chapman and Hall, London, 1986.\n",
      "     |  .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n",
      "     |         Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n",
      "     |  .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n",
      "     |         conditional density estimation\", Computational Statistics & Data\n",
      "     |         Analysis, Vol. 36, pp. 279-298, 2001.\n",
      "     |  .. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.\n",
      "     |         Series A (General), 132, 272\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Generate some random two-dimensional data:\n",
      "     |  \n",
      "     |  >>> from scipy import stats\n",
      "     |  >>> def measure(n):\n",
      "     |  ...     \"Measurement model, return two coupled measurements.\"\n",
      "     |  ...     m1 = np.random.normal(size=n)\n",
      "     |  ...     m2 = np.random.normal(scale=0.5, size=n)\n",
      "     |  ...     return m1+m2, m1-m2\n",
      "     |  \n",
      "     |  >>> m1, m2 = measure(2000)\n",
      "     |  >>> xmin = m1.min()\n",
      "     |  >>> xmax = m1.max()\n",
      "     |  >>> ymin = m2.min()\n",
      "     |  >>> ymax = m2.max()\n",
      "     |  \n",
      "     |  Perform a kernel density estimate on the data:\n",
      "     |  \n",
      "     |  >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
      "     |  >>> positions = np.vstack([X.ravel(), Y.ravel()])\n",
      "     |  >>> values = np.vstack([m1, m2])\n",
      "     |  >>> kernel = stats.gaussian_kde(values)\n",
      "     |  >>> Z = np.reshape(kernel(positions).T, X.shape)\n",
      "     |  \n",
      "     |  Plot the results:\n",
      "     |  \n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> fig, ax = plt.subplots()\n",
      "     |  >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
      "     |  ...           extent=[xmin, xmax, ymin, ymax])\n",
      "     |  >>> ax.plot(m1, m2, 'k.', markersize=2)\n",
      "     |  >>> ax.set_xlim([xmin, xmax])\n",
      "     |  >>> ax.set_ylim([ymin, ymax])\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__ = evaluate(self, points)\n",
      "     |  \n",
      "     |  __init__(self, dataset, bw_method=None, weights=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  covariance_factor = scotts_factor(self)\n",
      "     |  \n",
      "     |  evaluate(self, points)\n",
      "     |      Evaluate the estimated pdf on a set of points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      points : (# of dimensions, # of points)-array\n",
      "     |          Alternatively, a (# of dimensions,) vector can be passed in and\n",
      "     |          treated as a single point.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      values : (# of points,)-array\n",
      "     |          The values at each point.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError : if the dimensionality of the input points is different than\n",
      "     |                   the dimensionality of the KDE.\n",
      "     |  \n",
      "     |  integrate_box(self, low_bounds, high_bounds, maxpts=None)\n",
      "     |      Computes the integral of a pdf over a rectangular interval.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      low_bounds : array_like\n",
      "     |          A 1-D array containing the lower bounds of integration.\n",
      "     |      high_bounds : array_like\n",
      "     |          A 1-D array containing the upper bounds of integration.\n",
      "     |      maxpts : int, optional\n",
      "     |          The maximum number of points to use for integration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : scalar\n",
      "     |          The result of the integral.\n",
      "     |  \n",
      "     |  integrate_box_1d(self, low, high)\n",
      "     |      Computes the integral of a 1D pdf between two bounds.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      low : scalar\n",
      "     |          Lower bound of integration.\n",
      "     |      high : scalar\n",
      "     |          Upper bound of integration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : scalar\n",
      "     |          The result of the integral.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the KDE is over more than one dimension.\n",
      "     |  \n",
      "     |  integrate_gaussian(self, mean, cov)\n",
      "     |      Multiply estimated density by a multivariate Gaussian and integrate\n",
      "     |      over the whole space.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      mean : aray_like\n",
      "     |          A 1-D array, specifying the mean of the Gaussian.\n",
      "     |      cov : array_like\n",
      "     |          A 2-D array, specifying the covariance matrix of the Gaussian.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : scalar\n",
      "     |          The value of the integral.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the mean or covariance of the input Gaussian differs from\n",
      "     |          the KDE's dimensionality.\n",
      "     |  \n",
      "     |  integrate_kde(self, other)\n",
      "     |      Computes the integral of the product of this  kernel density estimate\n",
      "     |      with another.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : gaussian_kde instance\n",
      "     |          The other kde.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : scalar\n",
      "     |          The result of the integral.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the KDEs have different dimensionality.\n",
      "     |  \n",
      "     |  logpdf(self, x)\n",
      "     |      Evaluate the log of the estimated pdf on a provided set of points.\n",
      "     |  \n",
      "     |  pdf(self, x)\n",
      "     |      Evaluate the estimated pdf on a provided set of points.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is an alias for `gaussian_kde.evaluate`.  See the ``evaluate``\n",
      "     |      docstring for more details.\n",
      "     |  \n",
      "     |  resample(self, size=None, seed=None)\n",
      "     |      Randomly sample a dataset from the estimated pdf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      size : int, optional\n",
      "     |          The number of samples to draw.  If not provided, then the size is\n",
      "     |          the same as the effective number of samples in the underlying\n",
      "     |          dataset.\n",
      "     |      seed : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "     |          This parameter defines the object to use for drawing random\n",
      "     |          variates.\n",
      "     |          If `seed` is `None` the `~np.random.RandomState` singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "     |          with seed.\n",
      "     |          If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
      "     |          then that object is used.\n",
      "     |          Default is None.\n",
      "     |          Specify `seed` for reproducible drawing of random variates.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      resample : (self.d, `size`) ndarray\n",
      "     |          The sampled dataset.\n",
      "     |  \n",
      "     |  scotts_factor(self)\n",
      "     |      Computes the coefficient (`kde.factor`) that\n",
      "     |      multiplies the data covariance matrix to obtain the kernel covariance\n",
      "     |      matrix. The default is `scotts_factor`.  A subclass can overwrite this\n",
      "     |      method to provide a different method, or set it through a call to\n",
      "     |      `kde.set_bandwidth`.\n",
      "     |  \n",
      "     |  set_bandwidth(self, bw_method=None)\n",
      "     |      Compute the estimator bandwidth with given method.\n",
      "     |      \n",
      "     |      The new bandwidth calculated after a call to `set_bandwidth` is used\n",
      "     |      for subsequent evaluations of the estimated density.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      bw_method : str, scalar or callable, optional\n",
      "     |          The method used to calculate the estimator bandwidth.  This can be\n",
      "     |          'scott', 'silverman', a scalar constant or a callable.  If a\n",
      "     |          scalar, this will be used directly as `kde.factor`.  If a callable,\n",
      "     |          it should take a `gaussian_kde` instance as only parameter and\n",
      "     |          return a scalar.  If None (default), nothing happens; the current\n",
      "     |          `kde.covariance_factor` method is kept.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      .. versionadded:: 0.11\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import scipy.stats as stats\n",
      "     |      >>> x1 = np.array([-7, -5, 1, 4, 5.])\n",
      "     |      >>> kde = stats.gaussian_kde(x1)\n",
      "     |      >>> xs = np.linspace(-10, 10, num=50)\n",
      "     |      >>> y1 = kde(xs)\n",
      "     |      >>> kde.set_bandwidth(bw_method='silverman')\n",
      "     |      >>> y2 = kde(xs)\n",
      "     |      >>> kde.set_bandwidth(bw_method=kde.factor / 3.)\n",
      "     |      >>> y3 = kde(xs)\n",
      "     |      \n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> fig, ax = plt.subplots()\n",
      "     |      >>> ax.plot(x1, np.full(x1.shape, 1 / (4. * x1.size)), 'bo',\n",
      "     |      ...         label='Data points (rescaled)')\n",
      "     |      >>> ax.plot(xs, y1, label='Scott (default)')\n",
      "     |      >>> ax.plot(xs, y2, label='Silverman')\n",
      "     |      >>> ax.plot(xs, y3, label='Const (1/3 * Silverman)')\n",
      "     |      >>> ax.legend()\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  silverman_factor(self)\n",
      "     |      Compute the Silverman factor.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      s : float\n",
      "     |          The silverman factor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  neff\n",
      "     |  \n",
      "     |  weights\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class rv_continuous(rv_generic)\n",
      "     |  rv_continuous(momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |  \n",
      "     |  A generic continuous random variable class meant for subclassing.\n",
      "     |  \n",
      "     |  `rv_continuous` is a base class to construct specific distribution classes\n",
      "     |  and instances for continuous random variables. It cannot be used\n",
      "     |  directly as a distribution.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  momtype : int, optional\n",
      "     |      The type of generic moment calculation to use: 0 for pdf, 1 (default)\n",
      "     |      for ppf.\n",
      "     |  a : float, optional\n",
      "     |      Lower bound of the support of the distribution, default is minus\n",
      "     |      infinity.\n",
      "     |  b : float, optional\n",
      "     |      Upper bound of the support of the distribution, default is plus\n",
      "     |      infinity.\n",
      "     |  xtol : float, optional\n",
      "     |      The tolerance for fixed point calculation for generic ppf.\n",
      "     |  badvalue : float, optional\n",
      "     |      The value in a result arrays that indicates a value that for which\n",
      "     |      some argument restriction is violated, default is np.nan.\n",
      "     |  name : str, optional\n",
      "     |      The name of the instance. This string is used to construct the default\n",
      "     |      example for distributions.\n",
      "     |  longname : str, optional\n",
      "     |      This string is used as part of the first line of the docstring returned\n",
      "     |      when a subclass has no docstring of its own. Note: `longname` exists\n",
      "     |      for backwards compatibility, do not use for new subclasses.\n",
      "     |  shapes : str, optional\n",
      "     |      The shape of the distribution. For example ``\"m, n\"`` for a\n",
      "     |      distribution that takes two integers as the two shape arguments for all\n",
      "     |      its methods. If not provided, shape parameters will be inferred from\n",
      "     |      the signature of the private methods, ``_pdf`` and ``_cdf`` of the\n",
      "     |      instance.\n",
      "     |  extradoc :  str, optional, deprecated\n",
      "     |      This string is used as the last part of the docstring returned when a\n",
      "     |      subclass has no docstring of its own. Note: `extradoc` exists for\n",
      "     |      backwards compatibility, do not use for new subclasses.\n",
      "     |  seed : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "     |      This parameter defines the object to use for drawing random variates.\n",
      "     |      If `seed` is `None` the `~np.random.RandomState` singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "     |      with seed.\n",
      "     |      If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
      "     |      then that object is used.\n",
      "     |      Default is None.\n",
      "     |  \n",
      "     |  Methods\n",
      "     |  -------\n",
      "     |  rvs\n",
      "     |  pdf\n",
      "     |  logpdf\n",
      "     |  cdf\n",
      "     |  logcdf\n",
      "     |  sf\n",
      "     |  logsf\n",
      "     |  ppf\n",
      "     |  isf\n",
      "     |  moment\n",
      "     |  stats\n",
      "     |  entropy\n",
      "     |  expect\n",
      "     |  median\n",
      "     |  mean\n",
      "     |  std\n",
      "     |  var\n",
      "     |  interval\n",
      "     |  __call__\n",
      "     |  fit\n",
      "     |  fit_loc_scale\n",
      "     |  nnlf\n",
      "     |  support\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Public methods of an instance of a distribution class (e.g., ``pdf``,\n",
      "     |  ``cdf``) check their arguments and pass valid arguments to private,\n",
      "     |  computational methods (``_pdf``, ``_cdf``). For ``pdf(x)``, ``x`` is valid\n",
      "     |  if it is within the support of the distribution.\n",
      "     |  Whether a shape parameter is valid is decided by an ``_argcheck`` method\n",
      "     |  (which defaults to checking that its arguments are strictly positive.)\n",
      "     |  \n",
      "     |  **Subclassing**\n",
      "     |  \n",
      "     |  New random variables can be defined by subclassing the `rv_continuous` class\n",
      "     |  and re-defining at least the ``_pdf`` or the ``_cdf`` method (normalized\n",
      "     |  to location 0 and scale 1).\n",
      "     |  \n",
      "     |  If positive argument checking is not correct for your RV\n",
      "     |  then you will also need to re-define the ``_argcheck`` method.\n",
      "     |  \n",
      "     |  For most of the scipy.stats distributions, the support interval doesn't\n",
      "     |  depend on the shape parameters. ``x`` being in the support interval is\n",
      "     |  equivalent to ``self.a <= x <= self.b``.  If either of the endpoints of\n",
      "     |  the support do depend on the shape parameters, then\n",
      "     |  i) the distribution must implement the ``_get_support`` method; and\n",
      "     |  ii) those dependent endpoints must be omitted from the distribution's\n",
      "     |  call to the ``rv_continuous`` initializer.\n",
      "     |  \n",
      "     |  Correct, but potentially slow defaults exist for the remaining\n",
      "     |  methods but for speed and/or accuracy you can over-ride::\n",
      "     |  \n",
      "     |    _logpdf, _cdf, _logcdf, _ppf, _rvs, _isf, _sf, _logsf\n",
      "     |  \n",
      "     |  The default method ``_rvs`` relies on the inverse of the cdf, ``_ppf``,\n",
      "     |  applied to a uniform random variate. In order to generate random variates\n",
      "     |  efficiently, either the default ``_ppf`` needs to be overwritten (e.g.\n",
      "     |  if the inverse cdf can expressed in an explicit form) or a sampling\n",
      "     |  method needs to be implemented in a custom ``_rvs`` method.\n",
      "     |  \n",
      "     |  If possible, you should override ``_isf``, ``_sf`` or ``_logsf``.\n",
      "     |  The main reason would be to improve numerical accuracy: for example,\n",
      "     |  the survival function ``_sf`` is computed as ``1 - _cdf`` which can\n",
      "     |  result in loss of precision if ``_cdf(x)`` is close to one.\n",
      "     |  \n",
      "     |  **Methods that can be overwritten by subclasses**\n",
      "     |  ::\n",
      "     |  \n",
      "     |    _rvs\n",
      "     |    _pdf\n",
      "     |    _cdf\n",
      "     |    _sf\n",
      "     |    _ppf\n",
      "     |    _isf\n",
      "     |    _stats\n",
      "     |    _munp\n",
      "     |    _entropy\n",
      "     |    _argcheck\n",
      "     |    _get_support\n",
      "     |  \n",
      "     |  There are additional (internal and private) generic methods that can\n",
      "     |  be useful for cross-checking and for debugging, but might work in all\n",
      "     |  cases when directly called.\n",
      "     |  \n",
      "     |  A note on ``shapes``: subclasses need not specify them explicitly. In this\n",
      "     |  case, `shapes` will be automatically deduced from the signatures of the\n",
      "     |  overridden methods (`pdf`, `cdf` etc).\n",
      "     |  If, for some reason, you prefer to avoid relying on introspection, you can\n",
      "     |  specify ``shapes`` explicitly as an argument to the instance constructor.\n",
      "     |  \n",
      "     |  \n",
      "     |  **Frozen Distributions**\n",
      "     |  \n",
      "     |  Normally, you must provide shape parameters (and, optionally, location and\n",
      "     |  scale parameters to each call of a method of a distribution.\n",
      "     |  \n",
      "     |  Alternatively, the object may be called (as a function) to fix the shape,\n",
      "     |  location, and scale parameters returning a \"frozen\" continuous RV object:\n",
      "     |  \n",
      "     |  rv = generic(<shape(s)>, loc=0, scale=1)\n",
      "     |      `rv_frozen` object with the same methods but holding the given shape,\n",
      "     |      location, and scale fixed\n",
      "     |  \n",
      "     |  **Statistics**\n",
      "     |  \n",
      "     |  Statistics are computed using numerical integration by default.\n",
      "     |  For speed you can redefine this using ``_stats``:\n",
      "     |  \n",
      "     |   - take shape parameters and return mu, mu2, g1, g2\n",
      "     |   - If you can't compute one of these, return it as None\n",
      "     |   - Can also be defined with a keyword argument ``moments``, which is a\n",
      "     |     string composed of \"m\", \"v\", \"s\", and/or \"k\".\n",
      "     |     Only the components appearing in string should be computed and\n",
      "     |     returned in the order \"m\", \"v\", \"s\", or \"k\"  with missing values\n",
      "     |     returned as None.\n",
      "     |  \n",
      "     |  Alternatively, you can override ``_munp``, which takes ``n`` and shape\n",
      "     |  parameters and returns the n-th non-central moment of the distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  To create a new Gaussian distribution, we would do the following:\n",
      "     |  \n",
      "     |  >>> from scipy.stats import rv_continuous\n",
      "     |  >>> class gaussian_gen(rv_continuous):\n",
      "     |  ...     \"Gaussian distribution\"\n",
      "     |  ...     def _pdf(self, x):\n",
      "     |  ...         return np.exp(-x**2 / 2.) / np.sqrt(2.0 * np.pi)\n",
      "     |  >>> gaussian = gaussian_gen(name='gaussian')\n",
      "     |  \n",
      "     |  ``scipy.stats`` distributions are *instances*, so here we subclass\n",
      "     |  `rv_continuous` and create an instance. With this, we now have\n",
      "     |  a fully functional distribution with all relevant methods automagically\n",
      "     |  generated by the framework.\n",
      "     |  \n",
      "     |  Note that above we defined a standard normal distribution, with zero mean\n",
      "     |  and unit variance. Shifting and scaling of the distribution can be done\n",
      "     |  by using ``loc`` and ``scale`` parameters: ``gaussian.pdf(x, loc, scale)``\n",
      "     |  essentially computes ``y = (x - loc) / scale`` and\n",
      "     |  ``gaussian._pdf(y) / scale``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      rv_continuous\n",
      "     |      rv_generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, x, *args, **kwds)\n",
      "     |      Cumulative distribution function of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cdf : ndarray\n",
      "     |          Cumulative distribution function evaluated at `x`\n",
      "     |  \n",
      "     |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "     |      Calculate expected value of a function with respect to the\n",
      "     |      distribution by numerical integration.\n",
      "     |      \n",
      "     |      The expected value of a function ``f(x)`` with respect to a\n",
      "     |      distribution ``dist`` is defined as::\n",
      "     |      \n",
      "     |                  ub\n",
      "     |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
      "     |                  lb\n",
      "     |      \n",
      "     |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
      "     |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
      "     |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
      "     |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
      "     |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
      "     |      outside a finite interval in which case the expectation is\n",
      "     |      calculated within the finite range ``[lb, ub]``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : callable, optional\n",
      "     |          Function for which integral is calculated. Takes only one argument.\n",
      "     |          The default is the identity mapping f(x) = x.\n",
      "     |      args : tuple, optional\n",
      "     |          Shape parameters of the distribution.\n",
      "     |      loc : float, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : float, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      lb, ub : scalar, optional\n",
      "     |          Lower and upper bound for integration. Default is set to the\n",
      "     |          support of the distribution.\n",
      "     |      conditional : bool, optional\n",
      "     |          If True, the integral is corrected by the conditional probability\n",
      "     |          of the integration interval.  The return value is the expectation\n",
      "     |          of the function, conditional on being in the given interval.\n",
      "     |          Default is False.\n",
      "     |      \n",
      "     |      Additional keyword arguments are passed to the integration routine.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      expect : float\n",
      "     |          The calculated expected value.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The integration behavior of this function is inherited from\n",
      "     |      `scipy.integrate.quad`. Neither this function nor\n",
      "     |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
      "     |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
      "     |      ``cauchy(0).expect()`` returns ``0.0``.\n",
      "     |      \n",
      "     |      The function is not vectorized.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      To understand the effect of the bounds of integration consider\n",
      "     |      \n",
      "     |      >>> from scipy.stats import expon\n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
      "     |      0.6321205588285578\n",
      "     |      \n",
      "     |      This is close to\n",
      "     |      \n",
      "     |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
      "     |      0.6321205588285577\n",
      "     |      \n",
      "     |      If ``conditional=True``\n",
      "     |      \n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
      "     |      1.0000000000000002\n",
      "     |      \n",
      "     |      The slight deviation from 1 is due to numerical integration.\n",
      "     |  \n",
      "     |  fit(self, data, *args, **kwds)\n",
      "     |      Return MLEs for shape (if applicable), location, and scale\n",
      "     |      parameters from data.\n",
      "     |      \n",
      "     |      MLE stands for Maximum Likelihood Estimate.  Starting estimates for\n",
      "     |      the fit are given by input arguments; for any arguments not provided\n",
      "     |      with starting estimates, ``self._fitstart(data)`` is called to generate\n",
      "     |      such.\n",
      "     |      \n",
      "     |      One can hold some parameters fixed to specific values by passing in\n",
      "     |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
      "     |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
      "     |      respectively).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to use in calculating the MLEs.\n",
      "     |      arg1, arg2, arg3,... : floats, optional\n",
      "     |          Starting value(s) for any shape-characterizing arguments (those not\n",
      "     |          provided will be determined by a call to ``_fitstart(data)``).\n",
      "     |          No default value.\n",
      "     |      kwds : floats, optional\n",
      "     |          - `loc`: initial guess of the distribution's location parameter.\n",
      "     |          - `scale`: initial guess of the distribution's scale parameter.\n",
      "     |      \n",
      "     |          Special keyword arguments are recognized as holding certain\n",
      "     |          parameters fixed:\n",
      "     |      \n",
      "     |          - f0...fn : hold respective shape parameters fixed.\n",
      "     |            Alternatively, shape parameters to fix can be specified by name.\n",
      "     |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
      "     |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
      "     |            equivalent to ``f1``.\n",
      "     |      \n",
      "     |          - floc : hold location parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - fscale : hold scale parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - optimizer : The optimizer to use.  The optimizer must take ``func``,\n",
      "     |            and starting position as the first two arguments,\n",
      "     |            plus ``args`` (for extra arguments to pass to the\n",
      "     |            function to be optimized) and ``disp=0`` to suppress\n",
      "     |            output as keyword arguments.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mle_tuple : tuple of floats\n",
      "     |          MLEs for any shape parameters (if applicable), followed by those\n",
      "     |          for location and scale. For most random variables, shape statistics\n",
      "     |          will be returned, but there are exceptions (e.g. ``norm``).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This fit is computed by maximizing a log-likelihood function, with\n",
      "     |      penalty applied for samples outside of range of the distribution. The\n",
      "     |      returned answer is not guaranteed to be the globally optimal MLE, it\n",
      "     |      may only be locally optimal, or the optimization may fail altogether.\n",
      "     |      If the data contain any of np.nan, np.inf, or -np.inf, the fit routine\n",
      "     |      will throw a RuntimeError.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Generate some data to fit: draw random variates from the `beta`\n",
      "     |      distribution\n",
      "     |      \n",
      "     |      >>> from scipy.stats import beta\n",
      "     |      >>> a, b = 1., 2.\n",
      "     |      >>> x = beta.rvs(a, b, size=1000)\n",
      "     |      \n",
      "     |      Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
      "     |      \n",
      "     |      We can also use some prior knowledge about the dataset: let's keep\n",
      "     |      ``loc`` and ``scale`` fixed:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0, 1)\n",
      "     |      \n",
      "     |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
      "     |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
      "     |      equivalently, ``fa=1``:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
      "     |      >>> a1\n",
      "     |      1\n",
      "     |      \n",
      "     |      Not all distributions return estimates for the shape parameters.\n",
      "     |      ``norm`` for example just returns estimates for location and scale:\n",
      "     |      \n",
      "     |      >>> from scipy.stats import norm\n",
      "     |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
      "     |      >>> loc1, scale1 = norm.fit(x)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0.92087172783841631, 2.0015750750324668)\n",
      "     |  \n",
      "     |  fit_loc_scale(self, data, *args)\n",
      "     |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to fit.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Lhat : float\n",
      "     |          Estimated location parameter for the data.\n",
      "     |      Shat : float\n",
      "     |          Estimated scale parameter for the data.\n",
      "     |  \n",
      "     |  isf(self, q, *args, **kwds)\n",
      "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          upper tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : ndarray or scalar\n",
      "     |          Quantile corresponding to the upper tail probability q.\n",
      "     |  \n",
      "     |  logcdf(self, x, *args, **kwds)\n",
      "     |      Log of the cumulative distribution function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logcdf : array_like\n",
      "     |          Log of the cumulative distribution function evaluated at x\n",
      "     |  \n",
      "     |  logpdf(self, x, *args, **kwds)\n",
      "     |      Log of the probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      This uses a more numerically accurate calculation if available.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logpdf : array_like\n",
      "     |          Log of the probability density function evaluated at x\n",
      "     |  \n",
      "     |  logsf(self, x, *args, **kwds)\n",
      "     |      Log of the survival function of the given RV.\n",
      "     |      \n",
      "     |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
      "     |      evaluated at `x`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logsf : ndarray\n",
      "     |          Log of the survival function evaluated at `x`.\n",
      "     |  \n",
      "     |  nnlf(self, theta, x)\n",
      "     |      Return negative loglikelihood function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
      "     |      parameters (including loc and scale).\n",
      "     |  \n",
      "     |  pdf(self, x, *args, **kwds)\n",
      "     |      Probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pdf : ndarray\n",
      "     |          Probability density function evaluated at x\n",
      "     |  \n",
      "     |  ppf(self, q, *args, **kwds)\n",
      "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          lower tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : array_like\n",
      "     |          quantile corresponding to the lower tail probability q.\n",
      "     |  \n",
      "     |  sf(self, x, *args, **kwds)\n",
      "     |      Survival function (1 - `cdf`) at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sf : array_like\n",
      "     |          Survival function evaluated at x\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from rv_generic:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  entropy(self, *args, **kwds)\n",
      "     |      Differential entropy of the RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional  (continuous distributions only).\n",
      "     |          Scale parameter (default=1).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Entropy is defined base `e`:\n",
      "     |      \n",
      "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      "     |      True\n",
      "     |  \n",
      "     |  freeze(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  interval(self, alpha, *args, **kwds)\n",
      "     |      Confidence interval with equal areas around the median.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alpha : array_like of float\n",
      "     |          Probability that an rv will be drawn from the returned range.\n",
      "     |          Each value should be in the range [0, 1].\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : ndarray of float\n",
      "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      "     |          possible values.\n",
      "     |  \n",
      "     |  mean(self, *args, **kwds)\n",
      "     |      Mean of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mean : float\n",
      "     |          the mean of the distribution\n",
      "     |  \n",
      "     |  median(self, *args, **kwds)\n",
      "     |      Median of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      median : float\n",
      "     |          The median of the distribution.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      rv_discrete.ppf\n",
      "     |          Inverse of the CDF\n",
      "     |  \n",
      "     |  moment(self, n, *args, **kwds)\n",
      "     |      n-th order non-central moment of distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, n >= 1\n",
      "     |          Order of moment.\n",
      "     |      arg1, arg2, arg3,... : float\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |  \n",
      "     |  rvs(self, *args, **kwds)\n",
      "     |      Random variates of given type.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      size : int or tuple of ints, optional\n",
      "     |          Defining number of random variates (default is 1).\n",
      "     |      random_state : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "     |          If `seed` is `None` the `~np.random.RandomState` singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "     |          with seed.\n",
      "     |          If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
      "     |          then that object is used.\n",
      "     |          Default is None.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`.\n",
      "     |  \n",
      "     |  stats(self, *args, **kwds)\n",
      "     |      Some statistics of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional (continuous RVs only)\n",
      "     |          scale parameter (default=1)\n",
      "     |      moments : str, optional\n",
      "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
      "     |          'm' = mean,\n",
      "     |          'v' = variance,\n",
      "     |          's' = (Fisher's) skew,\n",
      "     |          'k' = (Fisher's) kurtosis.\n",
      "     |          (default is 'mv')\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stats : sequence\n",
      "     |          of requested moments.\n",
      "     |  \n",
      "     |  std(self, *args, **kwds)\n",
      "     |      Standard deviation of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      std : float\n",
      "     |          standard deviation of the distribution\n",
      "     |  \n",
      "     |  support(self, *args, **kwargs)\n",
      "     |      Return the support of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : float\n",
      "     |          end-points of the distribution's support.\n",
      "     |  \n",
      "     |  var(self, *args, **kwds)\n",
      "     |      Variance of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      var : float\n",
      "     |          the variance of the distribution\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from rv_generic:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  random_state\n",
      "     |      Get or set the RandomState object for generating random variates.\n",
      "     |      \n",
      "     |      This can be either None, int, a RandomState instance, or a\n",
      "     |      np.random.Generator instance.\n",
      "     |      \n",
      "     |      If None (or np.random), use the RandomState singleton used by np.random.\n",
      "     |      If already a RandomState or Generator instance, use it.\n",
      "     |      If an int, use a new RandomState instance seeded with seed.\n",
      "    \n",
      "    class rv_discrete(rv_generic)\n",
      "     |  rv_discrete(a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |  \n",
      "     |  A generic discrete random variable class meant for subclassing.\n",
      "     |  \n",
      "     |  `rv_discrete` is a base class to construct specific distribution classes\n",
      "     |  and instances for discrete random variables. It can also be used\n",
      "     |  to construct an arbitrary distribution defined by a list of support\n",
      "     |  points and corresponding probabilities.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  a : float, optional\n",
      "     |      Lower bound of the support of the distribution, default: 0\n",
      "     |  b : float, optional\n",
      "     |      Upper bound of the support of the distribution, default: plus infinity\n",
      "     |  moment_tol : float, optional\n",
      "     |      The tolerance for the generic calculation of moments.\n",
      "     |  values : tuple of two array_like, optional\n",
      "     |      ``(xk, pk)`` where ``xk`` are integers and ``pk`` are the non-zero\n",
      "     |      probabilities between 0 and 1 with ``sum(pk) = 1``. ``xk``\n",
      "     |      and ``pk`` must have the same shape.\n",
      "     |  inc : integer, optional\n",
      "     |      Increment for the support of the distribution.\n",
      "     |      Default is 1. (other values have not been tested)\n",
      "     |  badvalue : float, optional\n",
      "     |      The value in a result arrays that indicates a value that for which\n",
      "     |      some argument restriction is violated, default is np.nan.\n",
      "     |  name : str, optional\n",
      "     |      The name of the instance. This string is used to construct the default\n",
      "     |      example for distributions.\n",
      "     |  longname : str, optional\n",
      "     |      This string is used as part of the first line of the docstring returned\n",
      "     |      when a subclass has no docstring of its own. Note: `longname` exists\n",
      "     |      for backwards compatibility, do not use for new subclasses.\n",
      "     |  shapes : str, optional\n",
      "     |      The shape of the distribution. For example \"m, n\" for a distribution\n",
      "     |      that takes two integers as the two shape arguments for all its methods\n",
      "     |      If not provided, shape parameters will be inferred from\n",
      "     |      the signatures of the private methods, ``_pmf`` and ``_cdf`` of\n",
      "     |      the instance.\n",
      "     |  extradoc :  str, optional\n",
      "     |      This string is used as the last part of the docstring returned when a\n",
      "     |      subclass has no docstring of its own. Note: `extradoc` exists for\n",
      "     |      backwards compatibility, do not use for new subclasses.\n",
      "     |  seed : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "     |      This parameter defines the object to use for drawing random variates.\n",
      "     |      If `seed` is `None` the `~np.random.RandomState` singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "     |      with seed.\n",
      "     |      If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
      "     |      then that object is used.\n",
      "     |      Default is None.\n",
      "     |  \n",
      "     |  Methods\n",
      "     |  -------\n",
      "     |  rvs\n",
      "     |  pmf\n",
      "     |  logpmf\n",
      "     |  cdf\n",
      "     |  logcdf\n",
      "     |  sf\n",
      "     |  logsf\n",
      "     |  ppf\n",
      "     |  isf\n",
      "     |  moment\n",
      "     |  stats\n",
      "     |  entropy\n",
      "     |  expect\n",
      "     |  median\n",
      "     |  mean\n",
      "     |  std\n",
      "     |  var\n",
      "     |  interval\n",
      "     |  __call__\n",
      "     |  support\n",
      "     |  \n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  \n",
      "     |  This class is similar to `rv_continuous`. Whether a shape parameter is\n",
      "     |  valid is decided by an ``_argcheck`` method (which defaults to checking\n",
      "     |  that its arguments are strictly positive.)\n",
      "     |  The main differences are:\n",
      "     |  \n",
      "     |  - the support of the distribution is a set of integers\n",
      "     |  - instead of the probability density function, ``pdf`` (and the\n",
      "     |    corresponding private ``_pdf``), this class defines the\n",
      "     |    *probability mass function*, `pmf` (and the corresponding\n",
      "     |    private ``_pmf``.)\n",
      "     |  - scale parameter is not defined.\n",
      "     |  \n",
      "     |  To create a new discrete distribution, we would do the following:\n",
      "     |  \n",
      "     |  >>> from scipy.stats import rv_discrete\n",
      "     |  >>> class poisson_gen(rv_discrete):\n",
      "     |  ...     \"Poisson distribution\"\n",
      "     |  ...     def _pmf(self, k, mu):\n",
      "     |  ...         return exp(-mu) * mu**k / factorial(k)\n",
      "     |  \n",
      "     |  and create an instance::\n",
      "     |  \n",
      "     |  >>> poisson = poisson_gen(name=\"poisson\")\n",
      "     |  \n",
      "     |  Note that above we defined the Poisson distribution in the standard form.\n",
      "     |  Shifting the distribution can be done by providing the ``loc`` parameter\n",
      "     |  to the methods of the instance. For example, ``poisson.pmf(x, mu, loc)``\n",
      "     |  delegates the work to ``poisson._pmf(x-loc, mu)``.\n",
      "     |  \n",
      "     |  **Discrete distributions from a list of probabilities**\n",
      "     |  \n",
      "     |  Alternatively, you can construct an arbitrary discrete rv defined\n",
      "     |  on a finite set of values ``xk`` with ``Prob{X=xk} = pk`` by using the\n",
      "     |  ``values`` keyword argument to the `rv_discrete` constructor.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  Custom made discrete distribution:\n",
      "     |  \n",
      "     |  >>> from scipy import stats\n",
      "     |  >>> xk = np.arange(7)\n",
      "     |  >>> pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)\n",
      "     |  >>> custm = stats.rv_discrete(name='custm', values=(xk, pk))\n",
      "     |  >>>\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> fig, ax = plt.subplots(1, 1)\n",
      "     |  >>> ax.plot(xk, custm.pmf(xk), 'ro', ms=12, mec='r')\n",
      "     |  >>> ax.vlines(xk, 0, custm.pmf(xk), colors='r', lw=4)\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Random number generation:\n",
      "     |  \n",
      "     |  >>> R = custm.rvs(size=100)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      rv_discrete\n",
      "     |      rv_generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, k, *args, **kwds)\n",
      "     |      Cumulative distribution function of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like, int\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cdf : ndarray\n",
      "     |          Cumulative distribution function evaluated at `k`.\n",
      "     |  \n",
      "     |  expect(self, func=None, args=(), loc=0, lb=None, ub=None, conditional=False, maxcount=1000, tolerance=1e-10, chunksize=32)\n",
      "     |      Calculate expected value of a function with respect to the distribution\n",
      "     |      for discrete distribution by numerical summation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : callable, optional\n",
      "     |          Function for which the expectation value is calculated.\n",
      "     |          Takes only one argument.\n",
      "     |          The default is the identity mapping f(k) = k.\n",
      "     |      args : tuple, optional\n",
      "     |          Shape parameters of the distribution.\n",
      "     |      loc : float, optional\n",
      "     |          Location parameter.\n",
      "     |          Default is 0.\n",
      "     |      lb, ub : int, optional\n",
      "     |          Lower and upper bound for the summation, default is set to the\n",
      "     |          support of the distribution, inclusive (``ul <= k <= ub``).\n",
      "     |      conditional : bool, optional\n",
      "     |          If true then the expectation is corrected by the conditional\n",
      "     |          probability of the summation interval. The return value is the\n",
      "     |          expectation of the function, `func`, conditional on being in\n",
      "     |          the given interval (k such that ``ul <= k <= ub``).\n",
      "     |          Default is False.\n",
      "     |      maxcount : int, optional\n",
      "     |          Maximal number of terms to evaluate (to avoid an endless loop for\n",
      "     |          an infinite sum). Default is 1000.\n",
      "     |      tolerance : float, optional\n",
      "     |          Absolute tolerance for the summation. Default is 1e-10.\n",
      "     |      chunksize : int, optional\n",
      "     |          Iterate over the support of a distributions in chunks of this size.\n",
      "     |          Default is 32.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      expect : float\n",
      "     |          Expected value.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For heavy-tailed distributions, the expected value may or may not exist,\n",
      "     |      depending on the function, `func`. If it does exist, but the sum converges\n",
      "     |      slowly, the accuracy of the result may be rather low. For instance, for\n",
      "     |      ``zipf(4)``, accuracy for mean, variance in example is only 1e-5.\n",
      "     |      increasing `maxcount` and/or `chunksize` may improve the result, but may\n",
      "     |      also make zipf very slow.\n",
      "     |      \n",
      "     |      The function is not vectorized.\n",
      "     |  \n",
      "     |  isf(self, q, *args, **kwds)\n",
      "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          Upper tail probability.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      k : ndarray or scalar\n",
      "     |          Quantile corresponding to the upper tail probability, q.\n",
      "     |  \n",
      "     |  logcdf(self, k, *args, **kwds)\n",
      "     |      Log of the cumulative distribution function at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like, int\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logcdf : array_like\n",
      "     |          Log of the cumulative distribution function evaluated at k.\n",
      "     |  \n",
      "     |  logpmf(self, k, *args, **kwds)\n",
      "     |      Log of the probability mass function at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter. Default is 0.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logpmf : array_like\n",
      "     |          Log of the probability mass function evaluated at k.\n",
      "     |  \n",
      "     |  logsf(self, k, *args, **kwds)\n",
      "     |      Log of the survival function of the given RV.\n",
      "     |      \n",
      "     |      Returns the log of the \"survival function,\" defined as 1 - `cdf`,\n",
      "     |      evaluated at `k`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logsf : ndarray\n",
      "     |          Log of the survival function evaluated at `k`.\n",
      "     |  \n",
      "     |  pmf(self, k, *args, **kwds)\n",
      "     |      Probability mass function at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pmf : array_like\n",
      "     |          Probability mass function evaluated at k\n",
      "     |  \n",
      "     |  ppf(self, q, *args, **kwds)\n",
      "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          Lower tail probability.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      k : array_like\n",
      "     |          Quantile corresponding to the lower tail probability, q.\n",
      "     |  \n",
      "     |  rvs(self, *args, **kwargs)\n",
      "     |      Random variates of given type.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      size : int or tuple of ints, optional\n",
      "     |          Defining number of random variates (Default is 1).  Note that `size`\n",
      "     |          has to be given as keyword, not as positional argument.\n",
      "     |      random_state : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "     |          This parameter defines the object to use for drawing random\n",
      "     |          variates.\n",
      "     |          If `random_state` is `None` the `~np.random.RandomState` singleton\n",
      "     |          is used.\n",
      "     |          If `random_state` is an int, a new ``RandomState`` instance is used,\n",
      "     |          seeded with random_state.\n",
      "     |          If `random_state` is already a ``RandomState`` or ``Generator``\n",
      "     |          instance, then that object is used.\n",
      "     |          Default is None.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`.\n",
      "     |  \n",
      "     |  sf(self, k, *args, **kwds)\n",
      "     |      Survival function (1 - `cdf`) at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sf : array_like\n",
      "     |          Survival function evaluated at k.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(cls, a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from rv_generic:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  entropy(self, *args, **kwds)\n",
      "     |      Differential entropy of the RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional  (continuous distributions only).\n",
      "     |          Scale parameter (default=1).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Entropy is defined base `e`:\n",
      "     |      \n",
      "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      "     |      True\n",
      "     |  \n",
      "     |  freeze(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  interval(self, alpha, *args, **kwds)\n",
      "     |      Confidence interval with equal areas around the median.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alpha : array_like of float\n",
      "     |          Probability that an rv will be drawn from the returned range.\n",
      "     |          Each value should be in the range [0, 1].\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : ndarray of float\n",
      "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      "     |          possible values.\n",
      "     |  \n",
      "     |  mean(self, *args, **kwds)\n",
      "     |      Mean of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mean : float\n",
      "     |          the mean of the distribution\n",
      "     |  \n",
      "     |  median(self, *args, **kwds)\n",
      "     |      Median of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      median : float\n",
      "     |          The median of the distribution.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      rv_discrete.ppf\n",
      "     |          Inverse of the CDF\n",
      "     |  \n",
      "     |  moment(self, n, *args, **kwds)\n",
      "     |      n-th order non-central moment of distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, n >= 1\n",
      "     |          Order of moment.\n",
      "     |      arg1, arg2, arg3,... : float\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |  \n",
      "     |  stats(self, *args, **kwds)\n",
      "     |      Some statistics of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional (continuous RVs only)\n",
      "     |          scale parameter (default=1)\n",
      "     |      moments : str, optional\n",
      "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
      "     |          'm' = mean,\n",
      "     |          'v' = variance,\n",
      "     |          's' = (Fisher's) skew,\n",
      "     |          'k' = (Fisher's) kurtosis.\n",
      "     |          (default is 'mv')\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stats : sequence\n",
      "     |          of requested moments.\n",
      "     |  \n",
      "     |  std(self, *args, **kwds)\n",
      "     |      Standard deviation of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      std : float\n",
      "     |          standard deviation of the distribution\n",
      "     |  \n",
      "     |  support(self, *args, **kwargs)\n",
      "     |      Return the support of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : float\n",
      "     |          end-points of the distribution's support.\n",
      "     |  \n",
      "     |  var(self, *args, **kwds)\n",
      "     |      Variance of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      var : float\n",
      "     |          the variance of the distribution\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from rv_generic:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  random_state\n",
      "     |      Get or set the RandomState object for generating random variates.\n",
      "     |      \n",
      "     |      This can be either None, int, a RandomState instance, or a\n",
      "     |      np.random.Generator instance.\n",
      "     |      \n",
      "     |      If None (or np.random), use the RandomState singleton used by np.random.\n",
      "     |      If already a RandomState or Generator instance, use it.\n",
      "     |      If an int, use a new RandomState instance seeded with seed.\n",
      "    \n",
      "    class rv_histogram(scipy.stats._distn_infrastructure.rv_continuous)\n",
      "     |  rv_histogram(histogram, *args, **kwargs)\n",
      "     |  \n",
      "     |  Generates a distribution given by a histogram.\n",
      "     |  This is useful to generate a template distribution from a binned\n",
      "     |  datasample.\n",
      "     |  \n",
      "     |  As a subclass of the `rv_continuous` class, `rv_histogram` inherits from it\n",
      "     |  a collection of generic methods (see `rv_continuous` for the full list),\n",
      "     |  and implements them based on the properties of the provided binned\n",
      "     |  datasample.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  histogram : tuple of array_like\n",
      "     |    Tuple containing two array_like objects\n",
      "     |    The first containing the content of n bins\n",
      "     |    The second containing the (n+1) bin boundaries\n",
      "     |    In particular the return value np.histogram is accepted\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There are no additional shape parameters except for the loc and scale.\n",
      "     |  The pdf is defined as a stepwise function from the provided histogram\n",
      "     |  The cdf is a linear interpolation of the pdf.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.19.0\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  Create a scipy.stats distribution from a numpy histogram\n",
      "     |  \n",
      "     |  >>> import scipy.stats\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)\n",
      "     |  >>> hist = np.histogram(data, bins=100)\n",
      "     |  >>> hist_dist = scipy.stats.rv_histogram(hist)\n",
      "     |  \n",
      "     |  Behaves like an ordinary scipy rv_continuous distribution\n",
      "     |  \n",
      "     |  >>> hist_dist.pdf(1.0)\n",
      "     |  0.20538577847618705\n",
      "     |  >>> hist_dist.cdf(2.0)\n",
      "     |  0.90818568543056499\n",
      "     |  \n",
      "     |  PDF is zero above (below) the highest (lowest) bin of the histogram,\n",
      "     |  defined by the max (min) of the original dataset\n",
      "     |  \n",
      "     |  >>> hist_dist.pdf(np.max(data))\n",
      "     |  0.0\n",
      "     |  >>> hist_dist.cdf(np.max(data))\n",
      "     |  1.0\n",
      "     |  >>> hist_dist.pdf(np.min(data))\n",
      "     |  7.7591907244498314e-05\n",
      "     |  >>> hist_dist.cdf(np.min(data))\n",
      "     |  0.0\n",
      "     |  \n",
      "     |  PDF and CDF follow the histogram\n",
      "     |  \n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> X = np.linspace(-5.0, 5.0, 100)\n",
      "     |  >>> plt.title(\"PDF from Template\")\n",
      "     |  >>> plt.hist(data, density=True, bins=100)\n",
      "     |  >>> plt.plot(X, hist_dist.pdf(X), label='PDF')\n",
      "     |  >>> plt.plot(X, hist_dist.cdf(X), label='CDF')\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      rv_histogram\n",
      "     |      scipy.stats._distn_infrastructure.rv_continuous\n",
      "     |      scipy.stats._distn_infrastructure.rv_generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, histogram, *args, **kwargs)\n",
      "     |      Create a new distribution using the given histogram\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      histogram : tuple of array_like\n",
      "     |        Tuple containing two array_like objects\n",
      "     |        The first containing the content of n bins\n",
      "     |        The second containing the (n+1) bin boundaries\n",
      "     |        In particular the return value np.histogram is accepted\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.stats._distn_infrastructure.rv_continuous:\n",
      "     |  \n",
      "     |  cdf(self, x, *args, **kwds)\n",
      "     |      Cumulative distribution function of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cdf : ndarray\n",
      "     |          Cumulative distribution function evaluated at `x`\n",
      "     |  \n",
      "     |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "     |      Calculate expected value of a function with respect to the\n",
      "     |      distribution by numerical integration.\n",
      "     |      \n",
      "     |      The expected value of a function ``f(x)`` with respect to a\n",
      "     |      distribution ``dist`` is defined as::\n",
      "     |      \n",
      "     |                  ub\n",
      "     |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
      "     |                  lb\n",
      "     |      \n",
      "     |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
      "     |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
      "     |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
      "     |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
      "     |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
      "     |      outside a finite interval in which case the expectation is\n",
      "     |      calculated within the finite range ``[lb, ub]``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : callable, optional\n",
      "     |          Function for which integral is calculated. Takes only one argument.\n",
      "     |          The default is the identity mapping f(x) = x.\n",
      "     |      args : tuple, optional\n",
      "     |          Shape parameters of the distribution.\n",
      "     |      loc : float, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : float, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      lb, ub : scalar, optional\n",
      "     |          Lower and upper bound for integration. Default is set to the\n",
      "     |          support of the distribution.\n",
      "     |      conditional : bool, optional\n",
      "     |          If True, the integral is corrected by the conditional probability\n",
      "     |          of the integration interval.  The return value is the expectation\n",
      "     |          of the function, conditional on being in the given interval.\n",
      "     |          Default is False.\n",
      "     |      \n",
      "     |      Additional keyword arguments are passed to the integration routine.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      expect : float\n",
      "     |          The calculated expected value.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The integration behavior of this function is inherited from\n",
      "     |      `scipy.integrate.quad`. Neither this function nor\n",
      "     |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
      "     |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
      "     |      ``cauchy(0).expect()`` returns ``0.0``.\n",
      "     |      \n",
      "     |      The function is not vectorized.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      To understand the effect of the bounds of integration consider\n",
      "     |      \n",
      "     |      >>> from scipy.stats import expon\n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
      "     |      0.6321205588285578\n",
      "     |      \n",
      "     |      This is close to\n",
      "     |      \n",
      "     |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
      "     |      0.6321205588285577\n",
      "     |      \n",
      "     |      If ``conditional=True``\n",
      "     |      \n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
      "     |      1.0000000000000002\n",
      "     |      \n",
      "     |      The slight deviation from 1 is due to numerical integration.\n",
      "     |  \n",
      "     |  fit(self, data, *args, **kwds)\n",
      "     |      Return MLEs for shape (if applicable), location, and scale\n",
      "     |      parameters from data.\n",
      "     |      \n",
      "     |      MLE stands for Maximum Likelihood Estimate.  Starting estimates for\n",
      "     |      the fit are given by input arguments; for any arguments not provided\n",
      "     |      with starting estimates, ``self._fitstart(data)`` is called to generate\n",
      "     |      such.\n",
      "     |      \n",
      "     |      One can hold some parameters fixed to specific values by passing in\n",
      "     |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
      "     |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
      "     |      respectively).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to use in calculating the MLEs.\n",
      "     |      arg1, arg2, arg3,... : floats, optional\n",
      "     |          Starting value(s) for any shape-characterizing arguments (those not\n",
      "     |          provided will be determined by a call to ``_fitstart(data)``).\n",
      "     |          No default value.\n",
      "     |      kwds : floats, optional\n",
      "     |          - `loc`: initial guess of the distribution's location parameter.\n",
      "     |          - `scale`: initial guess of the distribution's scale parameter.\n",
      "     |      \n",
      "     |          Special keyword arguments are recognized as holding certain\n",
      "     |          parameters fixed:\n",
      "     |      \n",
      "     |          - f0...fn : hold respective shape parameters fixed.\n",
      "     |            Alternatively, shape parameters to fix can be specified by name.\n",
      "     |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
      "     |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
      "     |            equivalent to ``f1``.\n",
      "     |      \n",
      "     |          - floc : hold location parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - fscale : hold scale parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - optimizer : The optimizer to use.  The optimizer must take ``func``,\n",
      "     |            and starting position as the first two arguments,\n",
      "     |            plus ``args`` (for extra arguments to pass to the\n",
      "     |            function to be optimized) and ``disp=0`` to suppress\n",
      "     |            output as keyword arguments.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mle_tuple : tuple of floats\n",
      "     |          MLEs for any shape parameters (if applicable), followed by those\n",
      "     |          for location and scale. For most random variables, shape statistics\n",
      "     |          will be returned, but there are exceptions (e.g. ``norm``).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This fit is computed by maximizing a log-likelihood function, with\n",
      "     |      penalty applied for samples outside of range of the distribution. The\n",
      "     |      returned answer is not guaranteed to be the globally optimal MLE, it\n",
      "     |      may only be locally optimal, or the optimization may fail altogether.\n",
      "     |      If the data contain any of np.nan, np.inf, or -np.inf, the fit routine\n",
      "     |      will throw a RuntimeError.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Generate some data to fit: draw random variates from the `beta`\n",
      "     |      distribution\n",
      "     |      \n",
      "     |      >>> from scipy.stats import beta\n",
      "     |      >>> a, b = 1., 2.\n",
      "     |      >>> x = beta.rvs(a, b, size=1000)\n",
      "     |      \n",
      "     |      Now we can fit all four parameters (``a``, ``b``, ``loc`` and ``scale``):\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
      "     |      \n",
      "     |      We can also use some prior knowledge about the dataset: let's keep\n",
      "     |      ``loc`` and ``scale`` fixed:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0, 1)\n",
      "     |      \n",
      "     |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
      "     |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
      "     |      equivalently, ``fa=1``:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
      "     |      >>> a1\n",
      "     |      1\n",
      "     |      \n",
      "     |      Not all distributions return estimates for the shape parameters.\n",
      "     |      ``norm`` for example just returns estimates for location and scale:\n",
      "     |      \n",
      "     |      >>> from scipy.stats import norm\n",
      "     |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
      "     |      >>> loc1, scale1 = norm.fit(x)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0.92087172783841631, 2.0015750750324668)\n",
      "     |  \n",
      "     |  fit_loc_scale(self, data, *args)\n",
      "     |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to fit.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Lhat : float\n",
      "     |          Estimated location parameter for the data.\n",
      "     |      Shat : float\n",
      "     |          Estimated scale parameter for the data.\n",
      "     |  \n",
      "     |  isf(self, q, *args, **kwds)\n",
      "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          upper tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : ndarray or scalar\n",
      "     |          Quantile corresponding to the upper tail probability q.\n",
      "     |  \n",
      "     |  logcdf(self, x, *args, **kwds)\n",
      "     |      Log of the cumulative distribution function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logcdf : array_like\n",
      "     |          Log of the cumulative distribution function evaluated at x\n",
      "     |  \n",
      "     |  logpdf(self, x, *args, **kwds)\n",
      "     |      Log of the probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      This uses a more numerically accurate calculation if available.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logpdf : array_like\n",
      "     |          Log of the probability density function evaluated at x\n",
      "     |  \n",
      "     |  logsf(self, x, *args, **kwds)\n",
      "     |      Log of the survival function of the given RV.\n",
      "     |      \n",
      "     |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
      "     |      evaluated at `x`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logsf : ndarray\n",
      "     |          Log of the survival function evaluated at `x`.\n",
      "     |  \n",
      "     |  nnlf(self, theta, x)\n",
      "     |      Return negative loglikelihood function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
      "     |      parameters (including loc and scale).\n",
      "     |  \n",
      "     |  pdf(self, x, *args, **kwds)\n",
      "     |      Probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pdf : ndarray\n",
      "     |          Probability density function evaluated at x\n",
      "     |  \n",
      "     |  ppf(self, q, *args, **kwds)\n",
      "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          lower tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : array_like\n",
      "     |          quantile corresponding to the lower tail probability q.\n",
      "     |  \n",
      "     |  sf(self, x, *args, **kwds)\n",
      "     |      Survival function (1 - `cdf`) at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sf : array_like\n",
      "     |          Survival function evaluated at x\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  entropy(self, *args, **kwds)\n",
      "     |      Differential entropy of the RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional  (continuous distributions only).\n",
      "     |          Scale parameter (default=1).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Entropy is defined base `e`:\n",
      "     |      \n",
      "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      "     |      True\n",
      "     |  \n",
      "     |  freeze(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  interval(self, alpha, *args, **kwds)\n",
      "     |      Confidence interval with equal areas around the median.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alpha : array_like of float\n",
      "     |          Probability that an rv will be drawn from the returned range.\n",
      "     |          Each value should be in the range [0, 1].\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : ndarray of float\n",
      "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      "     |          possible values.\n",
      "     |  \n",
      "     |  mean(self, *args, **kwds)\n",
      "     |      Mean of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mean : float\n",
      "     |          the mean of the distribution\n",
      "     |  \n",
      "     |  median(self, *args, **kwds)\n",
      "     |      Median of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      median : float\n",
      "     |          The median of the distribution.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      rv_discrete.ppf\n",
      "     |          Inverse of the CDF\n",
      "     |  \n",
      "     |  moment(self, n, *args, **kwds)\n",
      "     |      n-th order non-central moment of distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, n >= 1\n",
      "     |          Order of moment.\n",
      "     |      arg1, arg2, arg3,... : float\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |  \n",
      "     |  rvs(self, *args, **kwds)\n",
      "     |      Random variates of given type.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      size : int or tuple of ints, optional\n",
      "     |          Defining number of random variates (default is 1).\n",
      "     |      random_state : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "     |          If `seed` is `None` the `~np.random.RandomState` singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used, seeded\n",
      "     |          with seed.\n",
      "     |          If `seed` is already a ``RandomState`` or ``Generator`` instance,\n",
      "     |          then that object is used.\n",
      "     |          Default is None.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`.\n",
      "     |  \n",
      "     |  stats(self, *args, **kwds)\n",
      "     |      Some statistics of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional (continuous RVs only)\n",
      "     |          scale parameter (default=1)\n",
      "     |      moments : str, optional\n",
      "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
      "     |          'm' = mean,\n",
      "     |          'v' = variance,\n",
      "     |          's' = (Fisher's) skew,\n",
      "     |          'k' = (Fisher's) kurtosis.\n",
      "     |          (default is 'mv')\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stats : sequence\n",
      "     |          of requested moments.\n",
      "     |  \n",
      "     |  std(self, *args, **kwds)\n",
      "     |      Standard deviation of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      std : float\n",
      "     |          standard deviation of the distribution\n",
      "     |  \n",
      "     |  support(self, *args, **kwargs)\n",
      "     |      Return the support of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : float\n",
      "     |          end-points of the distribution's support.\n",
      "     |  \n",
      "     |  var(self, *args, **kwds)\n",
      "     |      Variance of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      var : float\n",
      "     |          the variance of the distribution\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  random_state\n",
      "     |      Get or set the RandomState object for generating random variates.\n",
      "     |      \n",
      "     |      This can be either None, int, a RandomState instance, or a\n",
      "     |      np.random.Generator instance.\n",
      "     |      \n",
      "     |      If None (or np.random), use the RandomState singleton used by np.random.\n",
      "     |      If already a RandomState or Generator instance, use it.\n",
      "     |      If an int, use a new RandomState instance seeded with seed.\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(x, dist='norm')\n",
      "        Anderson-Darling test for data coming from a particular distribution.\n",
      "        \n",
      "        The Anderson-Darling test tests the null hypothesis that a sample is\n",
      "        drawn from a population that follows a particular distribution.\n",
      "        For the Anderson-Darling test, the critical values depend on\n",
      "        which distribution is being tested against.  This function works\n",
      "        for normal, exponential, logistic, or Gumbel (Extreme Value\n",
      "        Type I) distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Array of sample data.\n",
      "        dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r',\n",
      "            'extreme1'}, optional\n",
      "            The type of distribution to test against.  The default is 'norm'.\n",
      "            The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the\n",
      "            same distribution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Anderson-Darling test statistic.\n",
      "        critical_values : list\n",
      "            The critical values for this distribution.\n",
      "        significance_level : list\n",
      "            The significance levels for the corresponding critical values\n",
      "            in percents.  The function returns critical values for a\n",
      "            differing set of significance levels depending on the\n",
      "            distribution that is being tested against.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstest : The Kolmogorov-Smirnov test for goodness-of-fit.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Critical values provided are for the following significance levels:\n",
      "        \n",
      "        normal/exponenential\n",
      "            15%, 10%, 5%, 2.5%, 1%\n",
      "        logistic\n",
      "            25%, 10%, 5%, 2.5%, 1%, 0.5%\n",
      "        Gumbel\n",
      "            25%, 10%, 5%, 2.5%, 1%\n",
      "        \n",
      "        If the returned statistic is larger than these critical values then\n",
      "        for the corresponding significance level, the null hypothesis that\n",
      "        the data come from the chosen distribution can be rejected.\n",
      "        The returned statistic is referred to as 'A2' in the references.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
      "        .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and\n",
      "               Some Comparisons, Journal of the American Statistical Association,\n",
      "               Vol. 69, pp. 730-737.\n",
      "        .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit\n",
      "               Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,\n",
      "               pp. 357-369.\n",
      "        .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value\n",
      "               Distribution, Biometrika, Vol. 64, pp. 583-588.\n",
      "        .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference\n",
      "               to Tests for Exponentiality , Technical Report No. 262,\n",
      "               Department of Statistics, Stanford University, Stanford, CA.\n",
      "        .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution\n",
      "               Based on the Empirical Distribution Function, Biometrika, Vol. 66,\n",
      "               pp. 591-595.\n",
      "    \n",
      "    anderson_ksamp(samples, midrank=True)\n",
      "        The Anderson-Darling test for k-samples.\n",
      "        \n",
      "        The k-sample Anderson-Darling test is a modification of the\n",
      "        one-sample Anderson-Darling test. It tests the null hypothesis\n",
      "        that k-samples are drawn from the same population without having\n",
      "        to specify the distribution function of that population. The\n",
      "        critical values depend on the number of samples.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : sequence of 1-D array_like\n",
      "            Array of sample data in arrays.\n",
      "        midrank : bool, optional\n",
      "            Type of Anderson-Darling test which is computed. Default\n",
      "            (True) is the midrank test applicable to continuous and\n",
      "            discrete populations. If False, the right side empirical\n",
      "            distribution is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            Normalized k-sample Anderson-Darling test statistic.\n",
      "        critical_values : array\n",
      "            The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,\n",
      "            0.5%, 0.1%.\n",
      "        significance_level : float\n",
      "            An approximate significance level at which the null hypothesis for the\n",
      "            provided samples can be rejected. The value is floored / capped at\n",
      "            0.1% / 25%.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If less than 2 samples are provided, a sample is empty, or no\n",
      "            distinct observations are in the samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp : 2 sample Kolmogorov-Smirnov test\n",
      "        anderson : 1 sample Anderson-Darling test\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        [1]_ defines three versions of the k-sample Anderson-Darling test:\n",
      "        one for continuous distributions and two for discrete\n",
      "        distributions, in which ties between samples may occur. The\n",
      "        default of this routine is to compute the version based on the\n",
      "        midrank empirical distribution function. This test is applicable\n",
      "        to continuous and discrete data. If midrank is set to False, the\n",
      "        right side empirical distribution is used for a test for discrete\n",
      "        data. According to [1]_, the two discrete test statistics differ\n",
      "        only slightly if a few collisions due to round-off errors occur in\n",
      "        the test not adjusted for ties between samples.\n",
      "        \n",
      "        The critical values corresponding to the significance levels from 0.01\n",
      "        to 0.25 are taken from [1]_. p-values are floored / capped\n",
      "        at 0.1% / 25%. Since the range of critical values might be extended in\n",
      "        future releases, it is recommended not to test ``p == 0.25``, but rather\n",
      "        ``p >= 0.25`` (analogously for the lower bound).\n",
      "        \n",
      "        .. versionadded:: 0.14.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n",
      "               Anderson-Darling Tests, Journal of the American Statistical\n",
      "               Association, Vol. 82, pp. 918-924.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(314159)\n",
      "        \n",
      "        The null hypothesis that the two random samples come from the same\n",
      "        distribution can be rejected at the 5% level because the returned\n",
      "        test value is greater than the critical value for 5% (1.961) but\n",
      "        not at the 2.5% level. The interpolation gives an approximate\n",
      "        significance level of 3.2%:\n",
      "        \n",
      "        >>> stats.anderson_ksamp([np.random.normal(size=50),\n",
      "        ... np.random.normal(loc=0.5, size=30)])\n",
      "        (2.4615796189876105,\n",
      "          array([ 0.325,  1.226,  1.961,  2.718,  3.752, 4.592, 6.546]),\n",
      "          0.03176687568842282)\n",
      "        \n",
      "        \n",
      "        The null hypothesis cannot be rejected for three samples from an\n",
      "        identical distribution. The reported p-value (25%) has been capped and\n",
      "        may not be very accurate (since it corresponds to the value 0.449\n",
      "        whereas the statistic is -0.731):\n",
      "        \n",
      "        >>> stats.anderson_ksamp([np.random.normal(size=50),\n",
      "        ... np.random.normal(size=30), np.random.normal(size=20)])\n",
      "        (-0.73091722665244196,\n",
      "          array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856,\n",
      "          4.07210043, 5.56419101]),\n",
      "          0.25)\n",
      "    \n",
      "    ansari(x, y)\n",
      "        Perform the Ansari-Bradley test for equal scale parameters.\n",
      "        \n",
      "        The Ansari-Bradley test is a non-parametric test for the equality\n",
      "        of the scale parameter of the distributions from which two\n",
      "        samples were drawn.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of sample data.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Ansari-Bradley test statistic.\n",
      "        pvalue : float\n",
      "            The p-value of the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fligner : A non-parametric test for the equality of k variances\n",
      "        mood : A non-parametric test for the equality of two scale parameters\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The p-value given is exact when the sample sizes are both less than\n",
      "        55 and there are no ties, otherwise a normal approximation for the\n",
      "        p-value is used.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Sprent, Peter and N.C. Smeeton.  Applied nonparametric statistical\n",
      "               methods.  3rd ed. Chapman and Hall/CRC. 2001.  Section 5.8.2.\n",
      "    \n",
      "    bartlett(*args)\n",
      "        Perform Bartlett's test for equal variances.\n",
      "        \n",
      "        Bartlett's test tests the null hypothesis that all input samples\n",
      "        are from populations with equal variances.  For samples\n",
      "        from significantly non-normal populations, Levene's test\n",
      "        `levene` is more robust.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2,... : array_like\n",
      "            arrays of sample data.  Only 1d arrays are accepted, they may have\n",
      "            different lengths.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value of the test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fligner : A non-parametric test for the equality of k variances\n",
      "        levene : A robust parametric test for equality of k variances\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Conover et al. (1981) examine many of the existing parametric and\n",
      "        nonparametric tests by extensive simulations and they conclude that the\n",
      "        tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\n",
      "        superior in terms of robustness of departures from normality and power\n",
      "        ([3]_).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm\n",
      "        \n",
      "        .. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical\n",
      "                  Methods, Eighth Edition, Iowa State University Press.\n",
      "        \n",
      "        .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
      "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
      "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
      "               University.\n",
      "        \n",
      "        .. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical\n",
      "               Tests. Proceedings of the Royal Society of London. Series A,\n",
      "               Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Test whether or not the lists `a`, `b` and `c` come from populations\n",
      "        with equal variances.\n",
      "        \n",
      "        >>> from scipy.stats import bartlett\n",
      "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
      "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
      "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
      "        >>> stat, p = bartlett(a, b, c)\n",
      "        >>> p\n",
      "        1.1254782518834628e-05\n",
      "        \n",
      "        The very small p-value suggests that the populations do not have equal\n",
      "        variances.\n",
      "        \n",
      "        This is not surprising, given that the sample variance of `b` is much\n",
      "        larger than that of `a` and `c`:\n",
      "        \n",
      "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
      "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
      "    \n",
      "    bayes_mvs(data, alpha=0.9)\n",
      "        Bayesian confidence intervals for the mean, var, and std.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`.\n",
      "            Requires 2 or more data points.\n",
      "        alpha : float, optional\n",
      "            Probability that the returned confidence interval contains\n",
      "            the true parameter.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mean_cntr, var_cntr, std_cntr : tuple\n",
      "            The three results are for the mean, variance and standard deviation,\n",
      "            respectively.  Each result is a tuple of the form::\n",
      "        \n",
      "                (center, (lower, upper))\n",
      "        \n",
      "            with `center` the mean of the conditional pdf of the value given the\n",
      "            data, and `(lower, upper)` a confidence interval, centered on the\n",
      "            median, containing the estimate to a probability ``alpha``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        mvsdist\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Each tuple of mean, variance, and standard deviation estimates represent\n",
      "        the (center, (lower, upper)) with center the mean of the conditional pdf\n",
      "        of the value given the data and (lower, upper) is a confidence interval\n",
      "        centered on the median, containing the estimate to a probability\n",
      "        ``alpha``.\n",
      "        \n",
      "        Converts data to 1-D and assumes all data has the same mean and variance.\n",
      "        Uses Jeffrey's prior for variance and std.\n",
      "        \n",
      "        Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))``\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n",
      "        standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n",
      "        2006.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First a basic example to demonstrate the outputs:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> data = [6, 9, 12, 7, 8, 8, 13]\n",
      "        >>> mean, var, std = stats.bayes_mvs(data)\n",
      "        >>> mean\n",
      "        Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467))\n",
      "        >>> var\n",
      "        Variance(statistic=10.0, minmax=(3.176724206..., 24.45910382...))\n",
      "        >>> std\n",
      "        Std_dev(statistic=2.9724954732045084, minmax=(1.7823367265645143, 4.945614605014631))\n",
      "        \n",
      "        Now we generate some normally distributed random data, and get estimates of\n",
      "        mean and standard deviation with 95% confidence intervals for those\n",
      "        estimates:\n",
      "        \n",
      "        >>> n_samples = 100000\n",
      "        >>> data = stats.norm.rvs(size=n_samples)\n",
      "        >>> res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95)\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.hist(data, bins=100, density=True, label='Histogram of data')\n",
      "        >>> ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean')\n",
      "        >>> ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r',\n",
      "        ...            alpha=0.2, label=r'Estimated mean (95% limits)')\n",
      "        >>> ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale')\n",
      "        >>> ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2,\n",
      "        ...            label=r'Estimated scale (95% limits)')\n",
      "        \n",
      "        >>> ax.legend(fontsize=10)\n",
      "        >>> ax.set_xlim([-4, 4])\n",
      "        >>> ax.set_ylim([0, 0.5])\n",
      "        >>> plt.show()\n",
      "    \n",
      "    binned_statistic(x, values, statistic='mean', bins=10, range=None)\n",
      "        Compute a binned statistic for one or more sets of data.\n",
      "        \n",
      "        This is a generalization of a histogram function.  A histogram divides\n",
      "        the space into bins, and returns the count of the number of points in\n",
      "        each bin.  This function allows the computation of the sum, mean, median,\n",
      "        or other statistic of the values (or set of values) within each bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (N,) array_like\n",
      "            A sequence of values to be binned.\n",
      "        values : (N,) array_like or list of (N,) array_like\n",
      "            The data on which the statistic will be computed.  This must be\n",
      "            the same shape as `x`, or a set of sequences - each the same shape as\n",
      "            `x`.  If `values` is a set of sequences, the statistic will be computed\n",
      "            on each independently.\n",
      "        statistic : string or callable, optional\n",
      "            The statistic to compute (default is 'mean').\n",
      "            The following statistics are available:\n",
      "        \n",
      "              * 'mean' : compute the mean of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'std' : compute the standard deviation within each bin. This\n",
      "                is implicitly calculated with ddof=0.\n",
      "              * 'median' : compute the median of values for points within each\n",
      "                bin. Empty bins will be represented by NaN.\n",
      "              * 'count' : compute the count of points within each bin.  This is\n",
      "                identical to an unweighted histogram.  `values` array is not\n",
      "                referenced.\n",
      "              * 'sum' : compute the sum of values for points within each bin.\n",
      "                This is identical to a weighted histogram.\n",
      "              * 'min' : compute the minimum of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'max' : compute the maximum of values for point within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * function : a user-defined function which takes a 1D array of\n",
      "                values, and outputs a single numerical statistic. This function\n",
      "                will be called on the values in each bin.  Empty bins will be\n",
      "                represented by function([]), or NaN if this returns an error.\n",
      "        \n",
      "        bins : int or sequence of scalars, optional\n",
      "            If `bins` is an int, it defines the number of equal-width bins in the\n",
      "            given range (10 by default).  If `bins` is a sequence, it defines the\n",
      "            bin edges, including the rightmost edge, allowing for non-uniform bin\n",
      "            widths.  Values in `x` that are smaller than lowest bin edge are\n",
      "            assigned to bin number 0, values beyond the highest bin are assigned to\n",
      "            ``bins[-1]``.  If the bin edges are specified, the number of bins will\n",
      "            be, (nx = len(bins)-1).\n",
      "        range : (float, float) or [(float, float)], optional\n",
      "            The lower and upper range of the bins.  If not provided, range\n",
      "            is simply ``(x.min(), x.max())``.  Values outside the range are\n",
      "            ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : array\n",
      "            The values of the selected statistic in each bin.\n",
      "        bin_edges : array of dtype float\n",
      "            Return the bin edges ``(length(statistic)+1)``.\n",
      "        binnumber: 1-D ndarray of ints\n",
      "            Indices of the bins (corresponding to `bin_edges`) in which each value\n",
      "            of `x` belongs.  Same length as `values`.  A binnumber of `i` means the\n",
      "            corresponding value is between (bin_edges[i-1], bin_edges[i]).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.digitize, numpy.histogram, binned_statistic_2d, binned_statistic_dd\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All but the last (righthand-most) bin is half-open.  In other words, if\n",
      "        `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n",
      "        but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n",
      "        ``[3, 4]``, which *includes* 4.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        First some basic examples:\n",
      "        \n",
      "        Create two evenly spaced bins in the range of the given sample, and sum the\n",
      "        corresponding values in each of those bins:\n",
      "        \n",
      "        >>> values = [1.0, 1.0, 2.0, 1.5, 3.0]\n",
      "        >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n",
      "        BinnedStatisticResult(statistic=array([4. , 4.5]),\n",
      "                bin_edges=array([1., 4., 7.]), binnumber=array([1, 1, 1, 2, 2]))\n",
      "        \n",
      "        Multiple arrays of values can also be passed.  The statistic is calculated\n",
      "        on each set independently:\n",
      "        \n",
      "        >>> values = [[1.0, 1.0, 2.0, 1.5, 3.0], [2.0, 2.0, 4.0, 3.0, 6.0]]\n",
      "        >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n",
      "        BinnedStatisticResult(statistic=array([[4. , 4.5],\n",
      "               [8. , 9. ]]), bin_edges=array([1., 4., 7.]),\n",
      "               binnumber=array([1, 1, 1, 2, 2]))\n",
      "        \n",
      "        >>> stats.binned_statistic([1, 2, 1, 2, 4], np.arange(5), statistic='mean',\n",
      "        ...                        bins=3)\n",
      "        BinnedStatisticResult(statistic=array([1., 2., 4.]),\n",
      "                bin_edges=array([1., 2., 3., 4.]),\n",
      "                binnumber=array([1, 2, 1, 2, 3]))\n",
      "        \n",
      "        As a second example, we now generate some random data of sailing boat speed\n",
      "        as a function of wind speed, and then determine how fast our boat is for\n",
      "        certain wind speeds:\n",
      "        \n",
      "        >>> windspeed = 8 * np.random.rand(500)\n",
      "        >>> boatspeed = .3 * windspeed**.5 + .2 * np.random.rand(500)\n",
      "        >>> bin_means, bin_edges, binnumber = stats.binned_statistic(windspeed,\n",
      "        ...                 boatspeed, statistic='median', bins=[1,2,3,4,5,6,7])\n",
      "        >>> plt.figure()\n",
      "        >>> plt.plot(windspeed, boatspeed, 'b.', label='raw data')\n",
      "        >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,\n",
      "        ...            label='binned statistic of data')\n",
      "        >>> plt.legend()\n",
      "        \n",
      "        Now we can use ``binnumber`` to select all datapoints with a windspeed\n",
      "        below 1:\n",
      "        \n",
      "        >>> low_boatspeed = boatspeed[binnumber == 0]\n",
      "        \n",
      "        As a final example, we will use ``bin_edges`` and ``binnumber`` to make a\n",
      "        plot of a distribution that shows the mean and distribution around that\n",
      "        mean per bin, on top of a regular histogram and the probability\n",
      "        distribution function:\n",
      "        \n",
      "        >>> x = np.linspace(0, 5, num=500)\n",
      "        >>> x_pdf = stats.maxwell.pdf(x)\n",
      "        >>> samples = stats.maxwell.rvs(size=10000)\n",
      "        \n",
      "        >>> bin_means, bin_edges, binnumber = stats.binned_statistic(x, x_pdf,\n",
      "        ...         statistic='mean', bins=25)\n",
      "        >>> bin_width = (bin_edges[1] - bin_edges[0])\n",
      "        >>> bin_centers = bin_edges[1:] - bin_width/2\n",
      "        \n",
      "        >>> plt.figure()\n",
      "        >>> plt.hist(samples, bins=50, density=True, histtype='stepfilled',\n",
      "        ...          alpha=0.2, label='histogram of data')\n",
      "        >>> plt.plot(x, x_pdf, 'r-', label='analytical pdf')\n",
      "        >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=2,\n",
      "        ...            label='binned statistic of data')\n",
      "        >>> plt.plot((binnumber - 0.5) * bin_width, x_pdf, 'g.', alpha=0.5)\n",
      "        >>> plt.legend(fontsize=10)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    binned_statistic_2d(x, y, values, statistic='mean', bins=10, range=None, expand_binnumbers=False)\n",
      "        Compute a bidimensional binned statistic for one or more sets of data.\n",
      "        \n",
      "        This is a generalization of a histogram2d function.  A histogram divides\n",
      "        the space into bins, and returns the count of the number of points in\n",
      "        each bin.  This function allows the computation of the sum, mean, median,\n",
      "        or other statistic of the values (or set of values) within each bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (N,) array_like\n",
      "            A sequence of values to be binned along the first dimension.\n",
      "        y : (N,) array_like\n",
      "            A sequence of values to be binned along the second dimension.\n",
      "        values : (N,) array_like or list of (N,) array_like\n",
      "            The data on which the statistic will be computed.  This must be\n",
      "            the same shape as `x`, or a list of sequences - each with the same\n",
      "            shape as `x`.  If `values` is such a list, the statistic will be\n",
      "            computed on each independently.\n",
      "        statistic : string or callable, optional\n",
      "            The statistic to compute (default is 'mean').\n",
      "            The following statistics are available:\n",
      "        \n",
      "              * 'mean' : compute the mean of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'std' : compute the standard deviation within each bin. This\n",
      "                is implicitly calculated with ddof=0.\n",
      "              * 'median' : compute the median of values for points within each\n",
      "                bin. Empty bins will be represented by NaN.\n",
      "              * 'count' : compute the count of points within each bin.  This is\n",
      "                identical to an unweighted histogram.  `values` array is not\n",
      "                referenced.\n",
      "              * 'sum' : compute the sum of values for points within each bin.\n",
      "                This is identical to a weighted histogram.\n",
      "              * 'min' : compute the minimum of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'max' : compute the maximum of values for point within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * function : a user-defined function which takes a 1D array of\n",
      "                values, and outputs a single numerical statistic. This function\n",
      "                will be called on the values in each bin.  Empty bins will be\n",
      "                represented by function([]), or NaN if this returns an error.\n",
      "        \n",
      "        bins : int or [int, int] or array_like or [array, array], optional\n",
      "            The bin specification:\n",
      "        \n",
      "              * the number of bins for the two dimensions (nx = ny = bins),\n",
      "              * the number of bins in each dimension (nx, ny = bins),\n",
      "              * the bin edges for the two dimensions (x_edge = y_edge = bins),\n",
      "              * the bin edges in each dimension (x_edge, y_edge = bins).\n",
      "        \n",
      "            If the bin edges are specified, the number of bins will be,\n",
      "            (nx = len(x_edge)-1, ny = len(y_edge)-1).\n",
      "        \n",
      "        range : (2,2) array_like, optional\n",
      "            The leftmost and rightmost edges of the bins along each dimension\n",
      "            (if not specified explicitly in the `bins` parameters):\n",
      "            [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be\n",
      "            considered outliers and not tallied in the histogram.\n",
      "        expand_binnumbers : bool, optional\n",
      "            'False' (default): the returned `binnumber` is a shape (N,) array of\n",
      "            linearized bin indices.\n",
      "            'True': the returned `binnumber` is 'unraveled' into a shape (2,N)\n",
      "            ndarray, where each row gives the bin numbers in the corresponding\n",
      "            dimension.\n",
      "            See the `binnumber` returned value, and the `Examples` section.\n",
      "        \n",
      "            .. versionadded:: 0.17.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : (nx, ny) ndarray\n",
      "            The values of the selected statistic in each two-dimensional bin.\n",
      "        x_edge : (nx + 1) ndarray\n",
      "            The bin edges along the first dimension.\n",
      "        y_edge : (ny + 1) ndarray\n",
      "            The bin edges along the second dimension.\n",
      "        binnumber : (N,) array of ints or (2,N) ndarray of ints\n",
      "            This assigns to each element of `sample` an integer that represents the\n",
      "            bin in which this observation falls.  The representation depends on the\n",
      "            `expand_binnumbers` argument.  See `Notes` for details.\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.digitize, numpy.histogram2d, binned_statistic, binned_statistic_dd\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Binedges:\n",
      "        All but the last (righthand-most) bin is half-open.  In other words, if\n",
      "        `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n",
      "        but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n",
      "        ``[3, 4]``, which *includes* 4.\n",
      "        \n",
      "        `binnumber`:\n",
      "        This returned argument assigns to each element of `sample` an integer that\n",
      "        represents the bin in which it belongs.  The representation depends on the\n",
      "        `expand_binnumbers` argument. If 'False' (default): The returned\n",
      "        `binnumber` is a shape (N,) array of linearized indices mapping each\n",
      "        element of `sample` to its corresponding bin (using row-major ordering).\n",
      "        If 'True': The returned `binnumber` is a shape (2,N) ndarray where\n",
      "        each row indicates bin placements for each dimension respectively.  In each\n",
      "        dimension, a binnumber of `i` means the corresponding value is between\n",
      "        (D_edge[i-1], D_edge[i]), where 'D' is either 'x' or 'y'.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        Calculate the counts with explicit bin-edges:\n",
      "        \n",
      "        >>> x = [0.1, 0.1, 0.1, 0.6]\n",
      "        >>> y = [2.1, 2.6, 2.1, 2.1]\n",
      "        >>> binx = [0.0, 0.5, 1.0]\n",
      "        >>> biny = [2.0, 2.5, 3.0]\n",
      "        >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny])\n",
      "        >>> ret.statistic\n",
      "        array([[2., 1.],\n",
      "               [1., 0.]])\n",
      "        \n",
      "        The bin in which each sample is placed is given by the `binnumber`\n",
      "        returned parameter.  By default, these are the linearized bin indices:\n",
      "        \n",
      "        >>> ret.binnumber\n",
      "        array([5, 6, 5, 9])\n",
      "        \n",
      "        The bin indices can also be expanded into separate entries for each\n",
      "        dimension using the `expand_binnumbers` parameter:\n",
      "        \n",
      "        >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny],\n",
      "        ...                                 expand_binnumbers=True)\n",
      "        >>> ret.binnumber\n",
      "        array([[1, 1, 1, 2],\n",
      "               [1, 2, 1, 1]])\n",
      "        \n",
      "        Which shows that the first three elements belong in the xbin 1, and the\n",
      "        fourth into xbin 2; and so on for y.\n",
      "    \n",
      "    binned_statistic_dd(sample, values, statistic='mean', bins=10, range=None, expand_binnumbers=False, binned_statistic_result=None)\n",
      "        Compute a multidimensional binned statistic for a set of data.\n",
      "        \n",
      "        This is a generalization of a histogramdd function.  A histogram divides\n",
      "        the space into bins, and returns the count of the number of points in\n",
      "        each bin.  This function allows the computation of the sum, mean, median,\n",
      "        or other statistic of the values within each bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample : array_like\n",
      "            Data to histogram passed as a sequence of N arrays of length D, or\n",
      "            as an (N,D) array.\n",
      "        values : (N,) array_like or list of (N,) array_like\n",
      "            The data on which the statistic will be computed.  This must be\n",
      "            the same shape as `sample`, or a list of sequences - each with the\n",
      "            same shape as `sample`.  If `values` is such a list, the statistic\n",
      "            will be computed on each independently.\n",
      "        statistic : string or callable, optional\n",
      "            The statistic to compute (default is 'mean').\n",
      "            The following statistics are available:\n",
      "        \n",
      "              * 'mean' : compute the mean of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'median' : compute the median of values for points within each\n",
      "                bin. Empty bins will be represented by NaN.\n",
      "              * 'count' : compute the count of points within each bin.  This is\n",
      "                identical to an unweighted histogram.  `values` array is not\n",
      "                referenced.\n",
      "              * 'sum' : compute the sum of values for points within each bin.\n",
      "                This is identical to a weighted histogram.\n",
      "              * 'std' : compute the standard deviation within each bin. This\n",
      "                is implicitly calculated with ddof=0. If the number of values\n",
      "                within a given bin is 0 or 1, the computed standard deviation value\n",
      "                will be 0 for the bin.\n",
      "              * 'min' : compute the minimum of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'max' : compute the maximum of values for point within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * function : a user-defined function which takes a 1D array of\n",
      "                values, and outputs a single numerical statistic. This function\n",
      "                will be called on the values in each bin.  Empty bins will be\n",
      "                represented by function([]), or NaN if this returns an error.\n",
      "        \n",
      "        bins : sequence or positive int, optional\n",
      "            The bin specification must be in one of the following forms:\n",
      "        \n",
      "              * A sequence of arrays describing the bin edges along each dimension.\n",
      "              * The number of bins for each dimension (nx, ny, ... = bins).\n",
      "              * The number of bins for all dimensions (nx = ny = ... = bins).\n",
      "        range : sequence, optional\n",
      "            A sequence of lower and upper bin edges to be used if the edges are\n",
      "            not given explicitly in `bins`. Defaults to the minimum and maximum\n",
      "            values along each dimension.\n",
      "        expand_binnumbers : bool, optional\n",
      "            'False' (default): the returned `binnumber` is a shape (N,) array of\n",
      "            linearized bin indices.\n",
      "            'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n",
      "            ndarray, where each row gives the bin numbers in the corresponding\n",
      "            dimension.\n",
      "            See the `binnumber` returned value, and the `Examples` section of\n",
      "            `binned_statistic_2d`.\n",
      "        binned_statistic_result : binnedStatisticddResult\n",
      "            Result of a previous call to the function in order to reuse bin edges\n",
      "            and bin numbers with new values and/or a different statistic.\n",
      "            To reuse bin numbers, `expand_binnumbers` must have been set to False\n",
      "            (the default)\n",
      "        \n",
      "            .. versionadded:: 0.17.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : ndarray, shape(nx1, nx2, nx3,...)\n",
      "            The values of the selected statistic in each two-dimensional bin.\n",
      "        bin_edges : list of ndarrays\n",
      "            A list of D arrays describing the (nxi + 1) bin edges for each\n",
      "            dimension.\n",
      "        binnumber : (N,) array of ints or (D,N) ndarray of ints\n",
      "            This assigns to each element of `sample` an integer that represents the\n",
      "            bin in which this observation falls.  The representation depends on the\n",
      "            `expand_binnumbers` argument.  See `Notes` for details.\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Binedges:\n",
      "        All but the last (righthand-most) bin is half-open in each dimension.  In\n",
      "        other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n",
      "        ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n",
      "        last bin, however, is ``[3, 4]``, which *includes* 4.\n",
      "        \n",
      "        `binnumber`:\n",
      "        This returned argument assigns to each element of `sample` an integer that\n",
      "        represents the bin in which it belongs.  The representation depends on the\n",
      "        `expand_binnumbers` argument. If 'False' (default): The returned\n",
      "        `binnumber` is a shape (N,) array of linearized indices mapping each\n",
      "        element of `sample` to its corresponding bin (using row-major ordering).\n",
      "        If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n",
      "        each row indicates bin placements for each dimension respectively.  In each\n",
      "        dimension, a binnumber of `i` means the corresponding value is between\n",
      "        (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.mplot3d import Axes3D\n",
      "        \n",
      "        Take an array of 600 (x, y) coordinates as an example.\n",
      "        `binned_statistic_dd` can handle arrays of higher dimension `D`. But a plot\n",
      "        of dimension `D+1` is required.\n",
      "        \n",
      "        >>> mu = np.array([0., 1.])\n",
      "        >>> sigma = np.array([[1., -0.5],[-0.5, 1.5]])\n",
      "        >>> multinormal = stats.multivariate_normal(mu, sigma)\n",
      "        >>> data = multinormal.rvs(size=600, random_state=235412)\n",
      "        >>> data.shape\n",
      "        (600, 2)\n",
      "        \n",
      "        Create bins and count how many arrays fall in each bin:\n",
      "        \n",
      "        >>> N = 60\n",
      "        >>> x = np.linspace(-3, 3, N)\n",
      "        >>> y = np.linspace(-3, 4, N)\n",
      "        >>> ret = stats.binned_statistic_dd(data, np.arange(600), bins=[x, y],\n",
      "        ...                                 statistic='count')\n",
      "        >>> bincounts = ret.statistic\n",
      "        \n",
      "        Set the volume and the location of bars:\n",
      "        \n",
      "        >>> dx = x[1] - x[0]\n",
      "        >>> dy = y[1] - y[0]\n",
      "        >>> x, y = np.meshgrid(x[:-1]+dx/2, y[:-1]+dy/2)\n",
      "        >>> z = 0\n",
      "        \n",
      "        >>> bincounts = bincounts.ravel()\n",
      "        >>> x = x.ravel()\n",
      "        >>> y = y.ravel()\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111, projection='3d')\n",
      "        >>> with np.errstate(divide='ignore'):   # silence random axes3d warning\n",
      "        ...     ax.bar3d(x, y, z, dx, dy, bincounts)\n",
      "        \n",
      "        Reuse bin numbers and bin edges with new values:\n",
      "        \n",
      "        >>> ret2 = stats.binned_statistic_dd(data, -np.arange(600),\n",
      "        ...                                  binned_statistic_result=ret,\n",
      "        ...                                  statistic='mean')\n",
      "    \n",
      "    binom_test(x, n=None, p=0.5, alternative='two-sided')\n",
      "        Perform a test that the probability of success is p.\n",
      "        \n",
      "        This is an exact, two-sided test of the null hypothesis\n",
      "        that the probability of success in a Bernoulli experiment\n",
      "        is `p`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : int or array_like\n",
      "            The number of successes, or if x has length 2, it is the\n",
      "            number of successes and the number of failures.\n",
      "        n : int\n",
      "            The number of trials.  This is ignored if x gives both the\n",
      "            number of successes and failures.\n",
      "        p : float, optional\n",
      "            The hypothesized probability of success.  ``0 <= p <= 1``. The\n",
      "            default value is ``p = 0.5``.\n",
      "        alternative : {'two-sided', 'greater', 'less'}, optional\n",
      "            Indicates the alternative hypothesis. The default value is\n",
      "            'two-sided'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        p-value : float\n",
      "            The p-value of the hypothesis test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Binomial_test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        A car manufacturer claims that no more than 10% of their cars are unsafe.\n",
      "        15 cars are inspected for safety, 3 were found to be unsafe. Test the\n",
      "        manufacturer's claim:\n",
      "        \n",
      "        >>> stats.binom_test(3, n=15, p=0.1, alternative='greater')\n",
      "        0.18406106910639114\n",
      "        \n",
      "        The null hypothesis cannot be rejected at the 5% level of significance\n",
      "        because the returned p-value is greater than the critical value of 5%.\n",
      "    \n",
      "    boxcox(x, lmbda=None, alpha=None)\n",
      "        Return a dataset transformed by a Box-Cox power transformation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.  Must be positive 1-dimensional.  Must not be constant.\n",
      "        lmbda : {None, scalar}, optional\n",
      "            If `lmbda` is not None, do the transformation for that value.\n",
      "        \n",
      "            If `lmbda` is None, find the lambda that maximizes the log-likelihood\n",
      "            function and return it as the second output argument.\n",
      "        alpha : {None, float}, optional\n",
      "            If ``alpha`` is not None, return the ``100 * (1-alpha)%`` confidence\n",
      "            interval for `lmbda` as the third output argument.\n",
      "            Must be between 0.0 and 1.0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        boxcox : ndarray\n",
      "            Box-Cox power transformed array.\n",
      "        maxlog : float, optional\n",
      "            If the `lmbda` parameter is None, the second returned argument is\n",
      "            the lambda that maximizes the log-likelihood function.\n",
      "        (min_ci, max_ci) : tuple of float, optional\n",
      "            If `lmbda` parameter is None and ``alpha`` is not None, this returned\n",
      "            tuple of floats represents the minimum and maximum confidence limits\n",
      "            given ``alpha``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, boxcox_normplot, boxcox_normmax, boxcox_llf\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Box-Cox transform is given by::\n",
      "        \n",
      "            y = (x**lmbda - 1) / lmbda,  for lmbda > 0\n",
      "                log(x),                  for lmbda = 0\n",
      "        \n",
      "        `boxcox` requires the input data to be positive.  Sometimes a Box-Cox\n",
      "        transformation provides a shift parameter to achieve this; `boxcox` does\n",
      "        not.  Such a shift parameter is equivalent to adding a positive constant to\n",
      "        `x` before calling `boxcox`.\n",
      "        \n",
      "        The confidence limits returned when ``alpha`` is provided give the interval\n",
      "        where:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            llf(\\hat{\\lambda}) - llf(\\lambda) < \\frac{1}{2}\\chi^2(1 - \\alpha, 1),\n",
      "        \n",
      "        with ``llf`` the log-likelihood function and :math:`\\chi^2` the chi-squared\n",
      "        function.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal of the\n",
      "        Royal Statistical Society B, 26, 211-252 (1964).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        We generate some random variates from a non-normal distribution and make a\n",
      "        probability plot for it, to show it is non-normal in the tails:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax1 = fig.add_subplot(211)\n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
      "        >>> ax1.set_xlabel('')\n",
      "        >>> ax1.set_title('Probplot against normal distribution')\n",
      "        \n",
      "        We now use `boxcox` to transform the data so it's closest to normal:\n",
      "        \n",
      "        >>> ax2 = fig.add_subplot(212)\n",
      "        >>> xt, _ = stats.boxcox(x)\n",
      "        >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
      "        >>> ax2.set_title('Probplot after Box-Cox transformation')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    boxcox_llf(lmb, data)\n",
      "        The boxcox log-likelihood function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        lmb : scalar\n",
      "            Parameter for Box-Cox transformation.  See `boxcox` for details.\n",
      "        data : array_like\n",
      "            Data to calculate Box-Cox log-likelihood for.  If `data` is\n",
      "            multi-dimensional, the log-likelihood is calculated along the first\n",
      "            axis.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        llf : float or ndarray\n",
      "            Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,\n",
      "            an array otherwise.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        boxcox, probplot, boxcox_normplot, boxcox_normmax\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Box-Cox log-likelihood function is defined here as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            llf = (\\lambda - 1) \\sum_i(\\log(x_i)) -\n",
      "                  N/2 \\log(\\sum_i (y_i - \\bar{y})^2 / N),\n",
      "        \n",
      "        where ``y`` is the Box-Cox transformed input data ``x``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
      "        >>> np.random.seed(1245)\n",
      "        \n",
      "        Generate some random variates and calculate Box-Cox log-likelihood values\n",
      "        for them for a range of ``lmbda`` values:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n",
      "        >>> lmbdas = np.linspace(-2, 10)\n",
      "        >>> llf = np.zeros(lmbdas.shape, dtype=float)\n",
      "        >>> for ii, lmbda in enumerate(lmbdas):\n",
      "        ...     llf[ii] = stats.boxcox_llf(lmbda, x)\n",
      "        \n",
      "        Also find the optimal lmbda value with `boxcox`:\n",
      "        \n",
      "        >>> x_most_normal, lmbda_optimal = stats.boxcox(x)\n",
      "        \n",
      "        Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n",
      "        horizontal line to check that that's really the optimum:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(lmbdas, llf, 'b.-')\n",
      "        >>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')\n",
      "        >>> ax.set_xlabel('lmbda parameter')\n",
      "        >>> ax.set_ylabel('Box-Cox log-likelihood')\n",
      "        \n",
      "        Now add some probability plots to show that where the log-likelihood is\n",
      "        maximized the data transformed with `boxcox` looks closest to normal:\n",
      "        \n",
      "        >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
      "        >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
      "        ...     xt = stats.boxcox(x, lmbda=lmbda)\n",
      "        ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
      "        ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
      "        ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
      "        ...     ax_inset.set_xticklabels([])\n",
      "        ...     ax_inset.set_yticklabels([])\n",
      "        ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    boxcox_normmax(x, brack=(-2.0, 2.0), method='pearsonr')\n",
      "        Compute optimal Box-Cox transform parameter for input data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        brack : 2-tuple, optional\n",
      "            The starting interval for a downhill bracket search with\n",
      "            `optimize.brent`.  Note that this is in most cases not critical; the\n",
      "            final result is allowed to be outside this bracket.\n",
      "        method : str, optional\n",
      "            The method to determine the optimal transform parameter (`boxcox`\n",
      "            ``lmbda`` parameter). Options are:\n",
      "        \n",
      "            'pearsonr'  (default)\n",
      "                Maximizes the Pearson correlation coefficient between\n",
      "                ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be\n",
      "                normally-distributed.\n",
      "        \n",
      "            'mle'\n",
      "                Minimizes the log-likelihood `boxcox_llf`.  This is the method used\n",
      "                in `boxcox`.\n",
      "        \n",
      "            'all'\n",
      "                Use all optimization methods available, and return all results.\n",
      "                Useful to compare different methods.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        maxlog : float or ndarray\n",
      "            The optimal transform parameter found.  An array instead of a scalar\n",
      "            for ``method='all'``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        boxcox, boxcox_llf, boxcox_normplot\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> np.random.seed(1234)  # make this example reproducible\n",
      "        \n",
      "        Generate some data and determine optimal ``lmbda`` in various ways:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, size=30) + 5\n",
      "        >>> y, lmax_mle = stats.boxcox(x)\n",
      "        >>> lmax_pearsonr = stats.boxcox_normmax(x)\n",
      "        \n",
      "        >>> lmax_mle\n",
      "        7.177...\n",
      "        >>> lmax_pearsonr\n",
      "        7.916...\n",
      "        >>> stats.boxcox_normmax(x, method='all')\n",
      "        array([ 7.91667384,  7.17718692])\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.boxcox_normplot(x, -10, 10, plot=ax)\n",
      "        >>> ax.axvline(lmax_mle, color='r')\n",
      "        >>> ax.axvline(lmax_pearsonr, color='g', ls='--')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    boxcox_normplot(x, la, lb, plot=None, N=80)\n",
      "        Compute parameters for a Box-Cox normality plot, optionally show it.\n",
      "        \n",
      "        A Box-Cox normality plot shows graphically what the best transformation\n",
      "        parameter is to use in `boxcox` to obtain a distribution that is close\n",
      "        to normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        la, lb : scalar\n",
      "            The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`\n",
      "            for Box-Cox transformations.  These are also the limits of the\n",
      "            horizontal axis of the plot if that is generated.\n",
      "        plot : object, optional\n",
      "            If given, plots the quantiles and least squares fit.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        N : int, optional\n",
      "            Number of points on the horizontal axis (equally distributed from\n",
      "            `la` to `lb`).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lmbdas : ndarray\n",
      "            The ``lmbda`` values for which a Box-Cox transform was done.\n",
      "        ppcc : ndarray\n",
      "            Probability Plot Correlelation Coefficient, as obtained from `probplot`\n",
      "            when fitting the Box-Cox transformed input `x` against a normal\n",
      "            distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Even if `plot` is given, the figure is not shown or saved by\n",
      "        `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n",
      "        should be used after calling `probplot`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Generate some non-normally distributed data, and create a Box-Cox plot:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)\n",
      "        \n",
      "        Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n",
      "        the same plot:\n",
      "        \n",
      "        >>> _, maxlog = stats.boxcox(x)\n",
      "        >>> ax.axvline(maxlog, color='r')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    brunnermunzel(x, y, alternative='two-sided', distribution='t', nan_policy='propagate')\n",
      "        Compute the Brunner-Munzel test on samples x and y.\n",
      "        \n",
      "        The Brunner-Munzel test is a nonparametric test of the null hypothesis that\n",
      "        when values are taken one by one from each group, the probabilities of\n",
      "        getting large values in both groups are equal.\n",
      "        Unlike the Wilcoxon-Mann-Whitney's U test, this does not require the\n",
      "        assumption of equivariance of two groups. Note that this does not assume\n",
      "        the distributions are same. This test works on two independent samples,\n",
      "        which may have different sizes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Array of samples, should be one-dimensional.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        distribution : {'t', 'normal'}, optional\n",
      "            Defines how to get the p-value.\n",
      "            The following options are available (default is 't'):\n",
      "        \n",
      "              * 't': get the p-value by t-distribution\n",
      "              * 'normal': get the p-value by standard normal distribution.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Brunner-Munzer W statistic.\n",
      "        pvalue : float\n",
      "            p-value assuming an t distribution. One-sided or\n",
      "            two-sided, depending on the choice of `alternative` and `distribution`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        mannwhitneyu : Mann-Whitney rank test on two samples.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Brunner and Munzel recommended to estimate the p-value by t-distribution\n",
      "        when the size of data is 50 or less. If the size is lower than 10, it would\n",
      "        be better to use permuted Brunner Munzel test (see [2]_).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brunner, E. and Munzel, U. \"The nonparametric Benhrens-Fisher\n",
      "               problem: Asymptotic theory and a small-sample approximation\".\n",
      "               Biometrical Journal. Vol. 42(2000): 17-25.\n",
      "        .. [2] Neubert, K. and Brunner, E. \"A studentized permutation test for the\n",
      "               non-parametric Behrens-Fisher problem\". Computational Statistics and\n",
      "               Data Analysis. Vol. 51(2007): 5192-5204.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]\n",
      "        >>> x2 = [3,3,4,3,1,2,3,1,1,5,4]\n",
      "        >>> w, p_value = stats.brunnermunzel(x1, x2)\n",
      "        >>> w\n",
      "        3.1374674823029505\n",
      "        >>> p_value\n",
      "        0.0057862086661515377\n",
      "    \n",
      "    chi2_contingency(observed, correction=True, lambda_=None)\n",
      "        Chi-square test of independence of variables in a contingency table.\n",
      "        \n",
      "        This function computes the chi-square statistic and p-value for the\n",
      "        hypothesis test of independence of the observed frequencies in the\n",
      "        contingency table [1]_ `observed`.  The expected frequencies are computed\n",
      "        based on the marginal sums under the assumption of independence; see\n",
      "        `scipy.stats.contingency.expected_freq`.  The number of degrees of\n",
      "        freedom is (expressed using numpy functions and attributes)::\n",
      "        \n",
      "            dof = observed.size - sum(observed.shape) + observed.ndim - 1\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        observed : array_like\n",
      "            The contingency table. The table contains the observed frequencies\n",
      "            (i.e. number of occurrences) in each category.  In the two-dimensional\n",
      "            case, the table is often described as an \"R x C table\".\n",
      "        correction : bool, optional\n",
      "            If True, *and* the degrees of freedom is 1, apply Yates' correction\n",
      "            for continuity.  The effect of the correction is to adjust each\n",
      "            observed value by 0.5 towards the corresponding expected value.\n",
      "        lambda_ : float or str, optional.\n",
      "            By default, the statistic computed in this test is Pearson's\n",
      "            chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n",
      "            Cressie-Read power divergence family [3]_ to be used instead.  See\n",
      "            `power_divergence` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        chi2 : float\n",
      "            The test statistic.\n",
      "        p : float\n",
      "            The p-value of the test\n",
      "        dof : int\n",
      "            Degrees of freedom\n",
      "        expected : ndarray, same shape as `observed`\n",
      "            The expected frequencies, based on the marginal sums of the table.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        contingency.expected_freq\n",
      "        fisher_exact\n",
      "        chisquare\n",
      "        power_divergence\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        An often quoted guideline for the validity of this calculation is that\n",
      "        the test should be used only if the observed and expected frequencies\n",
      "        in each cell are at least 5.\n",
      "        \n",
      "        This is a test for the independence of different categories of a\n",
      "        population. The test is only meaningful when the dimension of\n",
      "        `observed` is two or more.  Applying the test to a one-dimensional\n",
      "        table will always result in `expected` equal to `observed` and a\n",
      "        chi-square statistic equal to 0.\n",
      "        \n",
      "        This function does not handle masked arrays, because the calculation\n",
      "        does not make sense with missing values.\n",
      "        \n",
      "        Like stats.chisquare, this function computes a chi-square statistic;\n",
      "        the convenience this function provides is to figure out the expected\n",
      "        frequencies and degrees of freedom from the given contingency table.\n",
      "        If these were already known, and if the Yates' correction was not\n",
      "        required, one could use stats.chisquare.  That is, if one calls::\n",
      "        \n",
      "            chi2, p, dof, ex = chi2_contingency(obs, correction=False)\n",
      "        \n",
      "        then the following is true::\n",
      "        \n",
      "            (chi2, p) == stats.chisquare(obs.ravel(), f_exp=ex.ravel(),\n",
      "                                         ddof=obs.size - 1 - dof)\n",
      "        \n",
      "        The `lambda_` argument was added in version 0.13.0 of scipy.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Contingency table\",\n",
      "               https://en.wikipedia.org/wiki/Contingency_table\n",
      "        .. [2] \"Pearson's chi-squared test\",\n",
      "               https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
      "        .. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
      "               Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
      "               pp. 440-464.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        A two-way example (2 x 3):\n",
      "        \n",
      "        >>> from scipy.stats import chi2_contingency\n",
      "        >>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n",
      "        >>> chi2_contingency(obs)\n",
      "        (2.7777777777777777,\n",
      "         0.24935220877729619,\n",
      "         2,\n",
      "         array([[ 12.,  12.,  16.],\n",
      "                [ 18.,  18.,  24.]]))\n",
      "        \n",
      "        Perform the test using the log-likelihood ratio (i.e. the \"G-test\")\n",
      "        instead of Pearson's chi-squared statistic.\n",
      "        \n",
      "        >>> g, p, dof, expctd = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
      "        >>> g, p\n",
      "        (2.7688587616781319, 0.25046668010954165)\n",
      "        \n",
      "        A four-way example (2 x 2 x 2 x 2):\n",
      "        \n",
      "        >>> obs = np.array(\n",
      "        ...     [[[[12, 17],\n",
      "        ...        [11, 16]],\n",
      "        ...       [[11, 12],\n",
      "        ...        [15, 16]]],\n",
      "        ...      [[[23, 15],\n",
      "        ...        [30, 22]],\n",
      "        ...       [[14, 17],\n",
      "        ...        [15, 16]]]])\n",
      "        >>> chi2_contingency(obs)\n",
      "        (8.7584514426741897,\n",
      "         0.64417725029295503,\n",
      "         11,\n",
      "         array([[[[ 14.15462386,  14.15462386],\n",
      "                  [ 16.49423111,  16.49423111]],\n",
      "                 [[ 11.2461395 ,  11.2461395 ],\n",
      "                  [ 13.10500554,  13.10500554]]],\n",
      "                [[[ 19.5591166 ,  19.5591166 ],\n",
      "                  [ 22.79202844,  22.79202844]],\n",
      "                 [[ 15.54012004,  15.54012004],\n",
      "                  [ 18.10873492,  18.10873492]]]]))\n",
      "    \n",
      "    chisquare(f_obs, f_exp=None, ddof=0, axis=0)\n",
      "        Calculate a one-way chi-square test.\n",
      "        \n",
      "        The chi-square test tests the null hypothesis that the categorical data\n",
      "        has the given frequencies.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f_obs : array_like\n",
      "            Observed frequencies in each category.\n",
      "        f_exp : array_like, optional\n",
      "            Expected frequencies in each category.  By default the categories are\n",
      "            assumed to be equally likely.\n",
      "        ddof : int, optional\n",
      "            \"Delta degrees of freedom\": adjustment to the degrees of freedom\n",
      "            for the p-value.  The p-value is computed using a chi-squared\n",
      "            distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n",
      "            is the number of observed frequencies.  The default value of `ddof`\n",
      "            is 0.\n",
      "        axis : int or None, optional\n",
      "            The axis of the broadcast result of `f_obs` and `f_exp` along which to\n",
      "            apply the test.  If axis is None, all values in `f_obs` are treated\n",
      "            as a single data set.  Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        chisq : float or ndarray\n",
      "            The chi-squared test statistic.  The value is a float if `axis` is\n",
      "            None or `f_obs` and `f_exp` are 1-D.\n",
      "        p : float or ndarray\n",
      "            The p-value of the test.  The value is a float if `ddof` and the\n",
      "            return value `chisq` are scalars.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.power_divergence\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This test is invalid when the observed or expected frequencies in each\n",
      "        category are too small.  A typical rule is that all of the observed\n",
      "        and expected frequencies should be at least 5.\n",
      "        \n",
      "        The default degrees of freedom, k-1, are for the case when no parameters\n",
      "        of the distribution are estimated. If p parameters are estimated by\n",
      "        efficient maximum likelihood then the correct degrees of freedom are\n",
      "        k-1-p. If the parameters are estimated in a different way, then the\n",
      "        dof can be between k-1-p and k-1. However, it is also possible that\n",
      "        the asymptotic distribution is not chi-square, in which case this test\n",
      "        is not appropriate.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
      "               Statistics\". Chapter 8.\n",
      "               https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n",
      "        .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When just `f_obs` is given, it is assumed that the expected frequencies\n",
      "        are uniform and given by the mean of the observed frequencies.\n",
      "        \n",
      "        >>> from scipy.stats import chisquare\n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12])\n",
      "        (2.0, 0.84914503608460956)\n",
      "        \n",
      "        With `f_exp` the expected frequencies can be given.\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n",
      "        (3.5, 0.62338762774958223)\n",
      "        \n",
      "        When `f_obs` is 2-D, by default the test is applied to each column.\n",
      "        \n",
      "        >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
      "        >>> obs.shape\n",
      "        (6, 2)\n",
      "        >>> chisquare(obs)\n",
      "        (array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))\n",
      "        \n",
      "        By setting ``axis=None``, the test is applied to all data in the array,\n",
      "        which is equivalent to applying the test to the flattened array.\n",
      "        \n",
      "        >>> chisquare(obs, axis=None)\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        >>> chisquare(obs.ravel())\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        \n",
      "        `ddof` is the change to make to the default degrees of freedom.\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n",
      "        (2.0, 0.73575888234288467)\n",
      "        \n",
      "        The calculation of the p-values is done by broadcasting the\n",
      "        chi-squared statistic with `ddof`.\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n",
      "        (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n",
      "        \n",
      "        `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n",
      "        shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n",
      "        `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n",
      "        statistics, we use ``axis=1``:\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12],\n",
      "        ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n",
      "        ...           axis=1)\n",
      "        (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n",
      "    \n",
      "    circmean(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate')\n",
      "        Compute the circular mean for samples in a range.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : array_like\n",
      "            Input array.\n",
      "        high : float or int, optional\n",
      "            High boundary for circular mean range.  Default is ``2*pi``.\n",
      "        low : float or int, optional\n",
      "            Low boundary for circular mean range.  Default is 0.\n",
      "        axis : int, optional\n",
      "            Axis along which means are computed.  The default is to compute\n",
      "            the mean of the flattened array.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        circmean : float\n",
      "            Circular mean.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import circmean\n",
      "        >>> circmean([0.1, 2*np.pi+0.2, 6*np.pi+0.3])\n",
      "        0.2\n",
      "        \n",
      "        >>> from scipy.stats import circmean\n",
      "        >>> circmean([0.2, 1.4, 2.6], high = 1, low = 0)\n",
      "        0.4\n",
      "    \n",
      "    circstd(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate')\n",
      "        Compute the circular standard deviation for samples assumed to be in the\n",
      "        range [low to high].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : array_like\n",
      "            Input array.\n",
      "        high : float or int, optional\n",
      "            High boundary for circular standard deviation range.\n",
      "            Default is ``2*pi``.\n",
      "        low : float or int, optional\n",
      "            Low boundary for circular standard deviation range.  Default is 0.\n",
      "        axis : int, optional\n",
      "            Axis along which standard deviations are computed.  The default is\n",
      "            to compute the standard deviation of the flattened array.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        circstd : float\n",
      "            Circular standard deviation.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This uses a definition of circular standard deviation that in the limit of\n",
      "        small angles returns a number close to the 'linear' standard deviation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import circstd\n",
      "        >>> circstd([0, 0.1*np.pi/2, 0.001*np.pi, 0.03*np.pi/2])\n",
      "        0.063564063306\n",
      "    \n",
      "    circvar(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate')\n",
      "        Compute the circular variance for samples assumed to be in a range.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : array_like\n",
      "            Input array.\n",
      "        high : float or int, optional\n",
      "            High boundary for circular variance range.  Default is ``2*pi``.\n",
      "        low : float or int, optional\n",
      "            Low boundary for circular variance range.  Default is 0.\n",
      "        axis : int, optional\n",
      "            Axis along which variances are computed.  The default is to compute\n",
      "            the variance of the flattened array.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        circvar : float\n",
      "            Circular variance.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This uses a definition of circular variance that in the limit of small\n",
      "        angles returns a number close to the 'linear' variance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import circvar\n",
      "        >>> circvar([0, 2*np.pi/3, 5*np.pi/3])\n",
      "        2.19722457734\n",
      "    \n",
      "    combine_pvalues(pvalues, method='fisher', weights=None)\n",
      "        Combine p-values from independent tests bearing upon the same hypothesis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pvalues : array_like, 1-D\n",
      "            Array of p-values assumed to come from independent tests.\n",
      "        method : {'fisher', 'pearson', 'tippett', 'stouffer', 'mudholkar_george'}, optional\n",
      "            Name of method to use to combine p-values.\n",
      "            The following methods are available (default is 'fisher'):\n",
      "        \n",
      "              * 'fisher': Fisher's method (Fisher's combined probability test), the\n",
      "                sum of the logarithm of the p-values\n",
      "              * 'pearson': Pearson's method (similar to Fisher's but uses sum of the\n",
      "                complement of the p-values inside the logarithms)\n",
      "              * 'tippett': Tippett's method (minimum of p-values)\n",
      "              * 'stouffer': Stouffer's Z-score method\n",
      "              * 'mudholkar_george': the difference of Fisher's and Pearson's methods\n",
      "                divided by 2\n",
      "        weights : array_like, 1-D, optional\n",
      "            Optional array of weights used only for Stouffer's Z-score method.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic: float\n",
      "            The statistic calculated by the specified method.\n",
      "        pval: float\n",
      "            The combined p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Fisher's method (also known as Fisher's combined probability test) [1]_ uses\n",
      "        a chi-squared statistic to compute a combined p-value. The closely related\n",
      "        Stouffer's Z-score method [2]_ uses Z-scores rather than p-values. The\n",
      "        advantage of Stouffer's method is that it is straightforward to introduce\n",
      "        weights, which can make Stouffer's method more powerful than Fisher's\n",
      "        method when the p-values are from studies of different size [6]_ [7]_.\n",
      "        The Pearson's method uses :math:`log(1-p_i)` inside the sum whereas Fisher's\n",
      "        method uses :math:`log(p_i)` [4]_. For Fisher's and Pearson's method, the\n",
      "        sum of the logarithms is multiplied by -2 in the implementation. This\n",
      "        quantity has a chi-square distribution that determines the p-value. The\n",
      "        `mudholkar_george` method is the difference of the Fisher's and Pearson's\n",
      "        test statistics, each of which include the -2 factor [4]_. However, the\n",
      "        `mudholkar_george` method does not include these -2 factors. The test\n",
      "        statistic of `mudholkar_george` is the sum of logisitic random variables and\n",
      "        equation 3.6 in [3]_ is used to approximate the p-value based on Student's\n",
      "        t-distribution.\n",
      "        \n",
      "        Fisher's method may be extended to combine p-values from dependent tests\n",
      "        [5]_. Extensions such as Brown's method and Kost's method are not currently\n",
      "        implemented.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Fisher%27s_method\n",
      "        .. [2] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n",
      "        .. [3] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n",
      "               random variables.\" Metrika 30.1 (1983): 1-13.\n",
      "        .. [4] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n",
      "               combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n",
      "        .. [5] Whitlock, M. C. \"Combining probability from independent tests: the\n",
      "               weighted Z-method is superior to Fisher's approach.\" Journal of\n",
      "               Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n",
      "        .. [6] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n",
      "               for combining probabilities in meta-analysis.\" Journal of\n",
      "               Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n",
      "        .. [7] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n",
      "    \n",
      "    cumfreq(a, numbins=10, defaultreallimits=None, weights=None)\n",
      "        Return a cumulative frequency histogram, using the histogram function.\n",
      "        \n",
      "        A cumulative histogram is a mapping that counts the cumulative number of\n",
      "        observations in all of the bins up to the specified bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        numbins : int, optional\n",
      "            The number of bins to use for the histogram. Default is 10.\n",
      "        defaultreallimits : tuple (lower, upper), optional\n",
      "            The lower and upper values for the range of the histogram.\n",
      "            If no value is given, a range slightly larger than the range of the\n",
      "            values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n",
      "            where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n",
      "        weights : array_like, optional\n",
      "            The weights for each value in `a`. Default is None, which gives each\n",
      "            value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cumcount : ndarray\n",
      "            Binned values of cumulative frequency.\n",
      "        lowerlimit : float\n",
      "            Lower real limit\n",
      "        binsize : float\n",
      "            Width of each bin.\n",
      "        extrapoints : int\n",
      "            Extra points.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy import stats\n",
      "        >>> x = [1, 4, 2, 1, 3, 1]\n",
      "        >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n",
      "        >>> res.cumcount\n",
      "        array([ 1.,  2.,  3.,  3.])\n",
      "        >>> res.extrapoints\n",
      "        3\n",
      "        \n",
      "        Create a normal distribution with 1000 random values\n",
      "        \n",
      "        >>> rng = np.random.RandomState(seed=12345)\n",
      "        >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n",
      "        \n",
      "        Calculate cumulative frequencies\n",
      "        \n",
      "        >>> res = stats.cumfreq(samples, numbins=25)\n",
      "        \n",
      "        Calculate space of values for x\n",
      "        \n",
      "        >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n",
      "        ...                                  res.cumcount.size)\n",
      "        \n",
      "        Plot histogram and cumulative histogram\n",
      "        \n",
      "        >>> fig = plt.figure(figsize=(10, 4))\n",
      "        >>> ax1 = fig.add_subplot(1, 2, 1)\n",
      "        >>> ax2 = fig.add_subplot(1, 2, 2)\n",
      "        >>> ax1.hist(samples, bins=25)\n",
      "        >>> ax1.set_title('Histogram')\n",
      "        >>> ax2.bar(x, res.cumcount, width=res.binsize)\n",
      "        >>> ax2.set_title('Cumulative histogram')\n",
      "        >>> ax2.set_xlim([x.min(), x.max()])\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate')\n",
      "        Compute several descriptive statistics of the passed array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "           Input data.\n",
      "        axis : int or None, optional\n",
      "           Axis along which statistics are calculated. Default is 0.\n",
      "           If None, compute over the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom (only for variance).  Default is 1.\n",
      "        bias : bool, optional\n",
      "            If False, then the skewness and kurtosis calculations are corrected for\n",
      "            statistical bias.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nobs : int or ndarray of ints\n",
      "           Number of observations (length of data along `axis`).\n",
      "           When 'omit' is chosen as nan_policy, each column is counted separately.\n",
      "        minmax: tuple of ndarrays or floats\n",
      "           Minimum and maximum value of data array.\n",
      "        mean : ndarray or float\n",
      "           Arithmetic mean of data along axis.\n",
      "        variance : ndarray or float\n",
      "           Unbiased variance of the data along axis, denominator is number of\n",
      "           observations minus one.\n",
      "        skewness : ndarray or float\n",
      "           Skewness, based on moment calculations with denominator equal to\n",
      "           the number of observations, i.e. no degrees of freedom correction.\n",
      "        kurtosis : ndarray or float\n",
      "           Kurtosis (Fisher).  The kurtosis is normalized so that it is\n",
      "           zero for the normal distribution.  No degrees of freedom are used.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        skew, kurtosis\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(10)\n",
      "        >>> stats.describe(a)\n",
      "        DescribeResult(nobs=10, minmax=(0, 9), mean=4.5, variance=9.166666666666666,\n",
      "                       skewness=0.0, kurtosis=-1.2242424242424244)\n",
      "        >>> b = [[1, 2], [3, 4]]\n",
      "        >>> stats.describe(b)\n",
      "        DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n",
      "                       mean=array([2., 3.]), variance=array([2., 2.]),\n",
      "                       skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n",
      "    \n",
      "    energy_distance(u_values, v_values, u_weights=None, v_weights=None)\n",
      "        Compute the energy distance between two 1D distributions.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u_values, v_values : array_like\n",
      "            Values observed in the (empirical) distribution.\n",
      "        u_weights, v_weights : array_like, optional\n",
      "            Weight for each value. If unspecified, each value is assigned the same\n",
      "            weight.\n",
      "            `u_weights` (resp. `v_weights`) must have the same length as\n",
      "            `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
      "            must still be positive and finite so that the weights can be normalized\n",
      "            to sum to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance : float\n",
      "            The computed distance between the distributions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The energy distance between two distributions :math:`u` and :math:`v`, whose\n",
      "        respective CDFs are :math:`U` and :math:`V`, equals to:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D(u, v) = \\left( 2\\mathbb E|X - Y| - \\mathbb E|X - X'| -\n",
      "            \\mathbb E|Y - Y'| \\right)^{1/2}\n",
      "        \n",
      "        where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are\n",
      "        independent random variables whose probability distribution is :math:`u`\n",
      "        (resp. :math:`v`).\n",
      "        \n",
      "        As shown in [2]_, for one-dimensional real-valued variables, the energy\n",
      "        distance is linked to the non-distribution-free version of the Cramer-von\n",
      "        Mises distance:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D(u, v) = \\sqrt{2} l_2(u, v) = \\left( 2 \\int_{-\\infty}^{+\\infty} (U-V)^2\n",
      "            \\right)^{1/2}\n",
      "        \n",
      "        Note that the common Cramer-von Mises criterion uses the distribution-free\n",
      "        version of the distance. See [2]_ (section 2), for more details about both\n",
      "        versions of the distance.\n",
      "        \n",
      "        The input distributions can be empirical, therefore coming from samples\n",
      "        whose values are effectively inputs of the function, or they can be seen as\n",
      "        generalized functions, in which case they are weighted sums of Dirac delta\n",
      "        functions located at the specified values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Energy distance\", https://en.wikipedia.org/wiki/Energy_distance\n",
      "        .. [2] Szekely \"E-statistics: The energy of statistical samples.\" Bowling\n",
      "               Green State University, Department of Mathematics and Statistics,\n",
      "               Technical Report 02-16 (2002).\n",
      "        .. [3] Rizzo, Szekely \"Energy distance.\" Wiley Interdisciplinary Reviews:\n",
      "               Computational Statistics, 8(1):27-38 (2015).\n",
      "        .. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n",
      "               Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n",
      "               Gradients\" (2017). :arXiv:`1705.10743`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import energy_distance\n",
      "        >>> energy_distance([0], [2])\n",
      "        2.0000000000000004\n",
      "        >>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])\n",
      "        1.0000000000000002\n",
      "        >>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],\n",
      "        ...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])\n",
      "        0.88003340976158217\n",
      "    \n",
      "    entropy(pk, qk=None, base=None, axis=0)\n",
      "        Calculate the entropy of a distribution for given probability values.\n",
      "        \n",
      "        If only probabilities `pk` are given, the entropy is calculated as\n",
      "        ``S = -sum(pk * log(pk), axis=axis)``.\n",
      "        \n",
      "        If `qk` is not None, then compute the Kullback-Leibler divergence\n",
      "        ``S = sum(pk * log(pk / qk), axis=axis)``.\n",
      "        \n",
      "        This routine will normalize `pk` and `qk` if they don't sum to 1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pk : sequence\n",
      "            Defines the (discrete) distribution. ``pk[i]`` is the (possibly\n",
      "            unnormalized) probability of event ``i``.\n",
      "        qk : sequence, optional\n",
      "            Sequence against which the relative entropy is computed. Should be in\n",
      "            the same format as `pk`.\n",
      "        base : float, optional\n",
      "            The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
      "        axis: int, optional\n",
      "            The axis along which the entropy is calculated. Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        S : float\n",
      "            The calculated entropy.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from scipy.stats import entropy\n",
      "        \n",
      "        Bernoulli trial with different p.\n",
      "        The outcome of a fair coin is the most uncertain:\n",
      "        \n",
      "        >>> entropy([1/2, 1/2], base=2)\n",
      "        1.0\n",
      "        \n",
      "        The outcome of a biased coin is less uncertain:\n",
      "        \n",
      "        >>> entropy([9/10, 1/10], base=2)\n",
      "        0.46899559358928117\n",
      "        \n",
      "        Relative entropy:\n",
      "        \n",
      "        >>> entropy([1/2, 1/2], qk=[9/10, 1/10])\n",
      "        0.5108256237659907\n",
      "    \n",
      "    epps_singleton_2samp(x, y, t=(0.4, 0.8))\n",
      "        Compute the Epps-Singleton (ES) test statistic.\n",
      "        \n",
      "        Test the null hypothesis that two samples have the same underlying\n",
      "        probability distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like\n",
      "            The two samples of observations to be tested. Input must not have more\n",
      "            than one dimension. Samples can have different lengths.\n",
      "        t : array-like, optional\n",
      "            The points (t1, ..., tn) where the empirical characteristic function is\n",
      "            to be evaluated. It should be positive distinct numbers. The default\n",
      "            value (0.4, 0.8) is proposed in [1]_. Input must not have more than\n",
      "            one dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The associated p-value based on the asymptotic chi2-distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp, anderson_ksamp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Testing whether two samples are generated by the same underlying\n",
      "        distribution is a classical question in statistics. A widely used test is\n",
      "        the Kolmogorov-Smirnov (KS) test which relies on the empirical\n",
      "        distribution function. Epps and Singleton introduce a test based on the\n",
      "        empirical characteristic function in [1]_.\n",
      "        \n",
      "        One advantage of the ES test compared to the KS test is that is does\n",
      "        not assume a continuous distribution. In [1]_, the authors conclude\n",
      "        that the test also has a higher power than the KS test in many\n",
      "        examples. They recommend the use of the ES test for discrete samples as\n",
      "        well as continuous samples with at least 25 observations each, whereas\n",
      "        `anderson_ksamp` is recommended for smaller sample sizes in the\n",
      "        continuous case.\n",
      "        \n",
      "        The p-value is computed from the asymptotic distribution of the test\n",
      "        statistic which follows a `chi2` distribution. If the sample size of both\n",
      "        `x` and `y` is below 25, the small sample correction proposed in [1]_ is\n",
      "        applied to the test statistic.\n",
      "        \n",
      "        The default values of `t` are determined in [1]_ by considering\n",
      "        various distributions and finding good values that lead to a high power\n",
      "        of the test in general. Table III in [1]_ gives the optimal values for\n",
      "        the distributions tested in that study. The values of `t` are scaled by\n",
      "        the semi-interquartile range in the implementation, see [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] T. W. Epps and K. J. Singleton, \"An omnibus test for the two-sample\n",
      "           problem using the empirical characteristic function\", Journal of\n",
      "           Statistical Computation and Simulation 26, p. 177--203, 1986.\n",
      "        \n",
      "        .. [2] S. J. Goerg and J. Kaiser, \"Nonparametric testing of distributions\n",
      "           - the Epps-Singleton two-sample test using the empirical characteristic\n",
      "           function\", The Stata Journal 9(3), p. 454--465, 2009.\n",
      "    \n",
      "    f_oneway(*args, axis=0)\n",
      "        Perform one-way ANOVA.\n",
      "        \n",
      "        The one-way ANOVA tests the null hypothesis that two or more groups have\n",
      "        the same population mean.  The test is applied to samples from two or\n",
      "        more groups, possibly with differing sizes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The sample measurements for each group.  There must be at least\n",
      "            two arguments.  If the arrays are multidimensional, then all the\n",
      "            dimensions of the array must be the same except for `axis`.\n",
      "        axis : int, optional\n",
      "            Axis of the input arrays along which the test is applied.\n",
      "            Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed F statistic of the test.\n",
      "        pvalue : float\n",
      "            The associated p-value from the F distribution.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        F_onewayConstantInputWarning\n",
      "            Raised if each of the input arrays is constant array.\n",
      "            In this case the F statistic is either infinite or isn't defined,\n",
      "            so ``np.inf`` or ``np.nan`` is returned.\n",
      "        \n",
      "        F_onewayBadInputSizesWarning\n",
      "            Raised if the length of any input array is 0, or if all the input\n",
      "            arrays have length 1.  ``np.nan`` is returned for the F statistic\n",
      "            and the p-value in these cases.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The ANOVA test has important assumptions that must be satisfied in order\n",
      "        for the associated p-value to be valid.\n",
      "        \n",
      "        1. The samples are independent.\n",
      "        2. Each sample is from a normally distributed population.\n",
      "        3. The population standard deviations of the groups are all equal.  This\n",
      "           property is known as homoscedasticity.\n",
      "        \n",
      "        If these assumptions are not true for a given set of data, it may still\n",
      "        be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`)\n",
      "        although with some loss of power.\n",
      "        \n",
      "        The length of each group must be at least one, and there must be at\n",
      "        least one group with length greater than one.  If these conditions\n",
      "        are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n",
      "        is returned.\n",
      "        \n",
      "        If each group contains constant values, and there exist at least two\n",
      "        groups with different values, the function generates a warning and\n",
      "        returns (``np.inf``, 0).\n",
      "        \n",
      "        If all values in all groups are the same, function generates a warning\n",
      "        and returns (``np.nan``, ``np.nan``).\n",
      "        \n",
      "        The algorithm is from Heiman [2]_, pp.394-7.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n",
      "               Chapter 14, 2014, http://vassarstats.net/textbook/\n",
      "        \n",
      "        .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n",
      "               integrated introduction for psychology\", Houghton, Mifflin and\n",
      "               Company, 2001.\n",
      "        \n",
      "        .. [3] G.H. McDonald, \"Handbook of Biological Statistics\", One-way ANOVA.\n",
      "               http://www.biostathandbook.com/onewayanova.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import f_oneway\n",
      "        \n",
      "        Here are some data [3]_ on a shell measurement (the length of the anterior\n",
      "        adductor muscle scar, standardized by dividing by length) in the mussel\n",
      "        Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n",
      "        Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n",
      "        much larger data set used in McDonald et al. (1991).\n",
      "        \n",
      "        >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n",
      "        ...              0.0659, 0.0923, 0.0836]\n",
      "        >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n",
      "        ...            0.0725]\n",
      "        >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n",
      "        >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n",
      "        ...            0.0689]\n",
      "        >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n",
      "        >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n",
      "        F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n",
      "        \n",
      "        `f_oneway` accepts multidimensional input arrays.  When the inputs\n",
      "        are multidimensional and `axis` is not given, the test is performed\n",
      "        along the first axis of the input arrays.  For the following data, the\n",
      "        test is performed three times, once for each column.\n",
      "        \n",
      "        >>> a = np.array([[9.87, 9.03, 6.81],\n",
      "        ...               [7.18, 8.35, 7.00],\n",
      "        ...               [8.39, 7.58, 7.68],\n",
      "        ...               [7.45, 6.33, 9.35],\n",
      "        ...               [6.41, 7.10, 9.33],\n",
      "        ...               [8.00, 8.24, 8.44]])\n",
      "        >>> b = np.array([[6.35, 7.30, 7.16],\n",
      "        ...               [6.65, 6.68, 7.63],\n",
      "        ...               [5.72, 7.73, 6.72],\n",
      "        ...               [7.01, 9.19, 7.41],\n",
      "        ...               [7.75, 7.87, 8.30],\n",
      "        ...               [6.90, 7.97, 6.97]])\n",
      "        >>> c = np.array([[3.31, 8.77, 1.01],\n",
      "        ...               [8.25, 3.24, 3.62],\n",
      "        ...               [6.32, 8.81, 5.19],\n",
      "        ...               [7.48, 8.83, 8.91],\n",
      "        ...               [8.59, 6.01, 6.07],\n",
      "        ...               [3.07, 9.72, 7.48]])\n",
      "        >>> F, p = f_oneway(a, b, c)\n",
      "        >>> F\n",
      "        array([1.75676344, 0.03701228, 3.76439349])\n",
      "        >>> p\n",
      "        array([0.20630784, 0.96375203, 0.04733157])\n",
      "    \n",
      "    find_repeats(arr)\n",
      "        Find repeats and repeat counts.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arr : array_like\n",
      "            Input array. This is cast to float64.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        values : ndarray\n",
      "            The unique values from the (flattened) input that are repeated.\n",
      "        \n",
      "        counts : ndarray\n",
      "            Number of times the corresponding 'value' is repeated.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In numpy >= 1.9 `numpy.unique` provides similar functionality. The main\n",
      "        difference is that `find_repeats` only returns repeated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])\n",
      "        RepeatedResults(values=array([2.]), counts=array([4]))\n",
      "        \n",
      "        >>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])\n",
      "        RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))\n",
      "    \n",
      "    fisher_exact(table, alternative='two-sided')\n",
      "        Perform a Fisher exact test on a 2x2 contingency table.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        table : array_like of ints\n",
      "            A 2x2 contingency table.  Elements should be non-negative integers.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        oddsratio : float\n",
      "            This is prior odds ratio and not a posterior estimate.\n",
      "        p_value : float\n",
      "            P-value, the probability of obtaining a distribution at least as\n",
      "            extreme as the one that was actually observed, assuming that the\n",
      "            null hypothesis is true.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chi2_contingency : Chi-square test of independence of variables in a\n",
      "            contingency table.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The calculated odds ratio is different from the one R uses. This scipy\n",
      "        implementation returns the (more common) \"unconditional Maximum\n",
      "        Likelihood Estimate\", while R uses the \"conditional Maximum Likelihood\n",
      "        Estimate\".\n",
      "        \n",
      "        For tables with large numbers, the (inexact) chi-square test implemented\n",
      "        in the function `chi2_contingency` can also be used.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Say we spend a few days counting whales and sharks in the Atlantic and\n",
      "        Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the\n",
      "        Indian ocean 2 whales and 5 sharks. Then our contingency table is::\n",
      "        \n",
      "                    Atlantic  Indian\n",
      "            whales     8        2\n",
      "            sharks     1        5\n",
      "        \n",
      "        We use this table to find the p-value:\n",
      "        \n",
      "        >>> import scipy.stats as stats\n",
      "        >>> oddsratio, pvalue = stats.fisher_exact([[8, 2], [1, 5]])\n",
      "        >>> pvalue\n",
      "        0.0349...\n",
      "        \n",
      "        The probability that we would observe this or an even more imbalanced ratio\n",
      "        by chance is about 3.5%.  A commonly used significance level is 5%--if we\n",
      "        adopt that, we can therefore conclude that our observed imbalance is\n",
      "        statistically significant; whales prefer the Atlantic while sharks prefer\n",
      "        the Indian ocean.\n",
      "    \n",
      "    fligner(*args, **kwds)\n",
      "        Perform Fligner-Killeen test for equality of variance.\n",
      "        \n",
      "        Fligner's test tests the null hypothesis that all input samples\n",
      "        are from populations with equal variances.  Fligner-Killeen's test is\n",
      "        distribution free when populations are identical [2]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            Arrays of sample data.  Need not be the same length.\n",
      "        center : {'mean', 'median', 'trimmed'}, optional\n",
      "            Keyword argument controlling which function of the data is used in\n",
      "            computing the test statistic.  The default is 'median'.\n",
      "        proportiontocut : float, optional\n",
      "            When `center` is 'trimmed', this gives the proportion of data points\n",
      "            to cut from each end. (See `scipy.stats.trim_mean`.)\n",
      "            Default is 0.05.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bartlett : A parametric test for equality of k variances in normal samples\n",
      "        levene : A robust parametric test for equality of k variances\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As with Levene's test there are three variants of Fligner's test that\n",
      "        differ by the measure of central tendency used in the test.  See `levene`\n",
      "        for more information.\n",
      "        \n",
      "        Conover et al. (1981) examine many of the existing parametric and\n",
      "        nonparametric tests by extensive simulations and they conclude that the\n",
      "        tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\n",
      "        superior in terms of robustness of departures from normality and power [3]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
      "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
      "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
      "               University.\n",
      "               https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf\n",
      "        \n",
      "        .. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample\n",
      "               tests for scale. 'Journal of the American Statistical Association.'\n",
      "               71(353), 210-213.\n",
      "        \n",
      "        .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
      "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
      "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
      "               University.\n",
      "        \n",
      "        .. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A\n",
      "               comparative study of tests for homogeneity of variances, with\n",
      "               applications to the outer continental shelf biding data.\n",
      "               Technometrics, 23(4), 351-361.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Test whether or not the lists `a`, `b` and `c` come from populations\n",
      "        with equal variances.\n",
      "        \n",
      "        >>> from scipy.stats import fligner\n",
      "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
      "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
      "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
      "        >>> stat, p = fligner(a, b, c)\n",
      "        >>> p\n",
      "        0.00450826080004775\n",
      "        \n",
      "        The small p-value suggests that the populations do not have equal\n",
      "        variances.\n",
      "        \n",
      "        This is not surprising, given that the sample variance of `b` is much\n",
      "        larger than that of `a` and `c`:\n",
      "        \n",
      "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
      "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
      "    \n",
      "    friedmanchisquare(*args)\n",
      "        Compute the Friedman test for repeated measurements.\n",
      "        \n",
      "        The Friedman test tests the null hypothesis that repeated measurements of\n",
      "        the same individuals have the same distribution.  It is often used\n",
      "        to test for consistency among measurements obtained in different ways.\n",
      "        For example, if two measurement techniques are used on the same set of\n",
      "        individuals, the Friedman test can be used to determine if the two\n",
      "        measurement techniques are consistent.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        measurements1, measurements2, measurements3... : array_like\n",
      "            Arrays of measurements.  All of the arrays must have the same number\n",
      "            of elements.  At least 3 sets of measurements must be given.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic, correcting for ties.\n",
      "        pvalue : float\n",
      "            The associated p-value assuming that the test statistic has a chi\n",
      "            squared distribution.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Due to the assumption that the test statistic has a chi squared\n",
      "        distribution, the p-value is only reliable for n > 10 and more than\n",
      "        6 repeated measurements.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Friedman_test\n",
      "    \n",
      "    gmean(a, axis=0, dtype=None)\n",
      "        Compute the geometric mean along the specified axis.\n",
      "        \n",
      "        Return the geometric average of the array elements.\n",
      "        That is:  n-th root of (x1 * x2 * ... * xn)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the geometric mean is computed. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        dtype : dtype, optional\n",
      "            Type of the returned array and of the accumulator in which the\n",
      "            elements are summed. If dtype is not specified, it defaults to the\n",
      "            dtype of a, unless a has an integer dtype with a precision less than\n",
      "            that of the default platform integer. In that case, the default\n",
      "            platform integer is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        gmean : ndarray\n",
      "            See `dtype` parameter above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.mean : Arithmetic average\n",
      "        numpy.average : Weighted average\n",
      "        hmean : Harmonic mean\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The geometric average is computed over a single dimension of the input\n",
      "        array, axis=0 by default, or all values in the array if axis=None.\n",
      "        float64 intermediate and return values are used for integer inputs.\n",
      "        \n",
      "        Use masked arrays to ignore any non-finite values in the input or that\n",
      "        arise in the calculations such as Not a Number and infinity because masked\n",
      "        arrays automatically mask any non-finite values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gmean\n",
      "        >>> gmean([1, 4])\n",
      "        2.0\n",
      "        >>> gmean([1, 2, 3, 4, 5, 6, 7])\n",
      "        3.3800151591412964\n",
      "    \n",
      "    gstd(a, axis=0, ddof=1)\n",
      "        Calculate the geometric standard deviation of an array.\n",
      "        \n",
      "        The geometric standard deviation describes the spread of a set of numbers\n",
      "        where the geometric mean is preferred. It is a multiplicative factor, and\n",
      "        so a dimensionless quantity.\n",
      "        \n",
      "        It is defined as the exponent of the standard deviation of ``log(a)``.\n",
      "        Mathematically the population geometric standard deviation can be\n",
      "        evaluated as::\n",
      "        \n",
      "            gstd = exp(std(log(a)))\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            An array like object containing the sample data.\n",
      "        axis : int, tuple or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Degree of freedom correction in the calculation of the\n",
      "            geometric standard deviation. Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray or float\n",
      "            An array of the geometric standard deviation. If `axis` is None or `a`\n",
      "            is a 1d array a float is returned.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As the calculation requires the use of logarithms the geometric standard\n",
      "        deviation only supports strictly positive values. Any non-positive or\n",
      "        infinite values will raise a `ValueError`.\n",
      "        The geometric standard deviation is sometimes confused with the exponent of\n",
      "        the standard deviation, ``exp(std(a))``. Instead the geometric standard\n",
      "        deviation is ``exp(std(log(a)))``.\n",
      "        The default value for `ddof` is different to the default value (0) used\n",
      "        by other ddof containing functions, such as ``np.std`` and ``np.nanstd``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find the geometric standard deviation of a log-normally distributed sample.\n",
      "        Note that the standard deviation of the distribution is one, on a\n",
      "        log scale this evaluates to approximately ``exp(1)``.\n",
      "        \n",
      "        >>> from scipy.stats import gstd\n",
      "        >>> np.random.seed(123)\n",
      "        >>> sample = np.random.lognormal(mean=0, sigma=1, size=1000)\n",
      "        >>> gstd(sample)\n",
      "        2.7217860664589946\n",
      "        \n",
      "        Compute the geometric standard deviation of a multidimensional array and\n",
      "        of a given axis.\n",
      "        \n",
      "        >>> a = np.arange(1, 25).reshape(2, 3, 4)\n",
      "        >>> gstd(a, axis=None)\n",
      "        2.2944076136018947\n",
      "        >>> gstd(a, axis=2)\n",
      "        array([[1.82424757, 1.22436866, 1.13183117],\n",
      "               [1.09348306, 1.07244798, 1.05914985]])\n",
      "        >>> gstd(a, axis=(1,2))\n",
      "        array([2.12939215, 1.22120169])\n",
      "        \n",
      "        The geometric standard deviation further handles masked arrays.\n",
      "        \n",
      "        >>> a = np.arange(1, 25).reshape(2, 3, 4)\n",
      "        >>> ma = np.ma.masked_where(a > 16, a)\n",
      "        >>> ma\n",
      "        masked_array(\n",
      "          data=[[[1, 2, 3, 4],\n",
      "                 [5, 6, 7, 8],\n",
      "                 [9, 10, 11, 12]],\n",
      "                [[13, 14, 15, 16],\n",
      "                 [--, --, --, --],\n",
      "                 [--, --, --, --]]],\n",
      "          mask=[[[False, False, False, False],\n",
      "                 [False, False, False, False],\n",
      "                 [False, False, False, False]],\n",
      "                [[False, False, False, False],\n",
      "                 [ True,  True,  True,  True],\n",
      "                 [ True,  True,  True,  True]]],\n",
      "          fill_value=999999)\n",
      "        >>> gstd(ma, axis=2)\n",
      "        masked_array(\n",
      "          data=[[1.8242475707663655, 1.2243686572447428, 1.1318311657788478],\n",
      "                [1.0934830582350938, --, --]],\n",
      "          mask=[[False, False, False],\n",
      "                [False,  True,  True]],\n",
      "          fill_value=999999)\n",
      "    \n",
      "    hmean(a, axis=0, dtype=None)\n",
      "        Calculate the harmonic mean along the specified axis.\n",
      "        \n",
      "        That is:  n / (1/x1 + 1/x2 + ... + 1/xn)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array, masked array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the harmonic mean is computed. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        dtype : dtype, optional\n",
      "            Type of the returned array and of the accumulator in which the\n",
      "            elements are summed. If `dtype` is not specified, it defaults to the\n",
      "            dtype of `a`, unless `a` has an integer `dtype` with a precision less\n",
      "            than that of the default platform integer. In that case, the default\n",
      "            platform integer is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        hmean : ndarray\n",
      "            See `dtype` parameter above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.mean : Arithmetic average\n",
      "        numpy.average : Weighted average\n",
      "        gmean : Geometric mean\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The harmonic mean is computed over a single dimension of the input\n",
      "        array, axis=0 by default, or all values in the array if axis=None.\n",
      "        float64 intermediate and return values are used for integer inputs.\n",
      "        \n",
      "        Use masked arrays to ignore any non-finite values in the input or that\n",
      "        arise in the calculations such as Not a Number and infinity.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import hmean\n",
      "        >>> hmean([1, 4])\n",
      "        1.6000000000000001\n",
      "        >>> hmean([1, 2, 3, 4, 5, 6, 7])\n",
      "        2.6997245179063363\n",
      "    \n",
      "    iqr(x, axis=None, rng=(25, 75), scale=1.0, nan_policy='propagate', interpolation='linear', keepdims=False)\n",
      "        Compute the interquartile range of the data along the specified axis.\n",
      "        \n",
      "        The interquartile range (IQR) is the difference between the 75th and\n",
      "        25th percentile of the data. It is a measure of the dispersion\n",
      "        similar to standard deviation or variance, but is much more robust\n",
      "        against outliers [2]_.\n",
      "        \n",
      "        The ``rng`` parameter allows this function to compute other\n",
      "        percentile ranges than the actual IQR. For example, setting\n",
      "        ``rng=(0, 100)`` is equivalent to `numpy.ptp`.\n",
      "        \n",
      "        The IQR of an empty array is `np.nan`.\n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or sequence of int, optional\n",
      "            Axis along which the range is computed. The default is to\n",
      "            compute the IQR for the entire array.\n",
      "        rng : Two-element sequence containing floats in range of [0,100] optional\n",
      "            Percentiles over which to compute the range. Each must be\n",
      "            between 0 and 100, inclusive. The default is the true IQR:\n",
      "            `(25, 75)`. The order of the elements is not important.\n",
      "        scale : scalar or str, optional\n",
      "            The numerical value of scale will be divided out of the final\n",
      "            result. The following string values are recognized:\n",
      "        \n",
      "              * 'raw' : No scaling, just return the raw IQR.\n",
      "                **Deprecated!**  Use `scale=1` instead.\n",
      "              * 'normal' : Scale by\n",
      "                :math:`2 \\sqrt{2} erf^{-1}(\\frac{1}{2}) \\approx 1.349`.\n",
      "        \n",
      "            The default is 1.0. The use of scale='raw' is deprecated.\n",
      "            Array-like scale is also allowed, as long\n",
      "            as it broadcasts correctly to the output such that\n",
      "            ``out / scale`` is a valid operation. The output dimensions\n",
      "            depend on the input array, `x`, the `axis` argument, and the\n",
      "            `keepdims` flag.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}, optional\n",
      "            Specifies the interpolation method to use when the percentile\n",
      "            boundaries lie between two data points `i` and `j`.\n",
      "            The following options are available (default is 'linear'):\n",
      "        \n",
      "              * 'linear': `i + (j - i) * fraction`, where `fraction` is the\n",
      "                fractional part of the index surrounded by `i` and `j`.\n",
      "              * 'lower': `i`.\n",
      "              * 'higher': `j`.\n",
      "              * 'nearest': `i` or `j` whichever is nearest.\n",
      "              * 'midpoint': `(i + j) / 2`.\n",
      "        \n",
      "        keepdims : bool, optional\n",
      "            If this is set to `True`, the reduced axes are left in the\n",
      "            result as dimensions with size one. With this option, the result\n",
      "            will broadcast correctly against the original array `x`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        iqr : scalar or ndarray\n",
      "            If ``axis=None``, a scalar is returned. If the input contains\n",
      "            integers or floats of smaller precision than ``np.float64``, then the\n",
      "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
      "            the same as that of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.std, numpy.var\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is heavily dependent on the version of `numpy` that is\n",
      "        installed. Versions greater than 1.11.0b3 are highly recommended, as they\n",
      "        include a number of enhancements and fixes to `numpy.percentile` and\n",
      "        `numpy.nanpercentile` that affect the operation of this function. The\n",
      "        following modifications apply:\n",
      "        \n",
      "        Below 1.10.0 : `nan_policy` is poorly defined.\n",
      "            The default behavior of `numpy.percentile` is used for 'propagate'. This\n",
      "            is a hybrid of 'omit' and 'propagate' that mostly yields a skewed\n",
      "            version of 'omit' since NaNs are sorted to the end of the data. A\n",
      "            warning is raised if there are NaNs in the data.\n",
      "        Below 1.9.0: `numpy.nanpercentile` does not exist.\n",
      "            This means that `numpy.percentile` is used regardless of `nan_policy`\n",
      "            and a warning is issued. See previous item for a description of the\n",
      "            behavior.\n",
      "        Below 1.9.0: `keepdims` and `interpolation` are not supported.\n",
      "            The keywords get ignored with a warning if supplied with non-default\n",
      "            values. However, multiple axes are still supported.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Interquartile range\" https://en.wikipedia.org/wiki/Interquartile_range\n",
      "        .. [2] \"Robust measures of scale\" https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
      "        .. [3] \"Quantile\" https://en.wikipedia.org/wiki/Quantile\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import iqr\n",
      "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
      "        >>> x\n",
      "        array([[10,  7,  4],\n",
      "               [ 3,  2,  1]])\n",
      "        >>> iqr(x)\n",
      "        4.0\n",
      "        >>> iqr(x, axis=0)\n",
      "        array([ 3.5,  2.5,  1.5])\n",
      "        >>> iqr(x, axis=1)\n",
      "        array([ 3.,  1.])\n",
      "        >>> iqr(x, axis=1, keepdims=True)\n",
      "        array([[ 3.],\n",
      "               [ 1.]])\n",
      "    \n",
      "    itemfreq(*args, **kwds)\n",
      "        `itemfreq` is deprecated!\n",
      "        `itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "        \n",
      "        Return a 2-D array of item frequencies.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (N,) array_like\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        itemfreq : (K, 2) ndarray\n",
      "            A 2-D frequency table.  Column 1 contains sorted, unique values from\n",
      "            `a`, column 2 contains their respective counts.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([1, 1, 5, 0, 1, 2, 2, 0, 1, 4])\n",
      "        >>> stats.itemfreq(a)\n",
      "        array([[ 0.,  2.],\n",
      "               [ 1.,  4.],\n",
      "               [ 2.,  2.],\n",
      "               [ 4.,  1.],\n",
      "               [ 5.,  1.]])\n",
      "        >>> np.bincount(a)\n",
      "        array([2, 4, 2, 0, 1, 1])\n",
      "        \n",
      "        >>> stats.itemfreq(a/10.)\n",
      "        array([[ 0. ,  2. ],\n",
      "               [ 0.1,  4. ],\n",
      "               [ 0.2,  2. ],\n",
      "               [ 0.4,  1. ],\n",
      "               [ 0.5,  1. ]])\n",
      "    \n",
      "    jarque_bera(x)\n",
      "        Perform the Jarque-Bera goodness of fit test on sample data.\n",
      "        \n",
      "        The Jarque-Bera test tests whether the sample data has the skewness and\n",
      "        kurtosis matching a normal distribution.\n",
      "        \n",
      "        Note that this test only works for a large enough number of data samples\n",
      "        (>2000) as the test statistic asymptotically has a Chi-squared distribution\n",
      "        with 2 degrees of freedom.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Observations of a random variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        jb_value : float\n",
      "            The test statistic.\n",
      "        p : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Jarque, C. and Bera, A. (1980) \"Efficient tests for normality,\n",
      "               homoscedasticity and serial independence of regression residuals\",\n",
      "               6 Econometric Letters 255-259.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> x = np.random.normal(0, 1, 100000)\n",
      "        >>> jarque_bera_test = stats.jarque_bera(x)\n",
      "        >>> jarque_bera_test\n",
      "        Jarque_beraResult(statistic=4.716570798957913, pvalue=0.0945822550304295)\n",
      "        >>> jarque_bera_test.statistic\n",
      "        4.716570798957913\n",
      "        >>> jarque_bera_test.pvalue\n",
      "        0.0945822550304295\n",
      "    \n",
      "    kendalltau(x, y, initial_lexsort=None, nan_policy='propagate', method='auto')\n",
      "        Calculate Kendall's tau, a correlation measure for ordinal data.\n",
      "        \n",
      "        Kendall's tau is a measure of the correspondence between two rankings.\n",
      "        Values close to 1 indicate strong agreement, values close to -1 indicate\n",
      "        strong disagreement.  This is the 1945 \"tau-b\" version of Kendall's\n",
      "        tau [2]_, which can account for ties and which reduces to the 1938 \"tau-a\"\n",
      "        version [1]_ in absence of ties.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of rankings, of the same shape. If arrays are not 1-D, they will\n",
      "            be flattened to 1-D.\n",
      "        initial_lexsort : bool, optional\n",
      "            Unused (deprecated).\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
      "            Defines which method is used to calculate the p-value [5]_.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto': selects the appropriate method based on a trade-off between\n",
      "                speed and accuracy\n",
      "              * 'asymptotic': uses a normal approximation valid for large samples\n",
      "              * 'exact': computes the exact p-value, but can only be used if no ties\n",
      "                are present\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float\n",
      "           The tau statistic.\n",
      "        pvalue : float\n",
      "           The two-sided p-value for a hypothesis test whose null hypothesis is\n",
      "           an absence of association, tau = 0.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n",
      "        weightedtau : Computes a weighted version of Kendall's tau.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The definition of Kendall's tau that is used is [2]_::\n",
      "        \n",
      "          tau = (P - Q) / sqrt((P + Q + T) * (P + Q + U))\n",
      "        \n",
      "        where P is the number of concordant pairs, Q the number of discordant\n",
      "        pairs, T the number of ties only in `x`, and U the number of ties only in\n",
      "        `y`.  If a tie occurs for the same pair in both `x` and `y`, it is not\n",
      "        added to either T or U.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Maurice G. Kendall, \"A New Measure of Rank Correlation\", Biometrika\n",
      "               Vol. 30, No. 1/2, pp. 81-93, 1938.\n",
      "        .. [2] Maurice G. Kendall, \"The treatment of ties in ranking problems\",\n",
      "               Biometrika Vol. 33, No. 3, pp. 239-251. 1945.\n",
      "        .. [3] Gottfried E. Noether, \"Elements of Nonparametric Statistics\", John\n",
      "               Wiley & Sons, 1967.\n",
      "        .. [4] Peter M. Fenwick, \"A new data structure for cumulative frequency\n",
      "               tables\", Software: Practice and Experience, Vol. 24, No. 3,\n",
      "               pp. 327-336, 1994.\n",
      "        .. [5] Maurice G. Kendall, \"Rank Correlation Methods\" (4th Edition),\n",
      "               Charles Griffin & Co., 1970.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x1 = [12, 2, 1, 12, 2]\n",
      "        >>> x2 = [1, 4, 7, 1, 0]\n",
      "        >>> tau, p_value = stats.kendalltau(x1, x2)\n",
      "        >>> tau\n",
      "        -0.47140452079103173\n",
      "        >>> p_value\n",
      "        0.2827454599327748\n",
      "    \n",
      "    kruskal(*args, **kwargs)\n",
      "        Compute the Kruskal-Wallis H-test for independent samples.\n",
      "        \n",
      "        The Kruskal-Wallis H-test tests the null hypothesis that the population\n",
      "        median of all of the groups are equal.  It is a non-parametric version of\n",
      "        ANOVA.  The test works on 2 or more independent samples, which may have\n",
      "        different sizes.  Note that rejecting the null hypothesis does not\n",
      "        indicate which of the groups differs.  Post hoc comparisons between\n",
      "        groups are required to determine which groups are different.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "           Two or more arrays with the sample measurements can be given as\n",
      "           arguments.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "           The Kruskal-Wallis H statistic, corrected for ties.\n",
      "        pvalue : float\n",
      "           The p-value for the test using the assumption that H has a chi\n",
      "           square distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        f_oneway : 1-way ANOVA.\n",
      "        mannwhitneyu : Mann-Whitney rank test on two samples.\n",
      "        friedmanchisquare : Friedman test for repeated measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Due to the assumption that H has a chi square distribution, the number\n",
      "        of samples in each group must not be too small.  A typical rule is\n",
      "        that each sample must have at least 5 measurements.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n",
      "           One-Criterion Variance Analysis\", Journal of the American Statistical\n",
      "           Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n",
      "        .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = [1, 3, 5, 7, 9]\n",
      "        >>> y = [2, 4, 6, 8, 10]\n",
      "        >>> stats.kruskal(x, y)\n",
      "        KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
      "        \n",
      "        >>> x = [1, 1, 1]\n",
      "        >>> y = [2, 2, 2]\n",
      "        >>> z = [2, 2]\n",
      "        >>> stats.kruskal(x, y, z)\n",
      "        KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n",
      "    \n",
      "    ks_1samp(x, cdf, args=(), alternative='two-sided', mode='auto')\n",
      "        Performs the Kolmogorov-Smirnov test for goodness of fit.\n",
      "        \n",
      "        This performs a test of the distribution F(x) of an observed\n",
      "        random variable against a given distribution G(x). Under the null\n",
      "        hypothesis, the two distributions are identical, F(x)=G(x). The\n",
      "        alternative hypothesis can be either 'two-sided' (default), 'less'\n",
      "        or 'greater'. The KS test is only valid for continuous distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            a 1-D array of observations of iid random variables.\n",
      "        cdf : callable\n",
      "            callable used to calculate the cdf.\n",
      "        args : tuple, sequence, optional\n",
      "            Distribution parameters, used with `cdf`.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided, see explanation in Notes\n",
      "              * 'greater': one-sided, see explanation in Notes\n",
      "        mode : {'auto', 'exact', 'approx', 'asymp'}, optional\n",
      "            Defines the distribution used for calculating the p-value.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto' : selects one of the other options.\n",
      "              * 'exact' : uses the exact distribution of test statistic.\n",
      "              * 'approx' : approximates the two-sided probability with twice the one-sided probability\n",
      "              * 'asymp': uses asymptotic distribution of test statistic\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            KS test statistic, either D, D+ or D- (depending on the value of 'alternative')\n",
      "        pvalue :  float\n",
      "            One-tailed or two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp, kstest\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In the one-sided test, the alternative is that the empirical\n",
      "        cumulative distribution function of the random variable is \"less\"\n",
      "        or \"greater\" than the cumulative distribution function G(x) of the\n",
      "        hypothesis, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        >>> x = np.linspace(-15, 15, 9)\n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf)\n",
      "        (0.44435602715924361, 0.038850142705171065)\n",
      "        \n",
      "        >>> np.random.seed(987654321) # set random seed to get the same result\n",
      "        >>> stats.ks_1samp(stats.norm.rvs(size=100), stats.norm.cdf)\n",
      "        (0.058352892479417884, 0.8653960860778898)\n",
      "        \n",
      "        *Test against one-sided alternative hypothesis*\n",
      "        \n",
      "        Shift distribution to larger values, so that `` CDF(x) < norm.cdf(x)``:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> x = stats.norm.rvs(loc=0.2, size=100)\n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n",
      "        (0.12464329735846891, 0.040989164077641749)\n",
      "        \n",
      "        Reject equal distribution against alternative hypothesis: less\n",
      "        \n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf, alternative='greater')\n",
      "        (0.0072115233216311081, 0.98531158590396395)\n",
      "        \n",
      "        Don't reject equal distribution against alternative hypothesis: greater\n",
      "        \n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf)\n",
      "        (0.12464329735846891, 0.08197335233541582)\n",
      "        \n",
      "        Don't reject equal distribution against alternative hypothesis: two-sided\n",
      "        \n",
      "        *Testing t distributed random variables against normal distribution*\n",
      "        \n",
      "        With 100 degrees of freedom the t distribution looks close to the normal\n",
      "        distribution, and the K-S test does not reject the hypothesis that the\n",
      "        sample came from the normal distribution:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> stats.ks_1samp(stats.t.rvs(100,size=100), stats.norm.cdf)\n",
      "        (0.072018929165471257, 0.6505883498379312)\n",
      "        \n",
      "        With 3 degrees of freedom the t distribution looks sufficiently different\n",
      "        from the normal distribution, that we can reject the hypothesis that the\n",
      "        sample came from the normal distribution at the 10% level:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> stats.ks_1samp(stats.t.rvs(3,size=100), stats.norm.cdf)\n",
      "        (0.131016895759829, 0.058826222555312224)\n",
      "    \n",
      "    ks_2samp(data1, data2, alternative='two-sided', mode='auto')\n",
      "        Compute the Kolmogorov-Smirnov statistic on 2 samples.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that 2 independent samples\n",
      "        are drawn from the same continuous distribution.  The alternative hypothesis\n",
      "        can be either 'two-sided' (default), 'less' or 'greater'.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data1, data2 : array_like, 1-Dimensional\n",
      "            Two arrays of sample observations assumed to be drawn from a continuous\n",
      "            distribution, sample sizes can be different.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided, see explanation in Notes\n",
      "              * 'greater': one-sided, see explanation in Notes\n",
      "        mode : {'auto', 'exact', 'asymp'}, optional\n",
      "            Defines the method used for calculating the p-value.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n",
      "              * 'exact' : use exact distribution of test statistic\n",
      "              * 'asymp' : use asymptotic distribution of test statistic\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            KS statistic.\n",
      "        pvalue : float\n",
      "            Two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This tests whether 2 samples are drawn from the same distribution. Note\n",
      "        that, like in the case of the one-sample KS test, the distribution is\n",
      "        assumed to be continuous.\n",
      "        \n",
      "        In the one-sided test, the alternative is that the empirical\n",
      "        cumulative distribution function F(x) of the data1 variable is \"less\"\n",
      "        or \"greater\" than the empirical cumulative distribution function G(x)\n",
      "        of the data2 variable, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.\n",
      "        \n",
      "        If the KS statistic is small or the p-value is high, then we cannot\n",
      "        reject the hypothesis that the distributions of the two samples\n",
      "        are the same.\n",
      "        \n",
      "        If the mode is 'auto', the computation is exact if the sample sizes are\n",
      "        less than 10000.  For larger sizes, the computation uses the\n",
      "        Kolmogorov-Smirnov distributions to compute an approximate value.\n",
      "        \n",
      "        The 'two-sided' 'exact' computation computes the complementary probability\n",
      "        and then subtracts from 1.  As such, the minimum probability it can return\n",
      "        is about 1e-16.  While the algorithm itself is exact, numerical\n",
      "        errors may accumulate for large sample sizes.   It is most suited to\n",
      "        situations in which one of the sample sizes is only a few thousand.\n",
      "        \n",
      "        We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n",
      "               Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(12345678)  #fix random seed to get the same result\n",
      "        >>> n1 = 200  # size of first sample\n",
      "        >>> n2 = 300  # size of second sample\n",
      "        \n",
      "        For a different distribution, we can reject the null hypothesis since the\n",
      "        pvalue is below 1%:\n",
      "        \n",
      "        >>> rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1)\n",
      "        >>> rvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5)\n",
      "        >>> stats.ks_2samp(rvs1, rvs2)\n",
      "        (0.20833333333333334, 5.129279597781977e-05)\n",
      "        \n",
      "        For a slightly different distribution, we cannot reject the null hypothesis\n",
      "        at a 10% or lower alpha since the p-value at 0.144 is higher than 10%\n",
      "        \n",
      "        >>> rvs3 = stats.norm.rvs(size=n2, loc=0.01, scale=1.0)\n",
      "        >>> stats.ks_2samp(rvs1, rvs3)\n",
      "        (0.10333333333333333, 0.14691437867433876)\n",
      "        \n",
      "        For an identical distribution, we cannot reject the null hypothesis since\n",
      "        the p-value is high, 41%:\n",
      "        \n",
      "        >>> rvs4 = stats.norm.rvs(size=n2, loc=0.0, scale=1.0)\n",
      "        >>> stats.ks_2samp(rvs1, rvs4)\n",
      "        (0.07999999999999996, 0.41126949729859719)\n",
      "    \n",
      "    kstat(data, n=2)\n",
      "        Return the nth k-statistic (1<=n<=4 so far).\n",
      "        \n",
      "        The nth k-statistic k_n is the unique symmetric unbiased estimator of the\n",
      "        nth cumulant kappa_n.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input array. Note that n-D input gets flattened.\n",
      "        n : int, {1, 2, 3, 4}, optional\n",
      "            Default is equal to 2.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kstat : float\n",
      "            The nth k-statistic.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstatvar: Returns an unbiased estimator of the variance of the k-statistic.\n",
      "        moment: Returns the n-th central moment about the mean for a sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For a sample size n, the first few k-statistics are given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            k_{1} = \\mu\n",
      "            k_{2} = \\frac{n}{n-1} m_{2}\n",
      "            k_{3} = \\frac{ n^{2} } {(n-1) (n-2)} m_{3}\n",
      "            k_{4} = \\frac{ n^{2} [(n + 1)m_{4} - 3(n - 1) m^2_{2}]} {(n-1) (n-2) (n-3)}\n",
      "        \n",
      "        where :math:`\\mu` is the sample mean, :math:`m_2` is the sample\n",
      "        variance, and :math:`m_i` is the i-th sample central moment.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        http://mathworld.wolfram.com/k-Statistic.html\n",
      "        \n",
      "        http://mathworld.wolfram.com/Cumulant.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rndm = np.random.RandomState(1234)\n",
      "        \n",
      "        As sample size increases, n-th moment and n-th k-statistic converge to the\n",
      "        same number (although they aren't identical). In the case of the normal\n",
      "        distribution, they converge to zero.\n",
      "        \n",
      "        >>> for n in [2, 3, 4, 5, 6, 7]:\n",
      "        ...     x = rndm.normal(size=10**n)\n",
      "        ...     m, k = stats.moment(x, 3), stats.kstat(x, 3)\n",
      "        ...     print(\"%.3g %.3g %.3g\" % (m, k, m-k))\n",
      "        -0.631 -0.651 0.0194\n",
      "        0.0282 0.0283 -8.49e-05\n",
      "        -0.0454 -0.0454 1.36e-05\n",
      "        7.53e-05 7.53e-05 -2.26e-09\n",
      "        0.00166 0.00166 -4.99e-09\n",
      "        -2.88e-06 -2.88e-06 8.63e-13\n",
      "    \n",
      "    kstatvar(data, n=2)\n",
      "        Return an unbiased estimator of the variance of the k-statistic.\n",
      "        \n",
      "        See `kstat` for more details of the k-statistic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input array. Note that n-D input gets flattened.\n",
      "        n : int, {1, 2}, optional\n",
      "            Default is equal to 2.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kstatvar : float\n",
      "            The nth k-statistic variance.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstat: Returns the n-th k-statistic.\n",
      "        moment: Returns the n-th central moment about the mean for a sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The variances of the first few k-statistics are given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            var(k_{1}) = \\frac{\\kappa^2}{n}\n",
      "            var(k_{2}) = \\frac{\\kappa^4}{n} + \\frac{2\\kappa^2_{2}}{n - 1}\n",
      "            var(k_{3}) = \\frac{\\kappa^6}{n} + \\frac{9 \\kappa_2 \\kappa_4}{n - 1} +\n",
      "                         \\frac{9 \\kappa^2_{3}}{n - 1} +\n",
      "                         \\frac{6 n \\kappa^3_{2}}{(n-1) (n-2)}\n",
      "            var(k_{4}) = \\frac{\\kappa^8}{n} + \\frac{16 \\kappa_2 \\kappa_6}{n - 1} +\n",
      "                         \\frac{48 \\kappa_{3} \\kappa_5}{n - 1} +\n",
      "                         \\frac{34 \\kappa^2_{4}}{n-1} + \\frac{72 n \\kappa^2_{2} \\kappa_4}{(n - 1) (n - 2)} +\n",
      "                         \\frac{144 n \\kappa_{2} \\kappa^2_{3}}{(n - 1) (n - 2)} +\n",
      "                         \\frac{24 (n + 1) n \\kappa^4_{2}}{(n - 1) (n - 2) (n - 3)}\n",
      "    \n",
      "    kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='auto')\n",
      "        Performs the (one sample or two samples) Kolmogorov-Smirnov test for goodness of fit.\n",
      "        \n",
      "        The one-sample test performs a test of the distribution F(x) of an observed\n",
      "        random variable against a given distribution G(x). Under the null\n",
      "        hypothesis, the two distributions are identical, F(x)=G(x). The\n",
      "        alternative hypothesis can be either 'two-sided' (default), 'less'\n",
      "        or 'greater'. The KS test is only valid for continuous distributions.\n",
      "        The two-sample test tests whether the two independent samples are drawn\n",
      "        from the same continuous distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rvs : str, array_like, or callable\n",
      "            If an array, it should be a 1-D array of observations of random\n",
      "            variables.\n",
      "            If a callable, it should be a function to generate random variables;\n",
      "            it is required to have a keyword argument `size`.\n",
      "            If a string, it should be the name of a distribution in `scipy.stats`,\n",
      "            which will be used to generate random variables.\n",
      "        cdf : str, array_like or callable\n",
      "            If array_like, it should be a 1-D array of observations of random\n",
      "            variables, and the two-sample test is performed (and rvs must be array_like)\n",
      "            If a callable, that callable is used to calculate the cdf.\n",
      "            If a string, it should be the name of a distribution in `scipy.stats`,\n",
      "            which will be used as the cdf function.\n",
      "        args : tuple, sequence, optional\n",
      "            Distribution parameters, used if `rvs` or `cdf` are strings or callables.\n",
      "        N : int, optional\n",
      "            Sample size if `rvs` is string or callable.  Default is 20.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided, see explanation in Notes\n",
      "              * 'greater': one-sided, see explanation in Notes\n",
      "        mode : {'auto', 'exact', 'approx', 'asymp'}, optional\n",
      "            Defines the distribution used for calculating the p-value.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto' : selects one of the other options.\n",
      "              * 'exact' : uses the exact distribution of test statistic.\n",
      "              * 'approx' : approximates the two-sided probability with twice the one-sided probability\n",
      "              * 'asymp': uses asymptotic distribution of test statistic\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            KS test statistic, either D, D+ or D-.\n",
      "        pvalue :  float\n",
      "            One-tailed or two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In the one-sided test, the alternative is that the empirical\n",
      "        cumulative distribution function of the random variable is \"less\"\n",
      "        or \"greater\" than the cumulative distribution function G(x) of the\n",
      "        hypothesis, ``F(x)<=G(x)``, resp. ``F(x)>=G(x)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        >>> x = np.linspace(-15, 15, 9)\n",
      "        >>> stats.kstest(x, 'norm')\n",
      "        (0.44435602715924361, 0.038850142705171065)\n",
      "        \n",
      "        >>> np.random.seed(987654321) # set random seed to get the same result\n",
      "        >>> stats.kstest(stats.norm.rvs(size=100), stats.norm.cdf)\n",
      "        (0.058352892479417884, 0.8653960860778898)\n",
      "        \n",
      "        The above lines are equivalent to:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> stats.kstest(stats.norm.rvs, 'norm', N=100)\n",
      "        (0.058352892479417884, 0.8653960860778898)\n",
      "        \n",
      "        *Test against one-sided alternative hypothesis*\n",
      "        \n",
      "        Shift distribution to larger values, so that ``CDF(x) < norm.cdf(x)``:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> x = stats.norm.rvs(loc=0.2, size=100)\n",
      "        >>> stats.kstest(x, 'norm', alternative='less')\n",
      "        (0.12464329735846891, 0.040989164077641749)\n",
      "        \n",
      "        Reject equal distribution against alternative hypothesis: less\n",
      "        \n",
      "        >>> stats.kstest(x, 'norm', alternative='greater')\n",
      "        (0.0072115233216311081, 0.98531158590396395)\n",
      "        \n",
      "        Don't reject equal distribution against alternative hypothesis: greater\n",
      "        \n",
      "        >>> stats.kstest(x, 'norm')\n",
      "        (0.12464329735846891, 0.08197335233541582)\n",
      "        \n",
      "        *Testing t distributed random variables against normal distribution*\n",
      "        \n",
      "        With 100 degrees of freedom the t distribution looks close to the normal\n",
      "        distribution, and the K-S test does not reject the hypothesis that the\n",
      "        sample came from the normal distribution:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> stats.kstest(stats.t.rvs(100, size=100), 'norm')\n",
      "        (0.072018929165471257, 0.6505883498379312)\n",
      "        \n",
      "        With 3 degrees of freedom the t distribution looks sufficiently different\n",
      "        from the normal distribution, that we can reject the hypothesis that the\n",
      "        sample came from the normal distribution at the 10% level:\n",
      "        \n",
      "        >>> np.random.seed(987654321)\n",
      "        >>> stats.kstest(stats.t.rvs(3, size=100), 'norm')\n",
      "        (0.131016895759829, 0.058826222555312224)\n",
      "    \n",
      "    kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate')\n",
      "        Compute the kurtosis (Fisher or Pearson) of a dataset.\n",
      "        \n",
      "        Kurtosis is the fourth central moment divided by the square of the\n",
      "        variance. If Fisher's definition is used, then 3.0 is subtracted from\n",
      "        the result to give 0.0 for a normal distribution.\n",
      "        \n",
      "        If bias is False then the kurtosis is calculated using k statistics to\n",
      "        eliminate bias coming from biased moment estimators\n",
      "        \n",
      "        Use `kurtosistest` to see if result is close enough to normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array\n",
      "            Data for which the kurtosis is calculated.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the kurtosis is calculated. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        fisher : bool, optional\n",
      "            If True, Fisher's definition is used (normal ==> 0.0). If False,\n",
      "            Pearson's definition is used (normal ==> 3.0).\n",
      "        bias : bool, optional\n",
      "            If False, then the calculations are corrected for statistical bias.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kurtosis : array\n",
      "            The kurtosis of values along an axis. If all values are equal,\n",
      "            return -3 for Fisher's definition and 0 for Pearson's definition.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In Fisher's definiton, the kurtosis of the normal distribution is zero.\n",
      "        In the following example, the kurtosis is close to zero, because it was\n",
      "        calculated from the dataset, not from the continuous distribution.\n",
      "        \n",
      "        >>> from scipy.stats import norm, kurtosis\n",
      "        >>> data = norm.rvs(size=1000, random_state=3)\n",
      "        >>> kurtosis(data)\n",
      "        -0.06928694200380558\n",
      "        \n",
      "        The distribution with a higher kurtosis has a heavier tail.\n",
      "        The zero valued kurtosis of the normal distribution in Fisher's definition\n",
      "        can serve as a reference point.\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> import scipy.stats as stats\n",
      "        >>> from scipy.stats import kurtosis\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, 100)\n",
      "        >>> ax = plt.subplot()\n",
      "        >>> distnames = ['laplace', 'norm', 'uniform']\n",
      "        \n",
      "        >>> for distname in distnames:\n",
      "        ...     if distname == 'uniform':\n",
      "        ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n",
      "        ...     else:\n",
      "        ...         dist = getattr(stats, distname)\n",
      "        ...     data = dist.rvs(size=1000)\n",
      "        ...     kur = kurtosis(data, fisher=True)\n",
      "        ...     y = dist.pdf(x)\n",
      "        ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n",
      "        ...     ax.legend()\n",
      "        \n",
      "        The Laplace distribution has a heavier tail than the normal distribution.\n",
      "        The uniform distribution (which has negative kurtosis) has the thinnest\n",
      "        tail.\n",
      "    \n",
      "    kurtosistest(a, axis=0, nan_policy='propagate')\n",
      "        Test whether a dataset has normal kurtosis.\n",
      "        \n",
      "        This function tests the null hypothesis that the kurtosis\n",
      "        of the population from which the sample was drawn is that\n",
      "        of the normal distribution: ``kurtosis = 3(n-1)/(n+1)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array\n",
      "            Array of the sample data.\n",
      "        axis : int or None, optional\n",
      "           Axis along which to compute test. Default is 0. If None,\n",
      "           compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed z-score for this test.\n",
      "        pvalue : float\n",
      "            The two-sided p-value for the hypothesis test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Valid only for n>20. This function uses the method described in [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] see e.g. F. J. Anscombe, W. J. Glynn, \"Distribution of the kurtosis\n",
      "           statistic b2 for normal samples\", Biometrika, vol. 70, pp. 227-234, 1983.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import kurtosistest\n",
      "        >>> kurtosistest(list(range(20)))\n",
      "        KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.08804338332528348)\n",
      "        \n",
      "        >>> np.random.seed(28041990)\n",
      "        >>> s = np.random.normal(0, 1, 1000)\n",
      "        >>> kurtosistest(s)\n",
      "        KurtosistestResult(statistic=1.2317590987707365, pvalue=0.21803908613450895)\n",
      "    \n",
      "    levene(*args, **kwds)\n",
      "        Perform Levene test for equal variances.\n",
      "        \n",
      "        The Levene test tests the null hypothesis that all input samples\n",
      "        are from populations with equal variances.  Levene's test is an\n",
      "        alternative to Bartlett's test `bartlett` in the case where\n",
      "        there are significant deviations from normality.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The sample data, possibly with different lengths. Only one-dimensional\n",
      "            samples are accepted.\n",
      "        center : {'mean', 'median', 'trimmed'}, optional\n",
      "            Which function of the data to use in the test.  The default\n",
      "            is 'median'.\n",
      "        proportiontocut : float, optional\n",
      "            When `center` is 'trimmed', this gives the proportion of data points\n",
      "            to cut from each end. (See `scipy.stats.trim_mean`.)\n",
      "            Default is 0.05.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value for the test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Three variations of Levene's test are possible.  The possibilities\n",
      "        and their recommended usages are:\n",
      "        \n",
      "          * 'median' : Recommended for skewed (non-normal) distributions>\n",
      "          * 'mean' : Recommended for symmetric, moderate-tailed distributions.\n",
      "          * 'trimmed' : Recommended for heavy-tailed distributions.\n",
      "        \n",
      "        The test version using the mean was proposed in the original article\n",
      "        of Levene ([2]_) while the median and trimmed mean have been studied by\n",
      "        Brown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe\n",
      "        test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm\n",
      "        .. [2] Levene, H. (1960). In Contributions to Probability and Statistics:\n",
      "               Essays in Honor of Harold Hotelling, I. Olkin et al. eds.,\n",
      "               Stanford University Press, pp. 278-292.\n",
      "        .. [3] Brown, M. B. and Forsythe, A. B. (1974), Journal of the American\n",
      "               Statistical Association, 69, 364-367\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Test whether or not the lists `a`, `b` and `c` come from populations\n",
      "        with equal variances.\n",
      "        \n",
      "        >>> from scipy.stats import levene\n",
      "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
      "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
      "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
      "        >>> stat, p = levene(a, b, c)\n",
      "        >>> p\n",
      "        0.002431505967249681\n",
      "        \n",
      "        The small p-value suggests that the populations do not have equal\n",
      "        variances.\n",
      "        \n",
      "        This is not surprising, given that the sample variance of `b` is much\n",
      "        larger than that of `a` and `c`:\n",
      "        \n",
      "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
      "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
      "    \n",
      "    linregress(x, y=None)\n",
      "        Calculate a linear least-squares regression for two sets of measurements.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Two sets of measurements.  Both arrays should have the same length.  If\n",
      "            only `x` is given (and ``y=None``), then it must be a two-dimensional\n",
      "            array where one dimension has length 2.  The two sets of measurements\n",
      "            are then found by splitting the array along the length-2 dimension.  In\n",
      "            the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\n",
      "            equivalent to ``linregress(x[0], x[1])``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        slope : float\n",
      "            Slope of the regression line.\n",
      "        intercept : float\n",
      "            Intercept of the regression line.\n",
      "        rvalue : float\n",
      "            Correlation coefficient.\n",
      "        pvalue : float\n",
      "            Two-sided p-value for a hypothesis test whose null hypothesis is\n",
      "            that the slope is zero, using Wald Test with t-distribution of\n",
      "            the test statistic.\n",
      "        stderr : float\n",
      "            Standard error of the estimated gradient.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        :func:`scipy.optimize.curve_fit` : Use non-linear\n",
      "         least squares to fit a function to data.\n",
      "        :func:`scipy.optimize.leastsq` : Minimize the sum of\n",
      "         squares of a set of equations.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Missing values are considered pair-wise: if a value is missing in `x`,\n",
      "        the corresponding value in `y` is masked.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        Generate some data:\n",
      "        \n",
      "        >>> np.random.seed(12345678)\n",
      "        >>> x = np.random.random(10)\n",
      "        >>> y = 1.6*x + np.random.random(10)\n",
      "        \n",
      "        Perform the linear regression:\n",
      "        \n",
      "        >>> slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
      "        >>> print(\"slope: %f    intercept: %f\" % (slope, intercept))\n",
      "        slope: 1.944864    intercept: 0.268578\n",
      "        \n",
      "        To get coefficient of determination (R-squared):\n",
      "        \n",
      "        >>> print(\"R-squared: %f\" % r_value**2)\n",
      "        R-squared: 0.735498\n",
      "        \n",
      "        Plot the data along with the fitted line:\n",
      "        \n",
      "        >>> plt.plot(x, y, 'o', label='original data')\n",
      "        >>> plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Example for the case where only x is provided as a 2x2 array:\n",
      "        \n",
      "        >>> x = np.array([[0, 1], [0, 2]])\n",
      "        >>> r = stats.linregress(x)\n",
      "        >>> r.slope, r.intercept\n",
      "        (2.0, 0.0)\n",
      "    \n",
      "    mannwhitneyu(x, y, use_continuity=True, alternative=None)\n",
      "        Compute the Mann-Whitney rank test on samples x and y.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Array of samples, should be one-dimensional.\n",
      "        use_continuity : bool, optional\n",
      "                Whether a continuity correction (1/2.) should be taken into\n",
      "                account. Default is True.\n",
      "        alternative : {None, 'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is None):\n",
      "        \n",
      "              * None: computes p-value half the size of the 'two-sided' p-value and\n",
      "                a different U statistic. The default behavior is not the same as\n",
      "                using 'less' or 'greater'; it only exists for backward compatibility\n",
      "                and is deprecated.\n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        \n",
      "            Use of the None option is deprecated.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Mann-Whitney U statistic, equal to min(U for x, U for y) if\n",
      "            `alternative` is equal to None (deprecated; exists for backward\n",
      "            compatibility), and U for y otherwise.\n",
      "        pvalue : float\n",
      "            p-value assuming an asymptotic normal distribution. One-sided or\n",
      "            two-sided, depending on the choice of `alternative`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Use only when the number of observation in each sample is > 20 and\n",
      "        you have 2 independent samples of ranks. Mann-Whitney U is\n",
      "        significant if the u-obtained is LESS THAN or equal to the critical\n",
      "        value of U.\n",
      "        \n",
      "        This test corrects for ties and by default uses a continuity correction.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Mann-Whitney_U_test\n",
      "        \n",
      "        .. [2] H.B. Mann and D.R. Whitney, \"On a Test of Whether one of Two Random\n",
      "               Variables is Stochastically Larger than the Other,\" The Annals of\n",
      "               Mathematical Statistics, vol. 18, no. 1, pp. 50-60, 1947.\n",
      "    \n",
      "    median_abs_deviation(x, axis=0, center=<function median at 0x7fcfd6672790>, scale=1.0, nan_policy='propagate')\n",
      "        Compute the median absolute deviation of the data along the given axis.\n",
      "        \n",
      "        The median absolute deviation (MAD, [1]_) computes the median over the\n",
      "        absolute deviations from the median. It is a measure of dispersion\n",
      "        similar to the standard deviation but more robust to outliers [2]_.\n",
      "        \n",
      "        The MAD of an empty array is ``np.nan``.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the range is computed. Default is 0. If None, compute\n",
      "            the MAD over the entire array.\n",
      "        center : callable, optional\n",
      "            A function that will return the central value. The default is to use\n",
      "            np.median. Any user defined function used will need to have the\n",
      "            function signature ``func(arr, axis)``.\n",
      "        scale : scalar or str, optional\n",
      "            The numerical value of scale will be divided out of the final\n",
      "            result. The default is 1.0. The string \"normal\" is also accepted,\n",
      "            and results in `scale` being the inverse of the standard normal\n",
      "            quantile function at 0.75, which is approximately 0.67449.\n",
      "            Array-like scale is also allowed, as long as it broadcasts correctly\n",
      "            to the output such that ``out / scale`` is a valid operation. The\n",
      "            output dimensions depend on the input array, `x`, and the `axis`\n",
      "            argument.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mad : scalar or ndarray\n",
      "            If ``axis=None``, a scalar is returned. If the input contains\n",
      "            integers or floats of smaller precision than ``np.float64``, then the\n",
      "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
      "            the same as that of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n",
      "        scipy.stats.tstd, scipy.stats.tvar\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `center` argument only affects the calculation of the central value\n",
      "        around which the MAD is calculated. That is, passing in ``center=np.mean``\n",
      "        will calculate the MAD around the mean - it will not calculate the *mean*\n",
      "        absolute deviation.\n",
      "        \n",
      "        The input array may contain `inf`, but if `center` returns `inf`, the\n",
      "        corresponding MAD for that data will be `nan`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Median absolute deviation\",\n",
      "               https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
      "        .. [2] \"Robust measures of scale\",\n",
      "               https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When comparing the behavior of `median_abs_deviation` with ``np.std``,\n",
      "        the latter is affected when we change a single value of an array to have an\n",
      "        outlier value while the MAD hardly changes:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n",
      "        >>> x.std()\n",
      "        0.9973906394005013\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        0.82832610097857\n",
      "        >>> x[0] = 345.6\n",
      "        >>> x.std()\n",
      "        34.42304872314415\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        0.8323442311590675\n",
      "        \n",
      "        Axis handling example:\n",
      "        \n",
      "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
      "        >>> x\n",
      "        array([[10,  7,  4],\n",
      "               [ 3,  2,  1]])\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        array([3.5, 2.5, 1.5])\n",
      "        >>> stats.median_abs_deviation(x, axis=None)\n",
      "        2.0\n",
      "        \n",
      "        Scale normal example:\n",
      "        \n",
      "        >>> x = stats.norm.rvs(size=1000000, scale=2, random_state=123456)\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        1.3487398527041636\n",
      "        >>> stats.median_abs_deviation(x, scale='normal')\n",
      "        1.9996446978061115\n",
      "    \n",
      "    median_absolute_deviation(*args, **kwds)\n",
      "        `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "        \n",
      "        To preserve the existing default behavior, use\n",
      "        `scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "        The value 1.4826 is not numerically precise for scaling\n",
      "        with a normal distribution. For a numerically precise value, use\n",
      "        `scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "        \n",
      "        \n",
      "        Compute the median absolute deviation of the data along the given axis.\n",
      "        \n",
      "        The median absolute deviation (MAD, [1]_) computes the median over the\n",
      "        absolute deviations from the median. It is a measure of dispersion\n",
      "        similar to the standard deviation but more robust to outliers [2]_.\n",
      "        \n",
      "        The MAD of an empty array is ``np.nan``.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the range is computed. Default is 0. If None, compute\n",
      "            the MAD over the entire array.\n",
      "        center : callable, optional\n",
      "            A function that will return the central value. The default is to use\n",
      "            np.median. Any user defined function used will need to have the function\n",
      "            signature ``func(arr, axis)``.\n",
      "        scale : int, optional\n",
      "            The scaling factor applied to the MAD. The default scale (1.4826)\n",
      "            ensures consistency with the standard deviation for normally distributed\n",
      "            data.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mad : scalar or ndarray\n",
      "            If ``axis=None``, a scalar is returned. If the input contains\n",
      "            integers or floats of smaller precision than ``np.float64``, then the\n",
      "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
      "            the same as that of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n",
      "        scipy.stats.tstd, scipy.stats.tvar\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `center` argument only affects the calculation of the central value\n",
      "        around which the MAD is calculated. That is, passing in ``center=np.mean``\n",
      "        will calculate the MAD around the mean - it will not calculate the *mean*\n",
      "        absolute deviation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Median absolute deviation\",\n",
      "               https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
      "        .. [2] \"Robust measures of scale\",\n",
      "               https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When comparing the behavior of `median_absolute_deviation` with ``np.std``,\n",
      "        the latter is affected when we change a single value of an array to have an\n",
      "        outlier value while the MAD hardly changes:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n",
      "        >>> x.std()\n",
      "        0.9973906394005013\n",
      "        >>> stats.median_absolute_deviation(x)\n",
      "        1.2280762773108278\n",
      "        >>> x[0] = 345.6\n",
      "        >>> x.std()\n",
      "        34.42304872314415\n",
      "        >>> stats.median_absolute_deviation(x)\n",
      "        1.2340335571164334\n",
      "        \n",
      "        Axis handling example:\n",
      "        \n",
      "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
      "        >>> x\n",
      "        array([[10,  7,  4],\n",
      "               [ 3,  2,  1]])\n",
      "        >>> stats.median_absolute_deviation(x)\n",
      "        array([5.1891, 3.7065, 2.2239])\n",
      "        >>> stats.median_absolute_deviation(x, axis=None)\n",
      "        2.9652\n",
      "    \n",
      "    median_test(*args, **kwds)\n",
      "        Perform a Mood's median test.\n",
      "        \n",
      "        Test that two or more samples come from populations with the same median.\n",
      "        \n",
      "        Let ``n = len(args)`` be the number of samples.  The \"grand median\" of\n",
      "        all the data is computed, and a contingency table is formed by\n",
      "        classifying the values in each sample as being above or below the grand\n",
      "        median.  The contingency table, along with `correction` and `lambda_`,\n",
      "        are passed to `scipy.stats.chi2_contingency` to compute the test statistic\n",
      "        and p-value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The set of samples.  There must be at least two samples.\n",
      "            Each sample must be a one-dimensional sequence containing at least\n",
      "            one value.  The samples are not required to have the same length.\n",
      "        ties : str, optional\n",
      "            Determines how values equal to the grand median are classified in\n",
      "            the contingency table.  The string must be one of::\n",
      "        \n",
      "                \"below\":\n",
      "                    Values equal to the grand median are counted as \"below\".\n",
      "                \"above\":\n",
      "                    Values equal to the grand median are counted as \"above\".\n",
      "                \"ignore\":\n",
      "                    Values equal to the grand median are not counted.\n",
      "        \n",
      "            The default is \"below\".\n",
      "        correction : bool, optional\n",
      "            If True, *and* there are just two samples, apply Yates' correction\n",
      "            for continuity when computing the test statistic associated with\n",
      "            the contingency table.  Default is True.\n",
      "        lambda_ : float or str, optional\n",
      "            By default, the statistic computed in this test is Pearson's\n",
      "            chi-squared statistic.  `lambda_` allows a statistic from the\n",
      "            Cressie-Read power divergence family to be used instead.  See\n",
      "            `power_divergence` for details.\n",
      "            Default is 1 (Pearson's chi-squared statistic).\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stat : float\n",
      "            The test statistic.  The statistic that is returned is determined by\n",
      "            `lambda_`.  The default is Pearson's chi-squared statistic.\n",
      "        p : float\n",
      "            The p-value of the test.\n",
      "        m : float\n",
      "            The grand median.\n",
      "        table : ndarray\n",
      "            The contingency table.  The shape of the table is (2, n), where\n",
      "            n is the number of samples.  The first row holds the counts of the\n",
      "            values above the grand median, and the second row holds the counts\n",
      "            of the values below the grand median.  The table allows further\n",
      "            analysis with, for example, `scipy.stats.chi2_contingency`, or with\n",
      "            `scipy.stats.fisher_exact` if there are two samples, without having\n",
      "            to recompute the table.  If ``nan_policy`` is \"propagate\" and there\n",
      "            are nans in the input, the return value for ``table`` is ``None``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kruskal : Compute the Kruskal-Wallis H-test for independent samples.\n",
      "        mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill\n",
      "            (1950), pp. 394-399.\n",
      "        .. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010).\n",
      "            See Sections 8.12 and 10.15.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        A biologist runs an experiment in which there are three groups of plants.\n",
      "        Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants.\n",
      "        Each plant produces a number of seeds.  The seed counts for each group\n",
      "        are::\n",
      "        \n",
      "            Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49\n",
      "            Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99\n",
      "            Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84\n",
      "        \n",
      "        The following code applies Mood's median test to these samples.\n",
      "        \n",
      "        >>> g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]\n",
      "        >>> g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]\n",
      "        >>> g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]\n",
      "        >>> from scipy.stats import median_test\n",
      "        >>> stat, p, med, tbl = median_test(g1, g2, g3)\n",
      "        \n",
      "        The median is\n",
      "        \n",
      "        >>> med\n",
      "        34.0\n",
      "        \n",
      "        and the contingency table is\n",
      "        \n",
      "        >>> tbl\n",
      "        array([[ 5, 10,  7],\n",
      "               [11,  5, 10]])\n",
      "        \n",
      "        `p` is too large to conclude that the medians are not the same:\n",
      "        \n",
      "        >>> p\n",
      "        0.12609082774093244\n",
      "        \n",
      "        The \"G-test\" can be performed by passing ``lambda_=\"log-likelihood\"`` to\n",
      "        `median_test`.\n",
      "        \n",
      "        >>> g, p, med, tbl = median_test(g1, g2, g3, lambda_=\"log-likelihood\")\n",
      "        >>> p\n",
      "        0.12224779737117837\n",
      "        \n",
      "        The median occurs several times in the data, so we'll get a different\n",
      "        result if, for example, ``ties=\"above\"`` is used:\n",
      "        \n",
      "        >>> stat, p, med, tbl = median_test(g1, g2, g3, ties=\"above\")\n",
      "        >>> p\n",
      "        0.063873276069553273\n",
      "        \n",
      "        >>> tbl\n",
      "        array([[ 5, 11,  9],\n",
      "               [11,  4,  8]])\n",
      "        \n",
      "        This example demonstrates that if the data set is not large and there\n",
      "        are values equal to the median, the p-value can be sensitive to the\n",
      "        choice of `ties`.\n",
      "    \n",
      "    mode(a, axis=0, nan_policy='propagate')\n",
      "        Return an array of the modal (most common) value in the passed array.\n",
      "        \n",
      "        If there is more than one such value, only the smallest is returned.\n",
      "        The bin-count for the modal bins is also returned.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            n-dimensional array of which to find mode(s).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mode : ndarray\n",
      "            Array of modal values.\n",
      "        count : ndarray\n",
      "            Array of counts for each mode.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = np.array([[6, 8, 3, 0],\n",
      "        ...               [3, 2, 1, 7],\n",
      "        ...               [8, 1, 8, 4],\n",
      "        ...               [5, 3, 0, 5],\n",
      "        ...               [4, 7, 5, 9]])\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.mode(a)\n",
      "        ModeResult(mode=array([[3, 1, 0, 0]]), count=array([[1, 1, 1, 1]]))\n",
      "        \n",
      "        To get mode of whole array, specify ``axis=None``:\n",
      "        \n",
      "        >>> stats.mode(a, axis=None)\n",
      "        ModeResult(mode=array([3]), count=array([3]))\n",
      "    \n",
      "    moment(a, moment=1, axis=0, nan_policy='propagate')\n",
      "        Calculate the nth moment about the mean for a sample.\n",
      "        \n",
      "        A moment is a specific quantitative measure of the shape of a set of\n",
      "        points. It is often used to calculate coefficients of skewness and kurtosis\n",
      "        due to its close relationship with them.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "           Input array.\n",
      "        moment : int or array_like of ints, optional\n",
      "           Order of central moment that is returned. Default is 1.\n",
      "        axis : int or None, optional\n",
      "           Axis along which the central moment is computed. Default is 0.\n",
      "           If None, compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        n-th central moment : ndarray or float\n",
      "           The appropriate moment along the given axis or over all values if axis\n",
      "           is None. The denominator for the moment calculation is the number of\n",
      "           observations, no degrees of freedom correction is done.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kurtosis, skew, describe\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The k-th central moment of a data sample is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            m_k = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^k\n",
      "        \n",
      "        Where n is the number of samples and x-bar is the mean. This function uses\n",
      "        exponentiation by squares [1]_ for efficiency.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import moment\n",
      "        >>> moment([1, 2, 3, 4, 5], moment=1)\n",
      "        0.0\n",
      "        >>> moment([1, 2, 3, 4, 5], moment=2)\n",
      "        2.0\n",
      "    \n",
      "    mood(x, y, axis=0)\n",
      "        Perform Mood's test for equal scale parameters.\n",
      "        \n",
      "        Mood's two-sample test for scale parameters is a non-parametric\n",
      "        test for the null hypothesis that two samples are drawn from the\n",
      "        same distribution with the same scale parameter.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of sample data.\n",
      "        axis : int, optional\n",
      "            The axis along which the samples are tested.  `x` and `y` can be of\n",
      "            different length along `axis`.\n",
      "            If `axis` is None, `x` and `y` are flattened and the test is done on\n",
      "            all values in the flattened arrays.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : scalar or ndarray\n",
      "            The z-score for the hypothesis test.  For 1-D inputs a scalar is\n",
      "            returned.\n",
      "        p-value : scalar ndarray\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fligner : A non-parametric test for the equality of k variances\n",
      "        ansari : A non-parametric test for the equality of 2 variances\n",
      "        bartlett : A parametric test for equality of k variances in normal samples\n",
      "        levene : A parametric test for equality of k variances\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The data are assumed to be drawn from probability distributions ``f(x)``\n",
      "        and ``f(x/s) / s`` respectively, for some probability density function f.\n",
      "        The null hypothesis is that ``s == 1``.\n",
      "        \n",
      "        For multi-dimensional arrays, if the inputs are of shapes\n",
      "        ``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the\n",
      "        resulting z and p values will have shape ``(n0, n2, n3)``.  Note that\n",
      "        ``n1`` and ``m1`` don't have to be equal, but the other dimensions do.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(1234)\n",
      "        >>> x2 = np.random.randn(2, 45, 6, 7)\n",
      "        >>> x1 = np.random.randn(2, 30, 6, 7)\n",
      "        >>> z, p = stats.mood(x1, x2, axis=1)\n",
      "        >>> p.shape\n",
      "        (2, 6, 7)\n",
      "        \n",
      "        Find the number of points where the difference in scale is not significant:\n",
      "        \n",
      "        >>> (p > 0.1).sum()\n",
      "        74\n",
      "        \n",
      "        Perform the test with different scales:\n",
      "        \n",
      "        >>> x1 = np.random.randn(2, 30)\n",
      "        >>> x2 = np.random.randn(2, 35) * 10.0\n",
      "        >>> stats.mood(x1, x2, axis=1)\n",
      "        (array([-5.7178125 , -5.25342163]), array([  1.07904114e-08,   1.49299218e-07]))\n",
      "    \n",
      "    multiscale_graphcorr(x, y, compute_distance=<function _euclidean_dist at 0x7fcfd8a14040>, reps=1000, workers=1, is_twosamp=False, random_state=None)\n",
      "        Computes the Multiscale Graph Correlation (MGC) test statistic.\n",
      "        \n",
      "        Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for\n",
      "        one property (e.g. cloud density), and the :math:`l`-nearest neighbors for\n",
      "        the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is\n",
      "        called the \"scale\". A priori, however, it is not know which scales will be\n",
      "        most informative. So, MGC computes all distance pairs, and then efficiently\n",
      "        computes the distance correlations for all scales. The local correlations\n",
      "        illustrate which scales are relatively informative about the relationship.\n",
      "        The key, therefore, to successfully discover and decipher relationships\n",
      "        between disparate data modalities is to adaptively determine which scales\n",
      "        are the most informative, and the geometric implication for the most\n",
      "        informative scales. Doing so not only provides an estimate of whether the\n",
      "        modalities are related, but also provides insight into how the\n",
      "        determination was made. This is especially important in high-dimensional\n",
      "        data, where simple visualizations do not reveal relationships to the\n",
      "        unaided human eye. Characterizations of this implementation in particular\n",
      "        have been derived from and benchmarked within in [2]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : ndarray\n",
      "            If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is\n",
      "            the number of samples and `p` and `q` are the number of dimensions,\n",
      "            then the MGC independence test will be run.  Alternatively, ``x`` and\n",
      "            ``y`` can have shapes ``(n, n)`` if they are distance or similarity\n",
      "            matrices, and ``compute_distance`` must be sent to ``None``. If ``x``\n",
      "            and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired\n",
      "            two-sample MGC test will be run.\n",
      "        compute_distance : callable, optional\n",
      "            A function that computes the distance or similarity among the samples\n",
      "            within each data matrix. Set to ``None`` if ``x`` and ``y`` are\n",
      "            already distance matrices. The default uses the euclidean norm metric.\n",
      "            If you are calling a custom function, either create the distance\n",
      "            matrix before-hand or create a function of the form\n",
      "            ``compute_distance(x)`` where `x` is the data matrix for which\n",
      "            pairwise distances are calculated.\n",
      "        reps : int, optional\n",
      "            The number of replications used to estimate the null when using the\n",
      "            permutation test. The default is ``1000``.\n",
      "        workers : int or map-like callable, optional\n",
      "            If ``workers`` is an int the population is subdivided into ``workers``\n",
      "            sections and evaluated in parallel (uses ``multiprocessing.Pool\n",
      "            <multiprocessing>``). Supply ``-1`` to use all cores available to the\n",
      "            Process. Alternatively supply a map-like callable, such as\n",
      "            ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable. The default is ``1``.\n",
      "        is_twosamp : bool, optional\n",
      "            If `True`, a two sample test will be run. If ``x`` and ``y`` have\n",
      "            shapes ``(n, p)`` and ``(m, p)``, this optional will be overriden and\n",
      "            set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes\n",
      "            ``(n, p)`` and a two sample test is desired. The default is ``False``.\n",
      "        random_state : int or np.random.RandomState instance, optional\n",
      "            If already a RandomState instance, use it.\n",
      "            If seed is an int, return a new RandomState instance seeded with seed.\n",
      "            If None, use np.random.RandomState. Default is None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stat : float\n",
      "            The sample MGC test statistic within `[-1, 1]`.\n",
      "        pvalue : float\n",
      "            The p-value obtained via permutation.\n",
      "        mgc_dict : dict\n",
      "            Contains additional useful additional returns containing the following\n",
      "            keys:\n",
      "        \n",
      "                - mgc_map : ndarray\n",
      "                    A 2D representation of the latent geometry of the relationship.\n",
      "                    of the relationship.\n",
      "                - opt_scale : (int, int)\n",
      "                    The estimated optimal scale as a `(x, y)` pair.\n",
      "                - null_dist : list\n",
      "                    The null distribution derived from the permuted matrices\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pearsonr : Pearson correlation coefficient and p-value for testing\n",
      "                   non-correlation.\n",
      "        kendalltau : Calculates Kendall's tau.\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A description of the process of MGC and applications on neuroscience data\n",
      "        can be found in [1]_. It is performed using the following steps:\n",
      "        \n",
      "        #. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and\n",
      "           modified to be mean zero columnwise. This results in two\n",
      "           :math:`n \\times n` distance matrices :math:`A` and :math:`B` (the\n",
      "           centering and unbiased modification) [3]_.\n",
      "        \n",
      "        #. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,\n",
      "        \n",
      "           * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs\n",
      "             are calculated for each property. Here, :math:`G_k (i, j)` indicates\n",
      "             the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`\n",
      "             and :math:`H_l (i, j)` indicates the :math:`l` smallested values of\n",
      "             the :math:`i`-th row of :math:`B`\n",
      "        \n",
      "           * Let :math:`\\circ` denotes the entry-wise matrix product, then local\n",
      "             correlations are summed and normalized using the following statistic:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            c^{kl} = \\frac{\\sum_{ij} A G_k B H_l}\n",
      "                          {\\sqrt{\\sum_{ij} A^2 G_k \\times \\sum_{ij} B^2 H_l}}\n",
      "        \n",
      "        #. The MGC test statistic is the smoothed optimal local correlation of\n",
      "           :math:`\\{ c^{kl} \\}`. Denote the smoothing operation as :math:`R(\\cdot)`\n",
      "           (which essentially set all isolated large correlations) as 0 and\n",
      "           connected large correlations the same as before, see [3]_.) MGC is,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MGC_n (x, y) = \\max_{(k, l)} R \\left(c^{kl} \\left( x_n, y_n \\right)\n",
      "                                                        \\right)\n",
      "        \n",
      "        The test statistic returns a value between :math:`(-1, 1)` since it is\n",
      "        normalized.\n",
      "        \n",
      "        The p-value returned is calculated using a permutation test. This process\n",
      "        is completed by first randomly permuting :math:`y` to estimate the null\n",
      "        distribution and then calculating the probability of observing a test\n",
      "        statistic, under the null, at least as extreme as the observed test\n",
      "        statistic.\n",
      "        \n",
      "        MGC requires at least 5 samples to run with reliable results. It can also\n",
      "        handle high-dimensional data sets.\n",
      "        \n",
      "        In addition, by manipulating the input data matrices, the two-sample\n",
      "        testing problem can be reduced to the independence testing problem [4]_.\n",
      "        Given sample data :math:`U` and :math:`V` of sizes :math:`p \\times n`\n",
      "        :math:`p \\times m`, data matrix :math:`X` and :math:`Y` can be created as\n",
      "        follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            X = [U | V] \\in \\mathcal{R}^{p \\times (n + m)}\n",
      "        \n",
      "            Y = [0_{1 \\times n} | 1_{1 \\times m}] \\in \\mathcal{R}^{(n + m)}\n",
      "        \n",
      "        Then, the MGC statistic can be calculated as normal. This methodology can\n",
      "        be extended to similar tests such as distance correlation [4]_.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,\n",
      "               Maggioni, M., & Shen, C. (2019). Discovering and deciphering\n",
      "               relationships across disparate data modalities. ELife.\n",
      "        .. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,\n",
      "               Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).\n",
      "               mgcpy: A Comprehensive High Dimensional Independence Testing Python\n",
      "               Package. ArXiv:1907.02088 [Cs, Stat].\n",
      "        .. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance\n",
      "               correlation to multiscale graph correlation. Journal of the American\n",
      "               Statistical Association.\n",
      "        .. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of\n",
      "               Distance and Kernel Methods for Hypothesis Testing. ArXiv:1806.05514\n",
      "               [Cs, Stat].\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import multiscale_graphcorr\n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = x\n",
      "        >>> stat, pvalue, _ = multiscale_graphcorr(x, y, workers=-1)\n",
      "        >>> '%.1f, %.3f' % (stat, pvalue)\n",
      "        '1.0, 0.001'\n",
      "        \n",
      "        Alternatively,\n",
      "        \n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = x\n",
      "        >>> mgc = multiscale_graphcorr(x, y)\n",
      "        >>> '%.1f, %.3f' % (mgc.stat, mgc.pvalue)\n",
      "        '1.0, 0.001'\n",
      "        \n",
      "        To run an unpaired two-sample test,\n",
      "        \n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = np.arange(79)\n",
      "        >>> mgc = multiscale_graphcorr(x, y, random_state=1)\n",
      "        >>> '%.3f, %.2f' % (mgc.stat, mgc.pvalue)\n",
      "        '0.033, 0.02'\n",
      "        \n",
      "        or, if shape of the inputs are the same,\n",
      "        \n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = x\n",
      "        >>> mgc = multiscale_graphcorr(x, y, is_twosamp=True)\n",
      "        >>> '%.3f, %.1f' % (mgc.stat, mgc.pvalue)\n",
      "        '-0.008, 1.0'\n",
      "    \n",
      "    mvsdist(data)\n",
      "        'Frozen' distributions for mean, variance, and standard deviation of data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input array. Converted to 1-D using ravel.\n",
      "            Requires 2 or more data-points.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mdist : \"frozen\" distribution object\n",
      "            Distribution object representing the mean of the data.\n",
      "        vdist : \"frozen\" distribution object\n",
      "            Distribution object representing the variance of the data.\n",
      "        sdist : \"frozen\" distribution object\n",
      "            Distribution object representing the standard deviation of the data.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bayes_mvs\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The return values from ``bayes_mvs(data)`` is equivalent to\n",
      "        ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.\n",
      "        \n",
      "        In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``\n",
      "        on the three distribution objects returned from this function will give\n",
      "        the same results that are returned from `bayes_mvs`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n",
      "        standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n",
      "        2006.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> data = [6, 9, 12, 7, 8, 8, 13]\n",
      "        >>> mean, var, std = stats.mvsdist(data)\n",
      "        \n",
      "        We now have frozen distribution objects \"mean\", \"var\" and \"std\" that we can\n",
      "        examine:\n",
      "        \n",
      "        >>> mean.mean()\n",
      "        9.0\n",
      "        >>> mean.interval(0.95)\n",
      "        (6.6120585482655692, 11.387941451734431)\n",
      "        >>> mean.std()\n",
      "        1.1952286093343936\n",
      "    \n",
      "    normaltest(a, axis=0, nan_policy='propagate')\n",
      "        Test whether a sample differs from a normal distribution.\n",
      "        \n",
      "        This function tests the null hypothesis that a sample comes\n",
      "        from a normal distribution.  It is based on D'Agostino and\n",
      "        Pearson's [1]_, [2]_ test that combines skew and kurtosis to\n",
      "        produce an omnibus test of normality.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            The array containing the sample to be tested.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. Default is 0. If None,\n",
      "            compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\n",
      "            ``k`` is the z-score returned by `kurtosistest`.\n",
      "        pvalue : float or array\n",
      "           A 2-sided chi squared probability for the hypothesis test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\n",
      "               moderate and large sample size\", Biometrika, 58, 341-348\n",
      "        \n",
      "        .. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n",
      "               normality\", Biometrika, 60, 613-622\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> pts = 1000\n",
      "        >>> np.random.seed(28041990)\n",
      "        >>> a = np.random.normal(0, 1, size=pts)\n",
      "        >>> b = np.random.normal(2, 1, size=pts)\n",
      "        >>> x = np.concatenate((a, b))\n",
      "        >>> k2, p = stats.normaltest(x)\n",
      "        >>> alpha = 1e-3\n",
      "        >>> print(\"p = {:g}\".format(p))\n",
      "        p = 3.27207e-11\n",
      "        >>> if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
      "        ...     print(\"The null hypothesis can be rejected\")\n",
      "        ... else:\n",
      "        ...     print(\"The null hypothesis cannot be rejected\")\n",
      "        The null hypothesis can be rejected\n",
      "    \n",
      "    obrientransform(*args)\n",
      "        Compute the O'Brien transform on input data (any number of arrays).\n",
      "        \n",
      "        Used to test for homogeneity of variance prior to running one-way stats.\n",
      "        Each array in ``*args`` is one level of a factor.\n",
      "        If `f_oneway` is run on the transformed data and found significant,\n",
      "        the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        args : tuple of array_like\n",
      "            Any number of arrays.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        obrientransform : ndarray\n",
      "            Transformed data for use in an ANOVA.  The first dimension\n",
      "            of the result corresponds to the sequence of transformed\n",
      "            arrays.  If the arrays given are all 1-D of the same length,\n",
      "            the return value is a 2-D array; otherwise it is a 1-D array\n",
      "            of type object, with each element being an ndarray.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n",
      "               Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We'll test the following data sets for differences in their variance.\n",
      "        \n",
      "        >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n",
      "        >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n",
      "        \n",
      "        Apply the O'Brien transform to the data.\n",
      "        \n",
      "        >>> from scipy.stats import obrientransform\n",
      "        >>> tx, ty = obrientransform(x, y)\n",
      "        \n",
      "        Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n",
      "        transformed data.\n",
      "        \n",
      "        >>> from scipy.stats import f_oneway\n",
      "        >>> F, p = f_oneway(tx, ty)\n",
      "        >>> p\n",
      "        0.1314139477040335\n",
      "        \n",
      "        If we require that ``p < 0.05`` for significance, we cannot conclude\n",
      "        that the variances are different.\n",
      "    \n",
      "    pearsonr(x, y)\n",
      "        Pearson correlation coefficient and p-value for testing non-correlation.\n",
      "        \n",
      "        The Pearson correlation coefficient [1]_ measures the linear relationship\n",
      "        between two datasets.  The calculation of the p-value relies on the\n",
      "        assumption that each dataset is normally distributed.  (See Kowalski [3]_\n",
      "        for a discussion of the effects of non-normality of the input on the\n",
      "        distribution of the correlation coefficient.)  Like other correlation\n",
      "        coefficients, this one varies between -1 and +1 with 0 implying no\n",
      "        correlation. Correlations of -1 or +1 imply an exact linear relationship.\n",
      "        Positive correlations imply that as x increases, so does y. Negative\n",
      "        correlations imply that as x increases, y decreases.\n",
      "        \n",
      "        The p-value roughly indicates the probability of an uncorrelated system\n",
      "        producing datasets that have a Pearson correlation at least as extreme\n",
      "        as the one computed from these datasets.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (N,) array_like\n",
      "            Input array.\n",
      "        y : (N,) array_like\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        r : float\n",
      "            Pearson's correlation coefficient.\n",
      "        p-value : float\n",
      "            Two-tailed p-value.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        PearsonRConstantInputWarning\n",
      "            Raised if an input is a constant array.  The correlation coefficient\n",
      "            is not defined in this case, so ``np.nan`` is returned.\n",
      "        \n",
      "        PearsonRNearConstantInputWarning\n",
      "            Raised if an input is \"nearly\" constant.  The array ``x`` is considered\n",
      "            nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.\n",
      "            Numerical errors in the calculation ``x - mean(x)`` in this case might\n",
      "            result in an inaccurate calculation of r.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        spearmanr : Spearman rank-order correlation coefficient.\n",
      "        kendalltau : Kendall's tau, a correlation measure for ordinal data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The correlation coefficient is calculated as follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            r = \\frac{\\sum (x - m_x) (y - m_y)}\n",
      "                     {\\sqrt{\\sum (x - m_x)^2 \\sum (y - m_y)^2}}\n",
      "        \n",
      "        where :math:`m_x` is the mean of the vector :math:`x` and :math:`m_y` is\n",
      "        the mean of the vector :math:`y`.\n",
      "        \n",
      "        Under the assumption that x and y are drawn from independent normal\n",
      "        distributions (so the population correlation coefficient is 0), the\n",
      "        probability density function of the sample correlation coefficient r\n",
      "        is ([1]_, [2]_)::\n",
      "        \n",
      "                   (1 - r**2)**(n/2 - 2)\n",
      "            f(r) = ---------------------\n",
      "                      B(1/2, n/2 - 1)\n",
      "        \n",
      "        where n is the number of samples, and B is the beta function.  This\n",
      "        is sometimes referred to as the exact distribution of r.  This is\n",
      "        the distribution that is used in `pearsonr` to compute the p-value.\n",
      "        The distribution is a beta distribution on the interval [-1, 1],\n",
      "        with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's\n",
      "        implementation of the beta distribution, the distribution of r is::\n",
      "        \n",
      "            dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
      "        \n",
      "        The p-value returned by `pearsonr` is a two-sided p-value.  For a\n",
      "        given sample with correlation coefficient r, the p-value is\n",
      "        the probability that abs(r') of a random sample x' and y' drawn from\n",
      "        the population with zero correlation would be greater than or equal\n",
      "        to abs(r).  In terms of the object ``dist`` shown above, the p-value\n",
      "        for a given r and length n can be computed as::\n",
      "        \n",
      "            p = 2*dist.cdf(-abs(r))\n",
      "        \n",
      "        When n is 2, the above continuous distribution is not well-defined.\n",
      "        One can interpret the limit of the beta distribution as the shape\n",
      "        parameters a and b approach a = b = 0 as a discrete distribution with\n",
      "        equal probability masses at r = 1 and r = -1.  More directly, one\n",
      "        can observe that, given the data x = [x1, x2] and y = [y1, y2], and\n",
      "        assuming x1 != x2 and y1 != y2, the only possible values for r are 1\n",
      "        and -1.  Because abs(r') for any sample x' and y' with length 2 will\n",
      "        be 1, the two-sided p-value for a sample of length 2 is always 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
      "        .. [2] Student, \"Probable error of a correlation coefficient\",\n",
      "               Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.\n",
      "        .. [3] C. J. Kowalski, \"On the Effects of Non-Normality on the Distribution\n",
      "               of the Sample Product-Moment Correlation Coefficient\"\n",
      "               Journal of the Royal Statistical Society. Series C (Applied\n",
      "               Statistics), Vol. 21, No. 1 (1972), pp. 1-12.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n",
      "        >>> b = np.arange(7)\n",
      "        >>> stats.pearsonr(a, b)\n",
      "        (0.8660254037844386, 0.011724811003954649)\n",
      "        \n",
      "        >>> stats.pearsonr([1, 2, 3, 4, 5], [10, 9, 2.5, 6, 4])\n",
      "        (-0.7426106572325057, 0.1505558088534455)\n",
      "    \n",
      "    percentileofscore(a, score, kind='rank')\n",
      "        Compute the percentile rank of a score relative to a list of scores.\n",
      "        \n",
      "        A `percentileofscore` of, for example, 80% means that 80% of the\n",
      "        scores in `a` are below the given score. In the case of gaps or\n",
      "        ties, the exact definition depends on the optional keyword, `kind`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of scores to which `score` is compared.\n",
      "        score : int or float\n",
      "            Score that is compared to the elements in `a`.\n",
      "        kind : {'rank', 'weak', 'strict', 'mean'}, optional\n",
      "            Specifies the interpretation of the resulting score.\n",
      "            The following options are available (default is 'rank'):\n",
      "        \n",
      "              * 'rank': Average percentage ranking of score.  In case of multiple\n",
      "                matches, average the percentage rankings of all matching scores.\n",
      "              * 'weak': This kind corresponds to the definition of a cumulative\n",
      "                distribution function.  A percentileofscore of 80% means that 80%\n",
      "                of values are less than or equal to the provided score.\n",
      "              * 'strict': Similar to \"weak\", except that only values that are\n",
      "                strictly less than the given score are counted.\n",
      "              * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n",
      "                in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        pcos : float\n",
      "            Percentile-position of score (0-100) relative to `a`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.percentile\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Three-quarters of the given values lie below a given score:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> stats.percentileofscore([1, 2, 3, 4], 3)\n",
      "        75.0\n",
      "        \n",
      "        With multiple matches, note how the scores of the two matches, 0.6\n",
      "        and 0.8 respectively, are averaged:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n",
      "        70.0\n",
      "        \n",
      "        Only 2/5 values are strictly less than 3:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n",
      "        40.0\n",
      "        \n",
      "        But 4/5 values are less than or equal to 3:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n",
      "        80.0\n",
      "        \n",
      "        The average between the weak and the strict scores is:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n",
      "        60.0\n",
      "    \n",
      "    pointbiserialr(x, y)\n",
      "        Calculate a point biserial correlation coefficient and its p-value.\n",
      "        \n",
      "        The point biserial correlation is used to measure the relationship\n",
      "        between a binary variable, x, and a continuous variable, y. Like other\n",
      "        correlation coefficients, this one varies between -1 and +1 with 0\n",
      "        implying no correlation. Correlations of -1 or +1 imply a determinative\n",
      "        relationship.\n",
      "        \n",
      "        This function uses a shortcut formula but produces the same result as\n",
      "        `pearsonr`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like of bools\n",
      "            Input array.\n",
      "        y : array_like\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float\n",
      "            R value.\n",
      "        pvalue : float\n",
      "            Two-sided p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.\n",
      "        It is equivalent to `pearsonr.`\n",
      "        \n",
      "        The value of the point-biserial correlation can be calculated from:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            r_{pb} = \\frac{\\overline{Y_{1}} -\n",
      "                     \\overline{Y_{0}}}{s_{y}}\\sqrt{\\frac{N_{1} N_{2}}{N (N - 1))}}\n",
      "        \n",
      "        Where :math:`Y_{0}` and :math:`Y_{1}` are means of the metric\n",
      "        observations coded 0 and 1 respectively; :math:`N_{0}` and :math:`N_{1}`\n",
      "        are number of observations coded 0 and 1 respectively; :math:`N` is the\n",
      "        total number of observations and :math:`s_{y}` is the standard\n",
      "        deviation of all the metric observations.\n",
      "        \n",
      "        A value of :math:`r_{pb}` that is significantly different from zero is\n",
      "        completely equivalent to a significant difference in means between the two\n",
      "        groups. Thus, an independent groups t Test with :math:`N-2` degrees of\n",
      "        freedom may be used to test whether :math:`r_{pb}` is nonzero. The\n",
      "        relation between the t-statistic for comparing two independent groups and\n",
      "        :math:`r_{pb}` is given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            t = \\sqrt{N - 2}\\frac{r_{pb}}{\\sqrt{1 - r^{2}_{pb}}}\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Lev, \"The Point Biserial Coefficient of Correlation\", Ann. Math.\n",
      "               Statist., Vol. 20, no.1, pp. 125-126, 1949.\n",
      "        \n",
      "        .. [2] R.F. Tate, \"Correlation Between a Discrete and a Continuous\n",
      "               Variable. Point-Biserial Correlation.\", Ann. Math. Statist., Vol. 25,\n",
      "               np. 3, pp. 603-607, 1954.\n",
      "        \n",
      "        .. [3] D. Kornbrot \"Point Biserial Correlation\", In Wiley StatsRef:\n",
      "               Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.\n",
      "               https://doi.org/10.1002/9781118445112.stat06227\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n",
      "        >>> b = np.arange(7)\n",
      "        >>> stats.pointbiserialr(a, b)\n",
      "        (0.8660254037844386, 0.011724811003954652)\n",
      "        >>> stats.pearsonr(a, b)\n",
      "        (0.86602540378443871, 0.011724811003954626)\n",
      "        >>> np.corrcoef(a, b)\n",
      "        array([[ 1.       ,  0.8660254],\n",
      "               [ 0.8660254,  1.       ]])\n",
      "    \n",
      "    power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None)\n",
      "        Cressie-Read power divergence statistic and goodness of fit test.\n",
      "        \n",
      "        This function tests the null hypothesis that the categorical data\n",
      "        has the given frequencies, using the Cressie-Read power divergence\n",
      "        statistic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f_obs : array_like\n",
      "            Observed frequencies in each category.\n",
      "        f_exp : array_like, optional\n",
      "            Expected frequencies in each category.  By default the categories are\n",
      "            assumed to be equally likely.\n",
      "        ddof : int, optional\n",
      "            \"Delta degrees of freedom\": adjustment to the degrees of freedom\n",
      "            for the p-value.  The p-value is computed using a chi-squared\n",
      "            distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n",
      "            is the number of observed frequencies.  The default value of `ddof`\n",
      "            is 0.\n",
      "        axis : int or None, optional\n",
      "            The axis of the broadcast result of `f_obs` and `f_exp` along which to\n",
      "            apply the test.  If axis is None, all values in `f_obs` are treated\n",
      "            as a single data set.  Default is 0.\n",
      "        lambda_ : float or str, optional\n",
      "            The power in the Cressie-Read power divergence statistic.  The default\n",
      "            is 1.  For convenience, `lambda_` may be assigned one of the following\n",
      "            strings, in which case the corresponding numerical value is used::\n",
      "        \n",
      "                String              Value   Description\n",
      "                \"pearson\"             1     Pearson's chi-squared statistic.\n",
      "                                            In this case, the function is\n",
      "                                            equivalent to `stats.chisquare`.\n",
      "                \"log-likelihood\"      0     Log-likelihood ratio. Also known as\n",
      "                                            the G-test [3]_.\n",
      "                \"freeman-tukey\"      -1/2   Freeman-Tukey statistic.\n",
      "                \"mod-log-likelihood\" -1     Modified log-likelihood ratio.\n",
      "                \"neyman\"             -2     Neyman's statistic.\n",
      "                \"cressie-read\"        2/3   The power recommended in [5]_.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or ndarray\n",
      "            The Cressie-Read power divergence test statistic.  The value is\n",
      "            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n",
      "        pvalue : float or ndarray\n",
      "            The p-value of the test.  The value is a float if `ddof` and the\n",
      "            return value `stat` are scalars.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chisquare\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This test is invalid when the observed or expected frequencies in each\n",
      "        category are too small.  A typical rule is that all of the observed\n",
      "        and expected frequencies should be at least 5.\n",
      "        \n",
      "        When `lambda_` is less than zero, the formula for the statistic involves\n",
      "        dividing by `f_obs`, so a warning or error may be generated if any value\n",
      "        in `f_obs` is 0.\n",
      "        \n",
      "        Similarly, a warning or error may be generated if any value in `f_exp` is\n",
      "        zero when `lambda_` >= 0.\n",
      "        \n",
      "        The default degrees of freedom, k-1, are for the case when no parameters\n",
      "        of the distribution are estimated. If p parameters are estimated by\n",
      "        efficient maximum likelihood then the correct degrees of freedom are\n",
      "        k-1-p. If the parameters are estimated in a different way, then the\n",
      "        dof can be between k-1-p and k-1. However, it is also possible that\n",
      "        the asymptotic distribution is not a chisquare, in which case this\n",
      "        test is not appropriate.\n",
      "        \n",
      "        This function handles masked arrays.  If an element of `f_obs` or `f_exp`\n",
      "        is masked, then data at that position is ignored, and does not count\n",
      "        towards the size of the data set.\n",
      "        \n",
      "        .. versionadded:: 0.13.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
      "               Statistics\". Chapter 8.\n",
      "               https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n",
      "        .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n",
      "        .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n",
      "        .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n",
      "               practice of statistics in biological research\", New York: Freeman\n",
      "               (1981)\n",
      "        .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
      "               Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
      "               pp. 440-464.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        (See `chisquare` for more examples.)\n",
      "        \n",
      "        When just `f_obs` is given, it is assumed that the expected frequencies\n",
      "        are uniform and given by the mean of the observed frequencies.  Here we\n",
      "        perform a G-test (i.e. use the log-likelihood ratio statistic):\n",
      "        \n",
      "        >>> from scipy.stats import power_divergence\n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n",
      "        (2.006573162632538, 0.84823476779463769)\n",
      "        \n",
      "        The expected frequencies can be given with the `f_exp` argument:\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12],\n",
      "        ...                  f_exp=[16, 16, 16, 16, 16, 8],\n",
      "        ...                  lambda_='log-likelihood')\n",
      "        (3.3281031458963746, 0.6495419288047497)\n",
      "        \n",
      "        When `f_obs` is 2-D, by default the test is applied to each column.\n",
      "        \n",
      "        >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
      "        >>> obs.shape\n",
      "        (6, 2)\n",
      "        >>> power_divergence(obs, lambda_=\"log-likelihood\")\n",
      "        (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n",
      "        \n",
      "        By setting ``axis=None``, the test is applied to all data in the array,\n",
      "        which is equivalent to applying the test to the flattened array.\n",
      "        \n",
      "        >>> power_divergence(obs, axis=None)\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        >>> power_divergence(obs.ravel())\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        \n",
      "        `ddof` is the change to make to the default degrees of freedom.\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n",
      "        (2.0, 0.73575888234288467)\n",
      "        \n",
      "        The calculation of the p-values is done by broadcasting the\n",
      "        test statistic with `ddof`.\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n",
      "        (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n",
      "        \n",
      "        `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n",
      "        shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n",
      "        `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n",
      "        statistics, we must use ``axis=1``:\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12],\n",
      "        ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n",
      "        ...                         [8, 20, 20, 16, 12, 12]],\n",
      "        ...                  axis=1)\n",
      "        (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n",
      "    \n",
      "    ppcc_max(x, brack=(0.0, 1.0), dist='tukeylambda')\n",
      "        Calculate the shape parameter that maximizes the PPCC.\n",
      "        \n",
      "        The probability plot correlation coefficient (PPCC) plot can be used to\n",
      "        determine the optimal shape parameter for a one-parameter family of\n",
      "        distributions.  ppcc_max returns the shape parameter that would maximize the\n",
      "        probability plot correlation coefficient for the given data to a\n",
      "        one-parameter family of distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)\n",
      "            then they are assumed to be a starting interval for a downhill bracket\n",
      "            search (see `scipy.optimize.brent`).\n",
      "        dist : str or stats.distributions instance, optional\n",
      "            Distribution or distribution function name.  Objects that look enough\n",
      "            like a stats.distributions instance (i.e. they have a ``ppf`` method)\n",
      "            are also accepted.  The default is ``'tukeylambda'``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        shape_value : float\n",
      "            The shape parameter at which the probability plot correlation\n",
      "            coefficient reaches its max value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ppcc_plot, probplot, boxcox\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The brack keyword serves as a starting point which is useful in corner\n",
      "        cases. One can use a plot to obtain a rough visual estimate of the location\n",
      "        for the maximum to start the search near it.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n",
      "               Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n",
      "        \n",
      "        .. [2] https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First we generate some random data from a Tukey-Lambda distribution,\n",
      "        with shape parameter -0.7:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000,\n",
      "        ...                           random_state=1234567) + 1e4\n",
      "        \n",
      "        Now we explore this data with a PPCC plot as well as the related\n",
      "        probability plot and Box-Cox normplot.  A red line is drawn where we\n",
      "        expect the PPCC value to be maximal (at the shape parameter -0.7 used\n",
      "        above):\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig = plt.figure(figsize=(8, 6))\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> res = stats.ppcc_plot(x, -5, 5, plot=ax)\n",
      "        \n",
      "        We calculate the value where the shape should reach its maximum and a red\n",
      "        line is drawn there. The line should coincide with the highest point in the\n",
      "        ppcc_plot.\n",
      "        \n",
      "        >>> max = stats.ppcc_max(x)\n",
      "        >>> ax.vlines(max, 0, 1, colors='r', label='Expected shape value')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    ppcc_plot(x, a, b, dist='tukeylambda', plot=None, N=80)\n",
      "        Calculate and optionally plot probability plot correlation coefficient.\n",
      "        \n",
      "        The probability plot correlation coefficient (PPCC) plot can be used to\n",
      "        determine the optimal shape parameter for a one-parameter family of\n",
      "        distributions.  It cannot be used for distributions without shape parameters\n",
      "        (like the normal distribution) or with multiple shape parameters.\n",
      "        \n",
      "        By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A\n",
      "        Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed\n",
      "        distributions via an approximately normal one, and is therefore particularly\n",
      "        useful in practice.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        a, b : scalar\n",
      "            Lower and upper bounds of the shape parameter to use.\n",
      "        dist : str or stats.distributions instance, optional\n",
      "            Distribution or distribution function name.  Objects that look enough\n",
      "            like a stats.distributions instance (i.e. they have a ``ppf`` method)\n",
      "            are also accepted.  The default is ``'tukeylambda'``.\n",
      "        plot : object, optional\n",
      "            If given, plots PPCC against the shape parameter.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        N : int, optional\n",
      "            Number of points on the horizontal axis (equally distributed from\n",
      "            `a` to `b`).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        svals : ndarray\n",
      "            The shape values for which `ppcc` was calculated.\n",
      "        ppcc : ndarray\n",
      "            The calculated probability plot correlation coefficient values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ppcc_max, probplot, boxcox_normplot, tukeylambda\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n",
      "        Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First we generate some random data from a Tukey-Lambda distribution,\n",
      "        with shape parameter -0.7:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> np.random.seed(1234567)\n",
      "        >>> x = stats.tukeylambda.rvs(-0.7, loc=2, scale=0.5, size=10000) + 1e4\n",
      "        \n",
      "        Now we explore this data with a PPCC plot as well as the related\n",
      "        probability plot and Box-Cox normplot.  A red line is drawn where we\n",
      "        expect the PPCC value to be maximal (at the shape parameter -0.7 used\n",
      "        above):\n",
      "        \n",
      "        >>> fig = plt.figure(figsize=(12, 4))\n",
      "        >>> ax1 = fig.add_subplot(131)\n",
      "        >>> ax2 = fig.add_subplot(132)\n",
      "        >>> ax3 = fig.add_subplot(133)\n",
      "        >>> res = stats.probplot(x, plot=ax1)\n",
      "        >>> res = stats.boxcox_normplot(x, -5, 5, plot=ax2)\n",
      "        >>> res = stats.ppcc_plot(x, -5, 5, plot=ax3)\n",
      "        >>> ax3.vlines(-0.7, 0, 1, colors='r', label='Expected shape value')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    probplot(x, sparams=(), dist='norm', fit=True, plot=None, rvalue=False)\n",
      "        Calculate quantiles for a probability plot, and optionally show the plot.\n",
      "        \n",
      "        Generates a probability plot of sample data against the quantiles of a\n",
      "        specified theoretical distribution (the normal distribution by default).\n",
      "        `probplot` optionally calculates a best-fit line for the data and plots the\n",
      "        results using Matplotlib or a given plot function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Sample/response data from which `probplot` creates the plot.\n",
      "        sparams : tuple, optional\n",
      "            Distribution-specific shape parameters (shape parameters plus location\n",
      "            and scale).\n",
      "        dist : str or stats.distributions instance, optional\n",
      "            Distribution or distribution function name. The default is 'norm' for a\n",
      "            normal probability plot.  Objects that look enough like a\n",
      "            stats.distributions instance (i.e. they have a ``ppf`` method) are also\n",
      "            accepted.\n",
      "        fit : bool, optional\n",
      "            Fit a least-squares regression (best-fit) line to the sample data if\n",
      "            True (default).\n",
      "        plot : object, optional\n",
      "            If given, plots the quantiles and least squares fit.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        (osm, osr) : tuple of ndarrays\n",
      "            Tuple of theoretical quantiles (osm, or order statistic medians) and\n",
      "            ordered responses (osr).  `osr` is simply sorted input `x`.\n",
      "            For details on how `osm` is calculated see the Notes section.\n",
      "        (slope, intercept, r) : tuple of floats, optional\n",
      "            Tuple  containing the result of the least-squares fit, if that is\n",
      "            performed by `probplot`. `r` is the square root of the coefficient of\n",
      "            determination.  If ``fit=False`` and ``plot=None``, this tuple is not\n",
      "            returned.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Even if `plot` is given, the figure is not shown or saved by `probplot`;\n",
      "        ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after\n",
      "        calling `probplot`.\n",
      "        \n",
      "        `probplot` generates a probability plot, which should not be confused with\n",
      "        a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this\n",
      "        type, see ``statsmodels.api.ProbPlot``.\n",
      "        \n",
      "        The formula used for the theoretical quantiles (horizontal axis of the\n",
      "        probability plot) is Filliben's estimate::\n",
      "        \n",
      "            quantiles = dist.ppf(val), for\n",
      "        \n",
      "                    0.5**(1/n),                  for i = n\n",
      "              val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1\n",
      "                    1 - 0.5**(1/n),              for i = 1\n",
      "        \n",
      "        where ``i`` indicates the i-th ordered value and ``n`` is the total number\n",
      "        of values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> nsample = 100\n",
      "        >>> np.random.seed(7654321)\n",
      "        \n",
      "        A t distribution with small degrees of freedom:\n",
      "        \n",
      "        >>> ax1 = plt.subplot(221)\n",
      "        >>> x = stats.t.rvs(3, size=nsample)\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        A t distribution with larger degrees of freedom:\n",
      "        \n",
      "        >>> ax2 = plt.subplot(222)\n",
      "        >>> x = stats.t.rvs(25, size=nsample)\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        A mixture of two normal distributions with broadcasting:\n",
      "        \n",
      "        >>> ax3 = plt.subplot(223)\n",
      "        >>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],\n",
      "        ...                    size=(nsample//2,2)).ravel()\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        A standard normal distribution:\n",
      "        \n",
      "        >>> ax4 = plt.subplot(224)\n",
      "        >>> x = stats.norm.rvs(loc=0, scale=1, size=nsample)\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        Produce a new figure with a loggamma distribution, using the ``dist`` and\n",
      "        ``sparams`` keywords:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> x = stats.loggamma.rvs(c=2.5, size=500)\n",
      "        >>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)\n",
      "        >>> ax.set_title(\"Probplot for loggamma dist with shape parameter 2.5\")\n",
      "        \n",
      "        Show the results with Matplotlib:\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    rankdata(a, method='average', *, axis=None)\n",
      "        Assign ranks to data, dealing with ties appropriately.\n",
      "        \n",
      "        By default (``axis=None``), the data array is first flattened, and a flat\n",
      "        array of ranks is returned. Separately reshape the rank array to the\n",
      "        shape of the data array if desired (see Examples).\n",
      "        \n",
      "        Ranks begin at 1.  The `method` argument controls how ranks are assigned\n",
      "        to equal values.  See [1]_ for further discussion of ranking methods.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            The array of values to be ranked.\n",
      "        method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n",
      "            The method used to assign ranks to tied elements.\n",
      "            The following methods are available (default is 'average'):\n",
      "        \n",
      "              * 'average': The average of the ranks that would have been assigned to\n",
      "                all the tied values is assigned to each value.\n",
      "              * 'min': The minimum of the ranks that would have been assigned to all\n",
      "                the tied values is assigned to each value.  (This is also\n",
      "                referred to as \"competition\" ranking.)\n",
      "              * 'max': The maximum of the ranks that would have been assigned to all\n",
      "                the tied values is assigned to each value.\n",
      "              * 'dense': Like 'min', but the rank of the next highest element is\n",
      "                assigned the rank immediately after those assigned to the tied\n",
      "                elements.\n",
      "              * 'ordinal': All values are given a distinct rank, corresponding to\n",
      "                the order that the values occur in `a`.\n",
      "        axis : {None, int}, optional\n",
      "            Axis along which to perform the ranking. If ``None``, the data array\n",
      "            is first flattened.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ranks : ndarray\n",
      "             An array of size equal to the size of `a`, containing rank\n",
      "             scores.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import rankdata\n",
      "        >>> rankdata([0, 2, 3, 2])\n",
      "        array([ 1. ,  2.5,  4. ,  2.5])\n",
      "        >>> rankdata([0, 2, 3, 2], method='min')\n",
      "        array([ 1,  2,  4,  2])\n",
      "        >>> rankdata([0, 2, 3, 2], method='max')\n",
      "        array([ 1,  3,  4,  3])\n",
      "        >>> rankdata([0, 2, 3, 2], method='dense')\n",
      "        array([ 1,  2,  3,  2])\n",
      "        >>> rankdata([0, 2, 3, 2], method='ordinal')\n",
      "        array([ 1,  2,  4,  3])\n",
      "        >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n",
      "        array([[1. , 2.5],\n",
      "              [4. , 2.5]])\n",
      "        >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n",
      "        array([[1. , 2.5, 2.5],\n",
      "               [2. , 1. , 3. ]])\n",
      "    \n",
      "    ranksums(x, y)\n",
      "        Compute the Wilcoxon rank-sum statistic for two samples.\n",
      "        \n",
      "        The Wilcoxon rank-sum test tests the null hypothesis that two sets\n",
      "        of measurements are drawn from the same distribution.  The alternative\n",
      "        hypothesis is that values in one sample are more likely to be\n",
      "        larger than the values in the other sample.\n",
      "        \n",
      "        This test should be used to compare two samples from continuous\n",
      "        distributions.  It does not handle ties between measurements\n",
      "        in x and y.  For tie-handling and an optional continuity correction\n",
      "        see `scipy.stats.mannwhitneyu`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x,y : array_like\n",
      "            The data from the two samples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic under the large-sample approximation that the\n",
      "            rank sum statistic is normally distributed.\n",
      "        pvalue : float\n",
      "            The two-sided p-value of the test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We can test the hypothesis that two independent unequal-sized samples are\n",
      "        drawn from the same distribution with computing the Wilcoxon rank-sum\n",
      "        statistic.\n",
      "        \n",
      "        >>> from scipy.stats import ranksums\n",
      "        >>> sample1 = np.random.uniform(-1, 1, 200)\n",
      "        >>> sample2 = np.random.uniform(-0.5, 1.5, 300) # a shifted distribution\n",
      "        >>> ranksums(sample1, sample2)\n",
      "        RanksumsResult(statistic=-7.887059, pvalue=3.09390448e-15)  # may vary\n",
      "        \n",
      "        The p-value of less than ``0.05`` indicates that this test rejects the\n",
      "        hypothesis at the 5% significance level.\n",
      "    \n",
      "    relfreq(a, numbins=10, defaultreallimits=None, weights=None)\n",
      "        Return a relative frequency histogram, using the histogram function.\n",
      "        \n",
      "        A relative frequency  histogram is a mapping of the number of\n",
      "        observations in each of the bins relative to the total of observations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        numbins : int, optional\n",
      "            The number of bins to use for the histogram. Default is 10.\n",
      "        defaultreallimits : tuple (lower, upper), optional\n",
      "            The lower and upper values for the range of the histogram.\n",
      "            If no value is given, a range slightly larger than the range of the\n",
      "            values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n",
      "            where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n",
      "        weights : array_like, optional\n",
      "            The weights for each value in `a`. Default is None, which gives each\n",
      "            value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        frequency : ndarray\n",
      "            Binned values of relative frequency.\n",
      "        lowerlimit : float\n",
      "            Lower real limit.\n",
      "        binsize : float\n",
      "            Width of each bin.\n",
      "        extrapoints : int\n",
      "            Extra points.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([2, 4, 1, 2, 3, 2])\n",
      "        >>> res = stats.relfreq(a, numbins=4)\n",
      "        >>> res.frequency\n",
      "        array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n",
      "        >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n",
      "        1.0\n",
      "        \n",
      "        Create a normal distribution with 1000 random values\n",
      "        \n",
      "        >>> rng = np.random.RandomState(seed=12345)\n",
      "        >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n",
      "        \n",
      "        Calculate relative frequencies\n",
      "        \n",
      "        >>> res = stats.relfreq(samples, numbins=25)\n",
      "        \n",
      "        Calculate space of values for x\n",
      "        \n",
      "        >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n",
      "        ...                                  res.frequency.size)\n",
      "        \n",
      "        Plot relative frequency histogram\n",
      "        \n",
      "        >>> fig = plt.figure(figsize=(5, 4))\n",
      "        >>> ax = fig.add_subplot(1, 1, 1)\n",
      "        >>> ax.bar(x, res.frequency, width=res.binsize)\n",
      "        >>> ax.set_title('Relative frequency histogram')\n",
      "        >>> ax.set_xlim([x.min(), x.max()])\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    rvs_ratio_uniforms(pdf, umax, vmin, vmax, size=1, c=0, random_state=None)\n",
      "        Generate random samples from a probability density function using the\n",
      "        ratio-of-uniforms method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pdf : callable\n",
      "            A function with signature `pdf(x)` that is proportional to the\n",
      "            probability density function of the distribution.\n",
      "        umax : float\n",
      "            The upper bound of the bounding rectangle in the u-direction.\n",
      "        vmin : float\n",
      "            The lower bound of the bounding rectangle in the v-direction.\n",
      "        vmax : float\n",
      "            The upper bound of the bounding rectangle in the v-direction.\n",
      "        size : int or tuple of ints, optional\n",
      "            Defining number of random variates (default is 1).\n",
      "        c : float, optional.\n",
      "            Shift parameter of ratio-of-uniforms method, see Notes. Default is 0.\n",
      "        random_state : {None, int, `~np.random.RandomState`, `~np.random.Generator`}, optional\n",
      "            If `random_state` is `None` the `~np.random.RandomState` singleton is\n",
      "            used.\n",
      "            If `random_state` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with random_state.\n",
      "            If `random_state` is already a ``RandomState`` or ``Generator``\n",
      "            instance, then that object is used.\n",
      "            Default is None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rvs : ndarray\n",
      "            The random variates distributed according to the probability\n",
      "            distribution defined by the pdf.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Given a univariate probability density function `pdf` and a constant `c`,\n",
      "        define the set ``A = {(u, v) : 0 < u <= sqrt(pdf(v/u + c))}``.\n",
      "        If `(U, V)` is a random vector uniformly distributed over `A`,\n",
      "        then `V/U + c` follows a distribution according to `pdf`.\n",
      "        \n",
      "        The above result (see [1]_, [2]_) can be used to sample random variables\n",
      "        using only the pdf, i.e. no inversion of the cdf is required. Typical\n",
      "        choices of `c` are zero or the mode of `pdf`. The set `A` is a subset of\n",
      "        the rectangle ``R = [0, umax] x [vmin, vmax]`` where\n",
      "        \n",
      "        - ``umax = sup sqrt(pdf(x))``\n",
      "        - ``vmin = inf (x - c) sqrt(pdf(x))``\n",
      "        - ``vmax = sup (x - c) sqrt(pdf(x))``\n",
      "        \n",
      "        In particular, these values are finite if `pdf` is bounded and\n",
      "        ``x**2 * pdf(x)`` is bounded (i.e. subquadratic tails).\n",
      "        One can generate `(U, V)` uniformly on `R` and return\n",
      "        `V/U + c` if `(U, V)` are also in `A` which can be directly\n",
      "        verified.\n",
      "        \n",
      "        The algorithm is not changed if one replaces `pdf` by k * `pdf` for any\n",
      "        constant k > 0. Thus, it is often convenient to work with a function\n",
      "        that is proportional to the probability density function by dropping\n",
      "        unneccessary normalization factors.\n",
      "        \n",
      "        Intuitively, the method works well if `A` fills up most of the\n",
      "        enclosing rectangle such that the probability is high that `(U, V)`\n",
      "        lies in `A` whenever it lies in `R` as the number of required\n",
      "        iterations becomes too large otherwise. To be more precise, note that\n",
      "        the expected number of iterations to draw `(U, V)` uniformly\n",
      "        distributed on `R` such that `(U, V)` is also in `A` is given by\n",
      "        the ratio ``area(R) / area(A) = 2 * umax * (vmax - vmin) / area(pdf)``,\n",
      "        where `area(pdf)` is the integral of `pdf` (which is equal to one if the\n",
      "        probability density function is used but can take on other values if a\n",
      "        function proportional to the density is used). The equality holds since\n",
      "        the area of `A` is equal to 0.5 * area(pdf) (Theorem 7.1 in [1]_).\n",
      "        If the sampling fails to generate a single random variate after 50000\n",
      "        iterations (i.e. not a single draw is in `A`), an exception is raised.\n",
      "        \n",
      "        If the bounding rectangle is not correctly specified (i.e. if it does not\n",
      "        contain `A`), the algorithm samples from a distribution different from\n",
      "        the one given by `pdf`. It is therefore recommended to perform a\n",
      "        test such as `~scipy.stats.kstest` as a check.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] L. Devroye, \"Non-Uniform Random Variate Generation\",\n",
      "           Springer-Verlag, 1986.\n",
      "        \n",
      "        .. [2] W. Hoermann and J. Leydold, \"Generating generalized inverse Gaussian\n",
      "           random variates\", Statistics and Computing, 24(4), p. 547--557, 2014.\n",
      "        \n",
      "        .. [3] A.J. Kinderman and J.F. Monahan, \"Computer Generation of Random\n",
      "           Variables Using the Ratio of Uniform Deviates\",\n",
      "           ACM Transactions on Mathematical Software, 3(3), p. 257--260, 1977.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        Simulate normally distributed random variables. It is easy to compute the\n",
      "        bounding rectangle explicitly in that case. For simplicity, we drop the\n",
      "        normalization factor of the density.\n",
      "        \n",
      "        >>> f = lambda x: np.exp(-x**2 / 2)\n",
      "        >>> v_bound = np.sqrt(f(np.sqrt(2))) * np.sqrt(2)\n",
      "        >>> umax, vmin, vmax = np.sqrt(f(0)), -v_bound, v_bound\n",
      "        >>> np.random.seed(12345)\n",
      "        >>> rvs = stats.rvs_ratio_uniforms(f, umax, vmin, vmax, size=2500)\n",
      "        \n",
      "        The K-S test confirms that the random variates are indeed normally\n",
      "        distributed (normality is not rejected at 5% significance level):\n",
      "        \n",
      "        >>> stats.kstest(rvs, 'norm')[1]\n",
      "        0.33783681428365553\n",
      "        \n",
      "        The exponential distribution provides another example where the bounding\n",
      "        rectangle can be determined explicitly.\n",
      "        \n",
      "        >>> np.random.seed(12345)\n",
      "        >>> rvs = stats.rvs_ratio_uniforms(lambda x: np.exp(-x), umax=1,\n",
      "        ...                                vmin=0, vmax=2*np.exp(-1), size=1000)\n",
      "        >>> stats.kstest(rvs, 'expon')[1]\n",
      "        0.928454552559516\n",
      "    \n",
      "    scoreatpercentile(a, per, limit=(), interpolation_method='fraction', axis=None)\n",
      "        Calculate the score at a given percentile of the input sequence.\n",
      "        \n",
      "        For example, the score at `per=50` is the median. If the desired quantile\n",
      "        lies between two data points, we interpolate between them, according to\n",
      "        the value of `interpolation`. If the parameter `limit` is provided, it\n",
      "        should be a tuple (lower, upper) of two values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            A 1-D array of values from which to extract score.\n",
      "        per : array_like\n",
      "            Percentile(s) at which to extract score.  Values should be in range\n",
      "            [0,100].\n",
      "        limit : tuple, optional\n",
      "            Tuple of two scalars, the lower and upper limits within which to\n",
      "            compute the percentile. Values of `a` outside\n",
      "            this (closed) interval will be ignored.\n",
      "        interpolation_method : {'fraction', 'lower', 'higher'}, optional\n",
      "            Specifies the interpolation method to use,\n",
      "            when the desired quantile lies between two data points `i` and `j`\n",
      "            The following options are available (default is 'fraction'):\n",
      "        \n",
      "              * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the\n",
      "                fractional part of the index surrounded by ``i`` and ``j``\n",
      "              * 'lower': ``i``\n",
      "              * 'higher': ``j``\n",
      "        \n",
      "        axis : int, optional\n",
      "            Axis along which the percentiles are computed. Default is None. If\n",
      "            None, compute over the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray\n",
      "            Score at percentile(s).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        percentileofscore, numpy.percentile\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function will become obsolete in the future.\n",
      "        For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality\n",
      "        that `scoreatpercentile` provides.  And it's significantly faster.\n",
      "        Therefore it's recommended to use `numpy.percentile` for users that have\n",
      "        numpy >= 1.9.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(100)\n",
      "        >>> stats.scoreatpercentile(a, 50)\n",
      "        49.5\n",
      "    \n",
      "    sem(a, axis=0, ddof=1, nan_policy='propagate')\n",
      "        Compute standard error of the mean.\n",
      "        \n",
      "        Calculate the standard error of the mean (or standard error of\n",
      "        measurement) of the values in the input array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            An array containing the values for which the standard error is\n",
      "            returned.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees-of-freedom. How many degrees of freedom to adjust\n",
      "            for bias in limited samples relative to the population estimate\n",
      "            of variance. Defaults to 1.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s : ndarray or float\n",
      "            The standard error of the mean in the sample(s), along the input axis.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The default value for `ddof` is different to the default (0) used by other\n",
      "        ddof containing routines, such as np.std and np.nanstd.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find standard error along the first axis:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(20).reshape(5,4)\n",
      "        >>> stats.sem(a)\n",
      "        array([ 2.8284,  2.8284,  2.8284,  2.8284])\n",
      "        \n",
      "        Find standard error across the whole array, using n degrees of freedom:\n",
      "        \n",
      "        >>> stats.sem(a, axis=None, ddof=0)\n",
      "        1.2893796958227628\n",
      "    \n",
      "    shapiro(x)\n",
      "        Perform the Shapiro-Wilk test for normality.\n",
      "        \n",
      "        The Shapiro-Wilk test tests the null hypothesis that the\n",
      "        data was drawn from a normal distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Array of sample data.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        p-value : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        anderson : The Anderson-Darling test for normality\n",
      "        kstest : The Kolmogorov-Smirnov test for goodness of fit.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm used is described in [4]_ but censoring parameters as\n",
      "        described are not implemented. For N > 5000 the W test statistic is accurate\n",
      "        but the p-value may not be.\n",
      "        \n",
      "        The chance of rejecting the null hypothesis when it is true is close to 5%\n",
      "        regardless of sample size.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
      "        .. [2] Shapiro, S. S. & Wilk, M.B (1965). An analysis of variance test for\n",
      "               normality (complete samples), Biometrika, Vol. 52, pp. 591-611.\n",
      "        .. [3] Razali, N. M. & Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk,\n",
      "               Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of\n",
      "               Statistical Modeling and Analytics, Vol. 2, pp. 21-33.\n",
      "        .. [4] ALGORITHM AS R94 APPL. STATIST. (1995) VOL. 44, NO. 4.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(12345678)\n",
      "        >>> x = stats.norm.rvs(loc=5, scale=3, size=100)\n",
      "        >>> shapiro_test = stats.shapiro(x)\n",
      "        >>> shapiro_test\n",
      "        ShapiroResult(statistic=0.9772805571556091, pvalue=0.08144091814756393)\n",
      "        >>> shapiro_test.statistic\n",
      "        0.9772805571556091\n",
      "        >>> shapiro_test.pvalue\n",
      "        0.08144091814756393\n",
      "    \n",
      "    siegelslopes(y, x=None, method='hierarchical')\n",
      "        Computes the Siegel estimator for a set of points (x, y).\n",
      "        \n",
      "        `siegelslopes` implements a method for robust linear regression\n",
      "        using repeated medians (see [1]_) to fit a line to the points (x, y).\n",
      "        The method is robust to outliers with an asymptotic breakdown point\n",
      "        of 50%.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Dependent variable.\n",
      "        x : array_like or None, optional\n",
      "            Independent variable. If None, use ``arange(len(y))`` instead.\n",
      "        method : {'hierarchical', 'separate'}\n",
      "            If 'hierarchical', estimate the intercept using the estimated\n",
      "            slope ``medslope`` (default option).\n",
      "            If 'separate', estimate the intercept independent of the estimated\n",
      "            slope. See Notes for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        medslope : float\n",
      "            Estimate of the slope of the regression line.\n",
      "        medintercept : float\n",
      "            Estimate of the intercept of the regression line.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        theilslopes : a similar technique without repeated medians\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``n = len(y)``, compute ``m_j`` as the median of\n",
      "        the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\n",
      "        ``medslope`` is then the median of all slopes ``m_j``.\n",
      "        Two ways are given to estimate the intercept in [1]_ which can be chosen\n",
      "        via the parameter ``method``.\n",
      "        The hierarchical approach uses the estimated slope ``medslope``\n",
      "        and computes ``medintercept`` as the median of ``y - medslope*x``.\n",
      "        The other approach estimates the intercept separately as follows: for\n",
      "        each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\n",
      "        lines through the remaining points and take the median ``i_j``.\n",
      "        ``medintercept`` is the median of the ``i_j``.\n",
      "        \n",
      "        The implementation computes `n` times the median of a vector of size `n`\n",
      "        which can be slow for large vectors. There are more efficient algorithms\n",
      "        (see [2]_) which are not implemented here.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\n",
      "               Biometrika, Vol. 69, pp. 242-244, 1982.\n",
      "        \n",
      "        .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\n",
      "               line\", Proceedings of the Third Annual ACM-SIAM Symposium on\n",
      "               Discrete Algorithms, pp. 409-413, 1992.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, num=150)\n",
      "        >>> y = x + np.random.normal(size=x.size)\n",
      "        >>> y[11:15] += 10  # add outliers\n",
      "        >>> y[-5:] -= 7\n",
      "        \n",
      "        Compute the slope and intercept.  For comparison, also compute the\n",
      "        least-squares fit with `linregress`:\n",
      "        \n",
      "        >>> res = stats.siegelslopes(y, x)\n",
      "        >>> lsq_res = stats.linregress(x, y)\n",
      "        \n",
      "        Plot the results. The Siegel regression line is shown in red. The green\n",
      "        line shows the least-squares fit for comparison.\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, y, 'b.')\n",
      "        >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n",
      "        >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    sigmaclip(a, low=4.0, high=4.0)\n",
      "        Perform iterative sigma-clipping of array elements.\n",
      "        \n",
      "        Starting from the full sample, all elements outside the critical range are\n",
      "        removed, i.e. all elements of the input array `c` that satisfy either of\n",
      "        the following conditions::\n",
      "        \n",
      "            c < mean(c) - std(c)*low\n",
      "            c > mean(c) + std(c)*high\n",
      "        \n",
      "        The iteration continues with the updated sample until no\n",
      "        elements are outside the (updated) range.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Data array, will be raveled if not 1-D.\n",
      "        low : float, optional\n",
      "            Lower bound factor of sigma clipping. Default is 4.\n",
      "        high : float, optional\n",
      "            Upper bound factor of sigma clipping. Default is 4.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        clipped : ndarray\n",
      "            Input array with clipped elements removed.\n",
      "        lower : float\n",
      "            Lower threshold value use for clipping.\n",
      "        upper : float\n",
      "            Upper threshold value use for clipping.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import sigmaclip\n",
      "        >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n",
      "        ...                     np.linspace(0, 20, 5)))\n",
      "        >>> fact = 1.5\n",
      "        >>> c, low, upp = sigmaclip(a, fact, fact)\n",
      "        >>> c\n",
      "        array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n",
      "        >>> c.var(), c.std()\n",
      "        (0.00055555555555555165, 0.023570226039551501)\n",
      "        >>> low, c.mean() - fact*c.std(), c.min()\n",
      "        (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n",
      "        >>> upp, c.mean() + fact*c.std(), c.max()\n",
      "        (10.035355339059327, 10.035355339059327, 10.033333333333333)\n",
      "        \n",
      "        >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n",
      "        ...                     np.linspace(-100, -50, 3)))\n",
      "        >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n",
      "        >>> (c == np.linspace(9.5, 10.5, 11)).all()\n",
      "        True\n",
      "    \n",
      "    skew(a, axis=0, bias=True, nan_policy='propagate')\n",
      "        Compute the sample skewness of a data set.\n",
      "        \n",
      "        For normally distributed data, the skewness should be about zero. For\n",
      "        unimodal continuous distributions, a skewness value greater than zero means\n",
      "        that there is more weight in the right tail of the distribution. The\n",
      "        function `skewtest` can be used to determine if the skewness value\n",
      "        is close enough to zero, statistically speaking.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : ndarray\n",
      "            Input array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which skewness is calculated. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        bias : bool, optional\n",
      "            If False, then the calculations are corrected for statistical bias.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        skewness : ndarray\n",
      "            The skewness of values along an axis, returning 0 where all values are\n",
      "            equal.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The sample skewness is computed as the Fisher-Pearson coefficient\n",
      "        of skewness, i.e.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            g_1=\\frac{m_3}{m_2^{3/2}}\n",
      "        \n",
      "        where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i\n",
      "        \n",
      "        is the biased sample :math:`i\\texttt{th}` central moment, and :math:`\\bar{x}` is\n",
      "        the sample mean.  If ``bias`` is False, the calculations are\n",
      "        corrected for bias and the value computed is the adjusted\n",
      "        Fisher-Pearson standardized moment coefficient, i.e.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            G_1=\\frac{k_3}{k_2^{3/2}}=\n",
      "                \\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "           Section 2.2.24.1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skew\n",
      "        >>> skew([1, 2, 3, 4, 5])\n",
      "        0.0\n",
      "        >>> skew([2, 8, 0, 4, 1, 9, 9, 0])\n",
      "        0.2650554122698573\n",
      "    \n",
      "    skewtest(a, axis=0, nan_policy='propagate')\n",
      "        Test whether the skew is different from the normal distribution.\n",
      "        \n",
      "        This function tests the null hypothesis that the skewness of\n",
      "        the population that the sample was drawn from is the same\n",
      "        as that of a corresponding normal distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array\n",
      "            The data to be tested.\n",
      "        axis : int or None, optional\n",
      "           Axis along which statistics are calculated. Default is 0.\n",
      "           If None, compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed z-score for this test.\n",
      "        pvalue : float\n",
      "            Two-sided p-value for the hypothesis test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The sample size must be at least 8.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,\n",
      "                \"A suggestion for using powerful and informative tests of\n",
      "                normality\", American Statistician 44, pp. 316-321, 1990.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skewtest\n",
      "        >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "        SkewtestResult(statistic=1.0108048609177787, pvalue=0.3121098361421897)\n",
      "        >>> skewtest([2, 8, 0, 4, 1, 9, 9, 0])\n",
      "        SkewtestResult(statistic=0.44626385374196975, pvalue=0.6554066631275459)\n",
      "        >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8000])\n",
      "        SkewtestResult(statistic=3.571773510360407, pvalue=0.0003545719905823133)\n",
      "        >>> skewtest([100, 100, 100, 100, 100, 100, 100, 101])\n",
      "        SkewtestResult(statistic=3.5717766638478072, pvalue=0.000354567720281634)\n",
      "    \n",
      "    spearmanr(a, b=None, axis=0, nan_policy='propagate')\n",
      "        Calculate a Spearman correlation coefficient with associated p-value.\n",
      "        \n",
      "        The Spearman rank-order correlation coefficient is a nonparametric measure\n",
      "        of the monotonicity of the relationship between two datasets. Unlike the\n",
      "        Pearson correlation, the Spearman correlation does not assume that both\n",
      "        datasets are normally distributed. Like other correlation coefficients,\n",
      "        this one varies between -1 and +1 with 0 implying no correlation.\n",
      "        Correlations of -1 or +1 imply an exact monotonic relationship. Positive\n",
      "        correlations imply that as x increases, so does y. Negative correlations\n",
      "        imply that as x increases, y decreases.\n",
      "        \n",
      "        The p-value roughly indicates the probability of an uncorrelated system\n",
      "        producing datasets that have a Spearman correlation at least as extreme\n",
      "        as the one computed from these datasets. The p-values are not entirely\n",
      "        reliable but are probably reasonable for datasets larger than 500 or so.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : 1D or 2D array_like, b is optional\n",
      "            One or two 1-D or 2-D arrays containing multiple variables and\n",
      "            observations. When these are 1-D, each represents a vector of\n",
      "            observations of a single variable. For the behavior in the 2-D case,\n",
      "            see under ``axis``, below.\n",
      "            Both arrays need to have the same length in the ``axis`` dimension.\n",
      "        axis : int or None, optional\n",
      "            If axis=0 (default), then each column represents a variable, with\n",
      "            observations in the rows. If axis=1, the relationship is transposed:\n",
      "            each row represents a variable, while the columns contain observations.\n",
      "            If axis=None, then both arrays will be raveled.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float or ndarray (2-D square)\n",
      "            Spearman correlation matrix or correlation coefficient (if only 2\n",
      "            variables are given as parameters. Correlation matrix is square with\n",
      "            length equal to total number of variables (columns or rows) in ``a``\n",
      "            and ``b`` combined.\n",
      "        pvalue : float\n",
      "            The two-sided p-value for a hypothesis test whose null hypothesis is\n",
      "            that two sets of data are uncorrelated, has same dimension as rho.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "           Section  14.7\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])\n",
      "        (0.82078268166812329, 0.088587005313543798)\n",
      "        >>> np.random.seed(1234321)\n",
      "        >>> x2n = np.random.randn(100, 2)\n",
      "        >>> y2n = np.random.randn(100, 2)\n",
      "        >>> stats.spearmanr(x2n)\n",
      "        (0.059969996999699973, 0.55338590803773591)\n",
      "        >>> stats.spearmanr(x2n[:,0], x2n[:,1])\n",
      "        (0.059969996999699973, 0.55338590803773591)\n",
      "        >>> rho, pval = stats.spearmanr(x2n, y2n)\n",
      "        >>> rho\n",
      "        array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],\n",
      "               [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],\n",
      "               [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],\n",
      "               [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])\n",
      "        >>> pval\n",
      "        array([[ 0.        ,  0.55338591,  0.06435364,  0.53617935],\n",
      "               [ 0.55338591,  0.        ,  0.27592895,  0.80234077],\n",
      "               [ 0.06435364,  0.27592895,  0.        ,  0.73039992],\n",
      "               [ 0.53617935,  0.80234077,  0.73039992,  0.        ]])\n",
      "        >>> rho, pval = stats.spearmanr(x2n.T, y2n.T, axis=1)\n",
      "        >>> rho\n",
      "        array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626],\n",
      "               [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653],\n",
      "               [ 0.18569457,  0.110003  ,  1.        ,  0.03488749],\n",
      "               [ 0.06258626,  0.02534653,  0.03488749,  1.        ]])\n",
      "        >>> stats.spearmanr(x2n, y2n, axis=None)\n",
      "        (0.10816770419260482, 0.1273562188027364)\n",
      "        >>> stats.spearmanr(x2n.ravel(), y2n.ravel())\n",
      "        (0.10816770419260482, 0.1273562188027364)\n",
      "        \n",
      "        >>> xint = np.random.randint(10, size=(100, 2))\n",
      "        >>> stats.spearmanr(xint)\n",
      "        (0.052760927029710199, 0.60213045837062351)\n",
      "    \n",
      "    theilslopes(y, x=None, alpha=0.95)\n",
      "        Computes the Theil-Sen estimator for a set of points (x, y).\n",
      "        \n",
      "        `theilslopes` implements a method for robust linear regression.  It\n",
      "        computes the slope as the median of all slopes between paired values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Dependent variable.\n",
      "        x : array_like or None, optional\n",
      "            Independent variable. If None, use ``arange(len(y))`` instead.\n",
      "        alpha : float, optional\n",
      "            Confidence degree between 0 and 1. Default is 95% confidence.\n",
      "            Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\n",
      "            interpreted as \"find the 90% confidence interval\".\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        medslope : float\n",
      "            Theil slope.\n",
      "        medintercept : float\n",
      "            Intercept of the Theil line, as ``median(y) - medslope*median(x)``.\n",
      "        lo_slope : float\n",
      "            Lower bound of the confidence interval on `medslope`.\n",
      "        up_slope : float\n",
      "            Upper bound of the confidence interval on `medslope`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        siegelslopes : a similar technique using repeated medians\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The implementation of `theilslopes` follows [1]_. The intercept is\n",
      "        not defined in [1]_, and here it is defined as ``median(y) -\n",
      "        medslope*median(x)``, which is given in [3]_. Other definitions of\n",
      "        the intercept exist in the literature. A confidence interval for\n",
      "        the intercept is not given as this question is not addressed in\n",
      "        [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] P.K. Sen, \"Estimates of the regression coefficient based on Kendall's tau\",\n",
      "               J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\n",
      "        .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\n",
      "               regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\n",
      "               53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\n",
      "        .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\n",
      "               John Wiley and Sons, New York, pp. 493.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, num=150)\n",
      "        >>> y = x + np.random.normal(size=x.size)\n",
      "        >>> y[11:15] += 10  # add outliers\n",
      "        >>> y[-5:] -= 7\n",
      "        \n",
      "        Compute the slope, intercept and 90% confidence interval.  For comparison,\n",
      "        also compute the least-squares fit with `linregress`:\n",
      "        \n",
      "        >>> res = stats.theilslopes(y, x, 0.90)\n",
      "        >>> lsq_res = stats.linregress(x, y)\n",
      "        \n",
      "        Plot the results. The Theil-Sen regression line is shown in red, with the\n",
      "        dashed red lines illustrating the confidence interval of the slope (note\n",
      "        that the dashed red lines are not the confidence interval of the regression\n",
      "        as the confidence interval of the intercept is not included). The green\n",
      "        line shows the least-squares fit for comparison.\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, y, 'b.')\n",
      "        >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n",
      "        >>> ax.plot(x, res[1] + res[2] * x, 'r--')\n",
      "        >>> ax.plot(x, res[1] + res[3] * x, 'r--')\n",
      "        >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    tiecorrect(rankvals)\n",
      "        Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rankvals : array_like\n",
      "            A 1-D sequence of ranks.  Typically this will be the array\n",
      "            returned by `~scipy.stats.rankdata`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        factor : float\n",
      "            Correction factor for U or H.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rankdata : Assign ranks to the data\n",
      "        mannwhitneyu : Mann-Whitney rank test\n",
      "        kruskal : Kruskal-Wallis H test\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n",
      "               Sciences.  New York: McGraw-Hill.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import tiecorrect, rankdata\n",
      "        >>> tiecorrect([1, 2.5, 2.5, 4])\n",
      "        0.9\n",
      "        >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n",
      "        >>> ranks\n",
      "        array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n",
      "        >>> tiecorrect(ranks)\n",
      "        0.9833333333333333\n",
      "    \n",
      "    tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate')\n",
      "        Compute the trimmed maximum.\n",
      "        \n",
      "        This function computes the maximum value of an array along a given axis,\n",
      "        while ignoring values larger than a specified upper limit.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        upperlimit : None or float, optional\n",
      "            Values in the input array greater than the given limit will be ignored.\n",
      "            When upperlimit is None, then all values are used. The default value\n",
      "            is None.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        inclusive : {True, False}, optional\n",
      "            This flag determines whether values exactly equal to the upper limit\n",
      "            are included.  The default value is True.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tmax : float, int or ndarray\n",
      "            Trimmed maximum.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tmax(x)\n",
      "        19\n",
      "        \n",
      "        >>> stats.tmax(x, 13)\n",
      "        13\n",
      "        \n",
      "        >>> stats.tmax(x, 13, inclusive=False)\n",
      "        12\n",
      "    \n",
      "    tmean(a, limits=None, inclusive=(True, True), axis=None)\n",
      "        Compute the trimmed mean.\n",
      "        \n",
      "        This function finds the arithmetic mean of given values, ignoring values\n",
      "        outside the given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored.  When limits is None (default), then all\n",
      "            values are used.  Either of the limit values in the tuple can also be\n",
      "            None representing a half-open interval.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. Default is None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tmean : float\n",
      "            Trimmed mean.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        trim_mean : Returns mean after trimming a proportion from both tails.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tmean(x)\n",
      "        9.5\n",
      "        >>> stats.tmean(x, (3,17))\n",
      "        10.0\n",
      "    \n",
      "    tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate')\n",
      "        Compute the trimmed minimum.\n",
      "        \n",
      "        This function finds the miminum value of an array `a` along the\n",
      "        specified axis, but only considering values greater than a specified\n",
      "        lower limit.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        lowerlimit : None or float, optional\n",
      "            Values in the input array less than the given limit will be ignored.\n",
      "            When lowerlimit is None, then all values are used. The default value\n",
      "            is None.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        inclusive : {True, False}, optional\n",
      "            This flag determines whether values exactly equal to the lower limit\n",
      "            are included.  The default value is True.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tmin : float, int or ndarray\n",
      "            Trimmed minimum.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tmin(x)\n",
      "        0\n",
      "        \n",
      "        >>> stats.tmin(x, 13)\n",
      "        13\n",
      "        \n",
      "        >>> stats.tmin(x, 13, inclusive=False)\n",
      "        14\n",
      "    \n",
      "    trim1(a, proportiontocut, tail='right', axis=0)\n",
      "        Slice off a proportion from ONE end of the passed array distribution.\n",
      "        \n",
      "        If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n",
      "        10% of scores. The lowest or highest values are trimmed (depending on\n",
      "        the tail).\n",
      "        Slice off less if proportion results in a non-integer slice index\n",
      "        (i.e. conservatively slices off `proportiontocut` ).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        proportiontocut : float\n",
      "            Fraction to cut off of 'left' or 'right' of distribution.\n",
      "        tail : {'left', 'right'}, optional\n",
      "            Defaults to 'right'.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to trim data. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        trim1 : ndarray\n",
      "            Trimmed version of array `a`. The order of the trimmed content is\n",
      "            undefined.\n",
      "    \n",
      "    trim_mean(a, proportiontocut, axis=0)\n",
      "        Return mean of array after trimming distribution from both tails.\n",
      "        \n",
      "        If `proportiontocut` = 0.1, slices off 'leftmost' and 'rightmost' 10% of\n",
      "        scores. The input is sorted before slicing. Slices off less if proportion\n",
      "        results in a non-integer slice index (i.e., conservatively slices off\n",
      "        `proportiontocut` ).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        proportiontocut : float\n",
      "            Fraction to cut off of both tails of the distribution.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the trimmed means are computed. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        trim_mean : ndarray\n",
      "            Mean of trimmed array.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        trimboth\n",
      "        tmean : Compute the trimmed mean ignoring values outside given `limits`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.trim_mean(x, 0.1)\n",
      "        9.5\n",
      "        >>> x2 = x.reshape(5, 4)\n",
      "        >>> x2\n",
      "        array([[ 0,  1,  2,  3],\n",
      "               [ 4,  5,  6,  7],\n",
      "               [ 8,  9, 10, 11],\n",
      "               [12, 13, 14, 15],\n",
      "               [16, 17, 18, 19]])\n",
      "        >>> stats.trim_mean(x2, 0.25)\n",
      "        array([  8.,   9.,  10.,  11.])\n",
      "        >>> stats.trim_mean(x2, 0.25, axis=1)\n",
      "        array([  1.5,   5.5,   9.5,  13.5,  17.5])\n",
      "    \n",
      "    trimboth(a, proportiontocut, axis=0)\n",
      "        Slice off a proportion of items from both ends of an array.\n",
      "        \n",
      "        Slice off the passed proportion of items from both ends of the passed\n",
      "        array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n",
      "        rightmost 10% of scores). The trimmed values are the lowest and\n",
      "        highest ones.\n",
      "        Slice off less if proportion results in a non-integer slice index (i.e.\n",
      "        conservatively slices off `proportiontocut`).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Data to trim.\n",
      "        proportiontocut : float\n",
      "            Proportion (in range 0-1) of total data set to trim of each end.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to trim data. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Trimmed version of array `a`. The order of the trimmed content\n",
      "            is undefined.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        trim_mean\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(20)\n",
      "        >>> b = stats.trimboth(a, 0.1)\n",
      "        >>> b.shape\n",
      "        (16,)\n",
      "    \n",
      "    tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1)\n",
      "        Compute the trimmed standard error of the mean.\n",
      "        \n",
      "        This function finds the standard error of the mean for given\n",
      "        values, ignoring values outside the given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored. When limits is None, then all values are\n",
      "            used. Either of the limit values in the tuple can also be None\n",
      "            representing a half-open interval.  The default value is None.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tsem : float\n",
      "            Trimmed standard error of the mean.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `tsem` uses unbiased sample standard deviation, i.e. it uses a\n",
      "        correction factor ``n / (n - 1)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tsem(x)\n",
      "        1.3228756555322954\n",
      "        >>> stats.tsem(x, (3,17))\n",
      "        1.1547005383792515\n",
      "    \n",
      "    tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1)\n",
      "        Compute the trimmed sample standard deviation.\n",
      "        \n",
      "        This function finds the sample standard deviation of given values,\n",
      "        ignoring values outside the given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored. When limits is None, then all values are\n",
      "            used. Either of the limit values in the tuple can also be None\n",
      "            representing a half-open interval.  The default value is None.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tstd : float\n",
      "            Trimmed sample standard deviation.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n",
      "        correction factor ``n / (n - 1)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tstd(x)\n",
      "        5.9160797830996161\n",
      "        >>> stats.tstd(x, (3,17))\n",
      "        4.4721359549995796\n",
      "    \n",
      "    ttest_1samp(a, popmean, axis=0, nan_policy='propagate')\n",
      "        Calculate the T-test for the mean of ONE group of scores.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that the expected value\n",
      "        (mean) of a sample of independent observations `a` is equal to the given\n",
      "        population mean, `popmean`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Sample observation.\n",
      "        popmean : float or array_like\n",
      "            Expected value in null hypothesis. If array_like, then it must have the\n",
      "            same shape as `a` excluding the axis dimension.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. If None, compute over the whole\n",
      "            array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            t-statistic.\n",
      "        pvalue : float or array\n",
      "            Two-sided p-value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        >>> np.random.seed(7654567)  # fix seed to get the same result\n",
      "        >>> rvs = stats.norm.rvs(loc=5, scale=10, size=(50,2))\n",
      "        \n",
      "        Test if mean of random sample is equal to true mean, and different mean.\n",
      "        We reject the null hypothesis in the second case and don't reject it in\n",
      "        the first case.\n",
      "        \n",
      "        >>> stats.ttest_1samp(rvs,5.0)\n",
      "        (array([-0.68014479, -0.04323899]), array([ 0.49961383,  0.96568674]))\n",
      "        >>> stats.ttest_1samp(rvs,0.0)\n",
      "        (array([ 2.77025808,  4.11038784]), array([ 0.00789095,  0.00014999]))\n",
      "        \n",
      "        Examples using axis and non-scalar dimension for population mean.\n",
      "        \n",
      "        >>> stats.ttest_1samp(rvs,[5.0,0.0])\n",
      "        (array([-0.68014479,  4.11038784]), array([  4.99613833e-01,   1.49986458e-04]))\n",
      "        >>> stats.ttest_1samp(rvs.T,[5.0,0.0],axis=1)\n",
      "        (array([-0.68014479,  4.11038784]), array([  4.99613833e-01,   1.49986458e-04]))\n",
      "        >>> stats.ttest_1samp(rvs,[[5.0],[0.0]])\n",
      "        (array([[-0.68014479, -0.04323899],\n",
      "               [ 2.77025808,  4.11038784]]), array([[  4.99613833e-01,   9.65686743e-01],\n",
      "               [  7.89094663e-03,   1.49986458e-04]]))\n",
      "    \n",
      "    ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate')\n",
      "        Calculate the T-test for the means of *two independent* samples of scores.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that 2 independent samples\n",
      "        have identical average (expected) values. This test assumes that the\n",
      "        populations have identical variances by default.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            The arrays must have the same shape, except in the dimension\n",
      "            corresponding to `axis` (the first, by default).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. If None, compute over the whole\n",
      "            arrays, `a`, and `b`.\n",
      "        equal_var : bool, optional\n",
      "            If True (default), perform a standard independent 2 sample test\n",
      "            that assumes equal population variances [1]_.\n",
      "            If False, perform Welch's t-test, which does not assume equal\n",
      "            population variance [2]_.\n",
      "        \n",
      "            .. versionadded:: 0.11.0\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            The calculated t-statistic.\n",
      "        pvalue : float or array\n",
      "            The two-tailed p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        We can use this test, if we observe two independent samples from\n",
      "        the same or different population, e.g. exam scores of boys and\n",
      "        girls or of two ethnic groups. The test measures whether the\n",
      "        average (expected) value differs significantly across samples. If\n",
      "        we observe a large p-value, for example larger than 0.05 or 0.1,\n",
      "        then we cannot reject the null hypothesis of identical average scores.\n",
      "        If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%,\n",
      "        then we reject the null hypothesis of equal averages.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "        \n",
      "        .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(12345678)\n",
      "        \n",
      "        Test with sample with identical means:\n",
      "        \n",
      "        >>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "        >>> rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "        >>> stats.ttest_ind(rvs1,rvs2)\n",
      "        (0.26833823296239279, 0.78849443369564776)\n",
      "        >>> stats.ttest_ind(rvs1,rvs2, equal_var = False)\n",
      "        (0.26833823296239279, 0.78849452749500748)\n",
      "        \n",
      "        `ttest_ind` underestimates p for unequal variances:\n",
      "        \n",
      "        >>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\n",
      "        >>> stats.ttest_ind(rvs1, rvs3)\n",
      "        (-0.46580283298287162, 0.64145827413436174)\n",
      "        >>> stats.ttest_ind(rvs1, rvs3, equal_var = False)\n",
      "        (-0.46580283298287162, 0.64149646246569292)\n",
      "        \n",
      "        When n1 != n2, the equal variance t-statistic is no longer equal to the\n",
      "        unequal variance t-statistic:\n",
      "        \n",
      "        >>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\n",
      "        >>> stats.ttest_ind(rvs1, rvs4)\n",
      "        (-0.99882539442782481, 0.3182832709103896)\n",
      "        >>> stats.ttest_ind(rvs1, rvs4, equal_var = False)\n",
      "        (-0.69712570584654099, 0.48716927725402048)\n",
      "        \n",
      "        T-test with different means, variance, and n:\n",
      "        \n",
      "        >>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\n",
      "        >>> stats.ttest_ind(rvs1, rvs5)\n",
      "        (-1.4679669854490653, 0.14263895620529152)\n",
      "        >>> stats.ttest_ind(rvs1, rvs5, equal_var = False)\n",
      "        (-0.94365973617132992, 0.34744170334794122)\n",
      "    \n",
      "    ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2, equal_var=True)\n",
      "        T-test for means of two independent samples from descriptive statistics.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that two independent\n",
      "        samples have identical average (expected) values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        mean1 : array_like\n",
      "            The mean(s) of sample 1.\n",
      "        std1 : array_like\n",
      "            The standard deviation(s) of sample 1.\n",
      "        nobs1 : array_like\n",
      "            The number(s) of observations of sample 1.\n",
      "        mean2 : array_like\n",
      "            The mean(s) of sample 2.\n",
      "        std2 : array_like\n",
      "            The standard deviations(s) of sample 2.\n",
      "        nobs2 : array_like\n",
      "            The number(s) of observations of sample 2.\n",
      "        equal_var : bool, optional\n",
      "            If True (default), perform a standard independent 2 sample test\n",
      "            that assumes equal population variances [1]_.\n",
      "            If False, perform Welch's t-test, which does not assume equal\n",
      "            population variance [2]_.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            The calculated t-statistics.\n",
      "        pvalue : float or array\n",
      "            The two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.ttest_ind\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "        \n",
      "        .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Suppose we have the summary data for two samples, as follows::\n",
      "        \n",
      "                             Sample   Sample\n",
      "                       Size   Mean   Variance\n",
      "            Sample 1    13    15.0     87.5\n",
      "            Sample 2    11    12.0     39.0\n",
      "        \n",
      "        Apply the t-test to this data (with the assumption that the population\n",
      "        variances are equal):\n",
      "        \n",
      "        >>> from scipy.stats import ttest_ind_from_stats\n",
      "        >>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,\n",
      "        ...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)\n",
      "        Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)\n",
      "        \n",
      "        For comparison, here is the data from which those summary statistics\n",
      "        were taken.  With this data, we can compute the same result using\n",
      "        `scipy.stats.ttest_ind`:\n",
      "        \n",
      "        >>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])\n",
      "        >>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])\n",
      "        >>> from scipy.stats import ttest_ind\n",
      "        >>> ttest_ind(a, b)\n",
      "        Ttest_indResult(statistic=0.905135809331027, pvalue=0.3751996797581486)\n",
      "        \n",
      "        Suppose we instead have binary data and would like to apply a t-test to\n",
      "        compare the proportion of 1s in two independent groups::\n",
      "        \n",
      "                              Number of    Sample     Sample\n",
      "                        Size    ones        Mean     Variance\n",
      "            Sample 1    150      30         0.2        0.16\n",
      "            Sample 2    200      45         0.225      0.174375\n",
      "        \n",
      "        The sample mean :math:`\\hat{p}` is the proportion of ones in the sample\n",
      "        and the variance for a binary observation is estimated by\n",
      "        :math:`\\hat{p}(1-\\hat{p})`.\n",
      "        \n",
      "        >>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.16), nobs1=150,\n",
      "        ...                      mean2=0.225, std2=np.sqrt(0.17437), nobs2=200)\n",
      "        Ttest_indResult(statistic=-0.564327545549774, pvalue=0.5728947691244874)\n",
      "        \n",
      "        For comparison, we could compute the t statistic and p-value using\n",
      "        arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.\n",
      "        \n",
      "        >>> group1 = np.array([1]*30 + [0]*(150-30))\n",
      "        >>> group2 = np.array([1]*45 + [0]*(200-45))\n",
      "        >>> ttest_ind(group1, group2)\n",
      "        Ttest_indResult(statistic=-0.5627179589855622, pvalue=0.573989277115258)\n",
      "    \n",
      "    ttest_rel(a, b, axis=0, nan_policy='propagate')\n",
      "        Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that 2 related or\n",
      "        repeated samples have identical average (expected) values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            The arrays must have the same shape.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. If None, compute over the whole\n",
      "            arrays, `a`, and `b`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            t-statistic.\n",
      "        pvalue : float or array\n",
      "            Two-sided p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Examples for use are scores of the same set of student in\n",
      "        different exams, or repeated sampling from the same units. The\n",
      "        test measures whether the average score differs significantly\n",
      "        across samples (e.g. exams). If we observe a large p-value, for\n",
      "        example greater than 0.05 or 0.1 then we cannot reject the null\n",
      "        hypothesis of identical average scores. If the p-value is smaller\n",
      "        than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n",
      "        hypothesis of equal averages. Small p-values are associated with\n",
      "        large t-statistics.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> np.random.seed(12345678) # fix random seed to get same numbers\n",
      "        \n",
      "        >>> rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
      "        >>> rvs2 = (stats.norm.rvs(loc=5,scale=10,size=500) +\n",
      "        ...         stats.norm.rvs(scale=0.2,size=500))\n",
      "        >>> stats.ttest_rel(rvs1,rvs2)\n",
      "        (0.24101764965300962, 0.80964043445811562)\n",
      "        >>> rvs3 = (stats.norm.rvs(loc=8,scale=10,size=500) +\n",
      "        ...         stats.norm.rvs(scale=0.2,size=500))\n",
      "        >>> stats.ttest_rel(rvs1,rvs3)\n",
      "        (-3.9995108708727933, 7.3082402191726459e-005)\n",
      "    \n",
      "    tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1)\n",
      "        Compute the trimmed variance.\n",
      "        \n",
      "        This function computes the sample variance of an array of values,\n",
      "        while ignoring values which are outside of given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored. When limits is None, then all values are\n",
      "            used. Either of the limit values in the tuple can also be None\n",
      "            representing a half-open interval.  The default value is None.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tvar : float\n",
      "            Trimmed variance.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `tvar` computes the unbiased sample variance, i.e. it uses a correction\n",
      "        factor ``n / (n - 1)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tvar(x)\n",
      "        35.0\n",
      "        >>> stats.tvar(x, (3,17))\n",
      "        20.0\n",
      "    \n",
      "    variation(a, axis=0, nan_policy='propagate')\n",
      "        Compute the coefficient of variation.\n",
      "        \n",
      "        The coefficient of variation is the ratio of the biased standard\n",
      "        deviation to the mean.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to calculate the coefficient of variation. Default\n",
      "            is 0. If None, compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        variation : ndarray\n",
      "            The calculated variation along the requested axis.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import variation\n",
      "        >>> variation([1, 2, 3, 4, 5])\n",
      "        0.47140452079103173\n",
      "    \n",
      "    wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None)\n",
      "        Compute the first Wasserstein distance between two 1D distributions.\n",
      "        \n",
      "        This distance is also known as the earth mover's distance, since it can be\n",
      "        seen as the minimum amount of \"work\" required to transform :math:`u` into\n",
      "        :math:`v`, where \"work\" is measured as the amount of distribution weight\n",
      "        that must be moved, multiplied by the distance it has to be moved.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u_values, v_values : array_like\n",
      "            Values observed in the (empirical) distribution.\n",
      "        u_weights, v_weights : array_like, optional\n",
      "            Weight for each value. If unspecified, each value is assigned the same\n",
      "            weight.\n",
      "            `u_weights` (resp. `v_weights`) must have the same length as\n",
      "            `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
      "            must still be positive and finite so that the weights can be normalized\n",
      "            to sum to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance : float\n",
      "            The computed distance between the distributions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The first Wasserstein distance between the distributions :math:`u` and\n",
      "        :math:`v` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int_{\\mathbb{R} \\times\n",
      "            \\mathbb{R}} |x-y| \\mathrm{d} \\pi (x, y)\n",
      "        \n",
      "        where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n",
      "        :math:`\\mathbb{R} \\times \\mathbb{R}` whose marginals are :math:`u` and\n",
      "        :math:`v` on the first and second factors respectively.\n",
      "        \n",
      "        If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and\n",
      "        :math:`v`, this distance also equals to:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            l_1(u, v) = \\int_{-\\infty}^{+\\infty} |U-V|\n",
      "        \n",
      "        See [2]_ for a proof of the equivalence of both definitions.\n",
      "        \n",
      "        The input distributions can be empirical, therefore coming from samples\n",
      "        whose values are effectively inputs of the function, or they can be seen as\n",
      "        generalized functions, in which case they are weighted sums of Dirac delta\n",
      "        functions located at the specified values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Wasserstein metric\", https://en.wikipedia.org/wiki/Wasserstein_metric\n",
      "        .. [2] Ramdas, Garcia, Cuturi \"On Wasserstein Two Sample Testing and Related\n",
      "               Families of Nonparametric Tests\" (2015). :arXiv:`1509.02237`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import wasserstein_distance\n",
      "        >>> wasserstein_distance([0, 1, 3], [5, 6, 8])\n",
      "        5.0\n",
      "        >>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])\n",
      "        0.25\n",
      "        >>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],\n",
      "        ...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])\n",
      "        4.0781331438047861\n",
      "    \n",
      "    weightedtau(x, y, rank=True, weigher=None, additive=True)\n",
      "        Compute a weighted version of Kendall's :math:`\\tau`.\n",
      "        \n",
      "        The weighted :math:`\\tau` is a weighted version of Kendall's\n",
      "        :math:`\\tau` in which exchanges of high weight are more influential than\n",
      "        exchanges of low weight. The default parameters compute the additive\n",
      "        hyperbolic version of the index, :math:`\\tau_\\mathrm h`, which has\n",
      "        been shown to provide the best balance between important and\n",
      "        unimportant elements [1]_.\n",
      "        \n",
      "        The weighting is defined by means of a rank array, which assigns a\n",
      "        nonnegative rank to each element, and a weigher function, which\n",
      "        assigns a weight based from the rank to each element. The weight of an\n",
      "        exchange is then the sum or the product of the weights of the ranks of\n",
      "        the exchanged elements. The default parameters compute\n",
      "        :math:`\\tau_\\mathrm h`: an exchange between elements with rank\n",
      "        :math:`r` and :math:`s` (starting from zero) has weight\n",
      "        :math:`1/(r+1) + 1/(s+1)`.\n",
      "        \n",
      "        Specifying a rank array is meaningful only if you have in mind an\n",
      "        external criterion of importance. If, as it usually happens, you do\n",
      "        not have in mind a specific rank, the weighted :math:`\\tau` is\n",
      "        defined by averaging the values obtained using the decreasing\n",
      "        lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the\n",
      "        behavior with default parameters.\n",
      "        \n",
      "        Note that if you are computing the weighted :math:`\\tau` on arrays of\n",
      "        ranks, rather than of scores (i.e., a larger value implies a lower\n",
      "        rank) you must negate the ranks, so that elements of higher rank are\n",
      "        associated with a larger value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of scores, of the same shape. If arrays are not 1-D, they will\n",
      "            be flattened to 1-D.\n",
      "        rank : array_like of ints or bool, optional\n",
      "            A nonnegative rank assigned to each element. If it is None, the\n",
      "            decreasing lexicographical rank by (`x`, `y`) will be used: elements of\n",
      "            higher rank will be those with larger `x`-values, using `y`-values to\n",
      "            break ties (in particular, swapping `x` and `y` will give a different\n",
      "            result). If it is False, the element indices will be used\n",
      "            directly as ranks. The default is True, in which case this\n",
      "            function returns the average of the values obtained using the\n",
      "            decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).\n",
      "        weigher : callable, optional\n",
      "            The weigher function. Must map nonnegative integers (zero\n",
      "            representing the most important element) to a nonnegative weight.\n",
      "            The default, None, provides hyperbolic weighing, that is,\n",
      "            rank :math:`r` is mapped to weight :math:`1/(r+1)`.\n",
      "        additive : bool, optional\n",
      "            If True, the weight of an exchange is computed by adding the\n",
      "            weights of the ranks of the exchanged elements; otherwise, the weights\n",
      "            are multiplied. The default is True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float\n",
      "           The weighted :math:`\\tau` correlation index.\n",
      "        pvalue : float\n",
      "           Presently ``np.nan``, as the null statistics is unknown (even in the\n",
      "           additive hyperbolic case).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kendalltau : Calculates Kendall's tau.\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function uses an :math:`O(n \\log n)`, mergesort-based algorithm\n",
      "        [1]_ that is a weighted extension of Knight's algorithm for Kendall's\n",
      "        :math:`\\tau` [2]_. It can compute Shieh's weighted :math:`\\tau` [3]_\n",
      "        between rankings without ties (i.e., permutations) by setting\n",
      "        `additive` and `rank` to False, as the definition given in [1]_ is a\n",
      "        generalization of Shieh's.\n",
      "        \n",
      "        NaNs are considered the smallest possible score.\n",
      "        \n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Sebastiano Vigna, \"A weighted correlation index for rankings with\n",
      "               ties\", Proceedings of the 24th international conference on World\n",
      "               Wide Web, pp. 1166-1176, ACM, 2015.\n",
      "        .. [2] W.R. Knight, \"A Computer Method for Calculating Kendall's Tau with\n",
      "               Ungrouped Data\", Journal of the American Statistical Association,\n",
      "               Vol. 61, No. 314, Part 1, pp. 436-439, 1966.\n",
      "        .. [3] Grace S. Shieh. \"A weighted Kendall's tau statistic\", Statistics &\n",
      "               Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, 0]\n",
      "        >>> tau, p_value = stats.weightedtau(x, y)\n",
      "        >>> tau\n",
      "        -0.56694968153682723\n",
      "        >>> p_value\n",
      "        nan\n",
      "        >>> tau, p_value = stats.weightedtau(x, y, additive=False)\n",
      "        >>> tau\n",
      "        -0.62205716951801038\n",
      "        \n",
      "        NaNs are considered the smallest possible score:\n",
      "        \n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, np.nan]\n",
      "        >>> tau, _ = stats.weightedtau(x, y)\n",
      "        >>> tau\n",
      "        -0.56694968153682723\n",
      "        \n",
      "        This is exactly Kendall's tau:\n",
      "        \n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, 0]\n",
      "        >>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)\n",
      "        >>> tau\n",
      "        -0.47140452079103173\n",
      "        \n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, 0]\n",
      "        >>> stats.weightedtau(x, y, rank=None)\n",
      "        WeightedTauResult(correlation=-0.4157652301037516, pvalue=nan)\n",
      "        >>> stats.weightedtau(y, x, rank=None)\n",
      "        WeightedTauResult(correlation=-0.7181341329699028, pvalue=nan)\n",
      "    \n",
      "    wilcoxon(x, y=None, zero_method='wilcox', correction=False, alternative='two-sided', mode='auto')\n",
      "        Calculate the Wilcoxon signed-rank test.\n",
      "        \n",
      "        The Wilcoxon signed-rank test tests the null hypothesis that two\n",
      "        related paired samples come from the same distribution. In particular,\n",
      "        it tests whether the distribution of the differences x - y is symmetric\n",
      "        about zero. It is a non-parametric version of the paired T-test.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Either the first set of measurements (in which case `y` is the second\n",
      "            set of measurements), or the differences between two sets of\n",
      "            measurements (in which case `y` is not to be specified.)  Must be\n",
      "            one-dimensional.\n",
      "        y : array_like, optional\n",
      "            Either the second set of measurements (if `x` is the first set of\n",
      "            measurements), or not specified (if `x` is the differences between\n",
      "            two sets of measurements.)  Must be one-dimensional.\n",
      "        zero_method : {'pratt', 'wilcox', 'zsplit'}, optional\n",
      "            The following options are available (default is 'wilcox'):\n",
      "         \n",
      "              * 'pratt': Includes zero-differences in the ranking process,\n",
      "                but drops the ranks of the zeros, see [4]_, (more conservative).\n",
      "              * 'wilcox': Discards all zero-differences, the default.\n",
      "              * 'zsplit': Includes zero-differences in the ranking process and \n",
      "                split the zero rank between positive and negative ones.\n",
      "        correction : bool, optional\n",
      "            If True, apply continuity correction by adjusting the Wilcoxon rank\n",
      "            statistic by 0.5 towards the mean value when computing the\n",
      "            z-statistic if a normal approximation is used.  Default is False.\n",
      "        alternative : {\"two-sided\", \"greater\", \"less\"}, optional\n",
      "            The alternative hypothesis to be tested, see Notes. Default is\n",
      "            \"two-sided\".\n",
      "        mode : {\"auto\", \"exact\", \"approx\"}\n",
      "            Method to calculate the p-value, see Notes. Default is \"auto\".\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            If `alternative` is \"two-sided\", the sum of the ranks of the\n",
      "            differences above or below zero, whichever is smaller.\n",
      "            Otherwise the sum of the ranks of the differences above zero.\n",
      "        pvalue : float\n",
      "            The p-value for the test depending on `alternative` and `mode`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kruskal, mannwhitneyu\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The test has been introduced in [4]_. Given n independent samples\n",
      "        (xi, yi) from a bivariate distribution (i.e. paired samples),\n",
      "        it computes the differences di = xi - yi. One assumption of the test\n",
      "        is that the differences are symmetric, see [2]_.\n",
      "        The two-sided test has the null hypothesis that the median of the\n",
      "        differences is zero against the alternative that it is different from\n",
      "        zero. The one-sided test has the null hypothesis that the median is \n",
      "        positive against the alternative that it is negative \n",
      "        (``alternative == 'less'``), or vice versa (``alternative == 'greater.'``).\n",
      "        \n",
      "        To derive the p-value, the exact distribution (``mode == 'exact'``)\n",
      "        can be used for sample sizes of up to 25. The default ``mode == 'auto'``\n",
      "        uses the exact distribution if there are at most 25 observations and no\n",
      "        ties, otherwise a normal approximation is used (``mode == 'approx'``).\n",
      "        \n",
      "        The treatment of ties can be controlled by the parameter `zero_method`.\n",
      "        If ``zero_method == 'pratt'``, the normal approximation is adjusted as in\n",
      "        [5]_. A typical rule is to require that n > 20 ([2]_, p. 383).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
      "        .. [2] Conover, W.J., Practical Nonparametric Statistics, 1971.\n",
      "        .. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed\n",
      "           Rank Procedures, Journal of the American Statistical Association,\n",
      "           Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526`\n",
      "        .. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods,\n",
      "           Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968`\n",
      "        .. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank\n",
      "           Sampling Distribution When Zero Differences are Present,\n",
      "           Journal of the American Statistical Association, Vol. 62, 1967,\n",
      "           pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In [4]_, the differences in height between cross- and self-fertilized\n",
      "        corn plants is given as follows:\n",
      "        \n",
      "        >>> d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75]\n",
      "        \n",
      "        Cross-fertilized plants appear to be be higher. To test the null\n",
      "        hypothesis that there is no height difference, we can apply the\n",
      "        two-sided test:\n",
      "        \n",
      "        >>> from scipy.stats import wilcoxon\n",
      "        >>> w, p = wilcoxon(d)\n",
      "        >>> w, p\n",
      "        (24.0, 0.041259765625)\n",
      "        \n",
      "        Hence, we would reject the null hypothesis at a confidence level of 5%,\n",
      "        concluding that there is a difference in height between the groups.\n",
      "        To confirm that the median of the differences can be assumed to be\n",
      "        positive, we use:\n",
      "        \n",
      "        >>> w, p = wilcoxon(d, alternative='greater')\n",
      "        >>> w, p\n",
      "        (96.0, 0.0206298828125)\n",
      "        \n",
      "        This shows that the null hypothesis that the median is negative can be\n",
      "        rejected at a confidence level of 5% in favor of the alternative that\n",
      "        the median is greater than zero. The p-values above are exact. Using the\n",
      "        normal approximation gives very similar values:\n",
      "        \n",
      "        >>> w, p = wilcoxon(d, mode='approx')\n",
      "        >>> w, p\n",
      "        (24.0, 0.04088813291185591)\n",
      "        \n",
      "        Note that the statistic changed to 96 in the one-sided case (the sum\n",
      "        of ranks of positive differences) whereas it is 24 in the two-sided\n",
      "        case (the minimum of sum of ranks above and below zero).\n",
      "    \n",
      "    yeojohnson(x, lmbda=None)\n",
      "        Return a dataset transformed by a Yeo-Johnson power transformation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.  Should be 1-dimensional.\n",
      "        lmbda : float, optional\n",
      "            If ``lmbda`` is ``None``, find the lambda that maximizes the\n",
      "            log-likelihood function and return it as the second output argument.\n",
      "            Otherwise the transformation is done for the given value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        yeojohnson: ndarray\n",
      "            Yeo-Johnson power transformed array.\n",
      "        maxlog : float, optional\n",
      "            If the `lmbda` parameter is None, the second returned argument is\n",
      "            the lambda that maximizes the log-likelihood function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Yeo-Johnson transform is given by::\n",
      "        \n",
      "            y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0\n",
      "                log(x + 1),                                  for x >= 0, lmbda = 0\n",
      "                -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2\n",
      "                -log(-x + 1),                                for x < 0, lmbda = 2\n",
      "        \n",
      "        Unlike `boxcox`, `yeojohnson` does not require the input data to be\n",
      "        positive.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        I. Yeo and R.A. Johnson, \"A New Family of Power Transformations to\n",
      "        Improve Normality or Symmetry\", Biometrika 87.4 (2000):\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        We generate some random variates from a non-normal distribution and make a\n",
      "        probability plot for it, to show it is non-normal in the tails:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax1 = fig.add_subplot(211)\n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
      "        >>> ax1.set_xlabel('')\n",
      "        >>> ax1.set_title('Probplot against normal distribution')\n",
      "        \n",
      "        We now use `yeojohnson` to transform the data so it's closest to normal:\n",
      "        \n",
      "        >>> ax2 = fig.add_subplot(212)\n",
      "        >>> xt, lmbda = stats.yeojohnson(x)\n",
      "        >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
      "        >>> ax2.set_title('Probplot after Yeo-Johnson transformation')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    yeojohnson_llf(lmb, data)\n",
      "        The yeojohnson log-likelihood function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        lmb : scalar\n",
      "            Parameter for Yeo-Johnson transformation. See `yeojohnson` for\n",
      "            details.\n",
      "        data : array_like\n",
      "            Data to calculate Yeo-Johnson log-likelihood for. If `data` is\n",
      "            multi-dimensional, the log-likelihood is calculated along the first\n",
      "            axis.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        llf : float\n",
      "            Yeo-Johnson log-likelihood of `data` given `lmb`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Yeo-Johnson log-likelihood function is defined here as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            llf = N/2 \\log(\\hat{\\sigma}^2) + (\\lambda - 1)\n",
      "                  \\sum_i \\text{ sign }(x_i)\\log(|x_i| + 1)\n",
      "        \n",
      "        where :math:`\\hat{\\sigma}^2` is estimated variance of the the Yeo-Johnson\n",
      "        transformed input data ``x``.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
      "        >>> np.random.seed(1245)\n",
      "        \n",
      "        Generate some random variates and calculate Yeo-Johnson log-likelihood\n",
      "        values for them for a range of ``lmbda`` values:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n",
      "        >>> lmbdas = np.linspace(-2, 10)\n",
      "        >>> llf = np.zeros(lmbdas.shape, dtype=float)\n",
      "        >>> for ii, lmbda in enumerate(lmbdas):\n",
      "        ...     llf[ii] = stats.yeojohnson_llf(lmbda, x)\n",
      "        \n",
      "        Also find the optimal lmbda value with `yeojohnson`:\n",
      "        \n",
      "        >>> x_most_normal, lmbda_optimal = stats.yeojohnson(x)\n",
      "        \n",
      "        Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n",
      "        horizontal line to check that that's really the optimum:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(lmbdas, llf, 'b.-')\n",
      "        >>> ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r')\n",
      "        >>> ax.set_xlabel('lmbda parameter')\n",
      "        >>> ax.set_ylabel('Yeo-Johnson log-likelihood')\n",
      "        \n",
      "        Now add some probability plots to show that where the log-likelihood is\n",
      "        maximized the data transformed with `yeojohnson` looks closest to normal:\n",
      "        \n",
      "        >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
      "        >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
      "        ...     xt = stats.yeojohnson(x, lmbda=lmbda)\n",
      "        ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
      "        ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
      "        ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
      "        ...     ax_inset.set_xticklabels([])\n",
      "        ...     ax_inset.set_yticklabels([])\n",
      "        ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    yeojohnson_normmax(x, brack=(-2, 2))\n",
      "        Compute optimal Yeo-Johnson transform parameter.\n",
      "        \n",
      "        Compute optimal Yeo-Johnson transform parameter for input data, using\n",
      "        maximum likelihood estimation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        brack : 2-tuple, optional\n",
      "            The starting interval for a downhill bracket search with\n",
      "            `optimize.brent`. Note that this is in most cases not critical; the\n",
      "            final result is allowed to be outside this bracket.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        maxlog : float\n",
      "            The optimal transform parameter found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        yeojohnson, yeojohnson_llf, yeojohnson_normplot\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> np.random.seed(1234)  # make this example reproducible\n",
      "        \n",
      "        Generate some data and determine optimal ``lmbda``\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, size=30) + 5\n",
      "        >>> lmax = stats.yeojohnson_normmax(x)\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)\n",
      "        >>> ax.axvline(lmax, color='r')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    yeojohnson_normplot(x, la, lb, plot=None, N=80)\n",
      "        Compute parameters for a Yeo-Johnson normality plot, optionally show it.\n",
      "        \n",
      "        A Yeo-Johnson normality plot shows graphically what the best\n",
      "        transformation parameter is to use in `yeojohnson` to obtain a\n",
      "        distribution that is close to normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        la, lb : scalar\n",
      "            The lower and upper bounds for the ``lmbda`` values to pass to\n",
      "            `yeojohnson` for Yeo-Johnson transformations. These are also the\n",
      "            limits of the horizontal axis of the plot if that is generated.\n",
      "        plot : object, optional\n",
      "            If given, plots the quantiles and least squares fit.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        N : int, optional\n",
      "            Number of points on the horizontal axis (equally distributed from\n",
      "            `la` to `lb`).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lmbdas : ndarray\n",
      "            The ``lmbda`` values for which a Yeo-Johnson transform was done.\n",
      "        ppcc : ndarray\n",
      "            Probability Plot Correlelation Coefficient, as obtained from `probplot`\n",
      "            when fitting the Box-Cox transformed input `x` against a normal\n",
      "            distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Even if `plot` is given, the figure is not shown or saved by\n",
      "        `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n",
      "        should be used after calling `probplot`.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Generate some non-normally distributed data, and create a Yeo-Johnson plot:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)\n",
      "        \n",
      "        Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n",
      "        the same plot:\n",
      "        \n",
      "        >>> _, maxlog = stats.yeojohnson(x)\n",
      "        >>> ax.axvline(maxlog, color='r')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    zmap(scores, compare, axis=0, ddof=0)\n",
      "        Calculate the relative z-scores.\n",
      "        \n",
      "        Return an array of z-scores, i.e., scores that are standardized to\n",
      "        zero mean and unit variance, where mean and variance are calculated\n",
      "        from the comparison array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scores : array_like\n",
      "            The input for which z-scores are calculated.\n",
      "        compare : array_like\n",
      "            The input from which the mean and standard deviation of the\n",
      "            normalization are taken; assumed to have the same dimension as\n",
      "            `scores`.\n",
      "        axis : int or None, optional\n",
      "            Axis over which mean and variance of `compare` are calculated.\n",
      "            Default is 0. If None, compute over the whole array `scores`.\n",
      "        ddof : int, optional\n",
      "            Degrees of freedom correction in the calculation of the\n",
      "            standard deviation. Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        zscore : array_like\n",
      "            Z-scores, in the same shape as `scores`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function preserves ndarray subclasses, and works also with\n",
      "        matrices and masked arrays (it uses `asanyarray` instead of\n",
      "        `asarray` for parameters).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import zmap\n",
      "        >>> a = [0.5, 2.0, 2.5, 3]\n",
      "        >>> b = [0, 1, 2, 3, 4]\n",
      "        >>> zmap(a, b)\n",
      "        array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n",
      "    \n",
      "    zscore(a, axis=0, ddof=0, nan_policy='propagate')\n",
      "        Compute the z score.\n",
      "        \n",
      "        Compute the z score of each value in the sample, relative to the\n",
      "        sample mean and standard deviation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            An array like object containing the sample data.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Degrees of freedom correction in the calculation of the\n",
      "            standard deviation. Default is 0.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        zscore : array_like\n",
      "            The z-scores, standardized by mean and standard deviation of\n",
      "            input array `a`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function preserves ndarray subclasses, and works also with\n",
      "        matrices and masked arrays (it uses `asanyarray` instead of\n",
      "        `asarray` for parameters).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n",
      "        ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.zscore(a)\n",
      "        array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n",
      "                0.6748, -1.1488, -1.3324])\n",
      "        \n",
      "        Computing along a specified axis, using n-1 degrees of freedom\n",
      "        (``ddof=1``) to calculate the standard deviation:\n",
      "        \n",
      "        >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n",
      "        ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n",
      "        ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n",
      "        ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n",
      "        ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n",
      "        >>> stats.zscore(b, axis=1, ddof=1)\n",
      "        array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n",
      "               [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n",
      "               [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n",
      "               [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n",
      "               [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n",
      "\n",
      "DATA\n",
      "    __all__ = ['F_onewayBadInputSizesWarning', 'F_onewayConstantInputWarni...\n",
      "    alpha = <scipy.stats._continuous_distns.alpha_gen object>\n",
      "    anglit = <scipy.stats._continuous_distns.anglit_gen object>\n",
      "    arcsine = <scipy.stats._continuous_distns.arcsine_gen object>\n",
      "    argus = <scipy.stats._continuous_distns.argus_gen object>\n",
      "    bernoulli = <scipy.stats._discrete_distns.bernoulli_gen object>\n",
      "    beta = <scipy.stats._continuous_distns.beta_gen object>\n",
      "    betabinom = <scipy.stats._discrete_distns.betabinom_gen object>\n",
      "    betaprime = <scipy.stats._continuous_distns.betaprime_gen object>\n",
      "    binom = <scipy.stats._discrete_distns.binom_gen object>\n",
      "    boltzmann = <scipy.stats._discrete_distns.boltzmann_gen object>\n",
      "    bradford = <scipy.stats._continuous_distns.bradford_gen object>\n",
      "    burr = <scipy.stats._continuous_distns.burr_gen object>\n",
      "    burr12 = <scipy.stats._continuous_distns.burr12_gen object>\n",
      "    cauchy = <scipy.stats._continuous_distns.cauchy_gen object>\n",
      "    chi = <scipy.stats._continuous_distns.chi_gen object>\n",
      "    chi2 = <scipy.stats._continuous_distns.chi2_gen object>\n",
      "    cosine = <scipy.stats._continuous_distns.cosine_gen object>\n",
      "    crystalball = <scipy.stats._continuous_distns.crystalball_gen object>\n",
      "    dgamma = <scipy.stats._continuous_distns.dgamma_gen object>\n",
      "    dirichlet = <scipy.stats._multivariate.dirichlet_gen object>\n",
      "    dlaplace = <scipy.stats._discrete_distns.dlaplace_gen object>\n",
      "    dweibull = <scipy.stats._continuous_distns.dweibull_gen object>\n",
      "    erlang = <scipy.stats._continuous_distns.erlang_gen object>\n",
      "    expon = <scipy.stats._continuous_distns.expon_gen object>\n",
      "    exponnorm = <scipy.stats._continuous_distns.exponnorm_gen object>\n",
      "    exponpow = <scipy.stats._continuous_distns.exponpow_gen object>\n",
      "    exponweib = <scipy.stats._continuous_distns.exponweib_gen object>\n",
      "    f = <scipy.stats._continuous_distns.f_gen object>\n",
      "    fatiguelife = <scipy.stats._continuous_distns.fatiguelife_gen object>\n",
      "    fisk = <scipy.stats._continuous_distns.fisk_gen object>\n",
      "    foldcauchy = <scipy.stats._continuous_distns.foldcauchy_gen object>\n",
      "    foldnorm = <scipy.stats._continuous_distns.foldnorm_gen object>\n",
      "    frechet_l = <scipy.stats._continuous_distns.frechet_l_gen object>\n",
      "    frechet_r = <scipy.stats._continuous_distns.frechet_r_gen object>\n",
      "    gamma = <scipy.stats._continuous_distns.gamma_gen object>\n",
      "    gausshyper = <scipy.stats._continuous_distns.gausshyper_gen object>\n",
      "    genexpon = <scipy.stats._continuous_distns.genexpon_gen object>\n",
      "    genextreme = <scipy.stats._continuous_distns.genextreme_gen object>\n",
      "    gengamma = <scipy.stats._continuous_distns.gengamma_gen object>\n",
      "    genhalflogistic = <scipy.stats._continuous_distns.genhalflogistic_gen ...\n",
      "    geninvgauss = <scipy.stats._continuous_distns.geninvgauss_gen object>\n",
      "    genlogistic = <scipy.stats._continuous_distns.genlogistic_gen object>\n",
      "    gennorm = <scipy.stats._continuous_distns.gennorm_gen object>\n",
      "    genpareto = <scipy.stats._continuous_distns.genpareto_gen object>\n",
      "    geom = <scipy.stats._discrete_distns.geom_gen object>\n",
      "    gilbrat = <scipy.stats._continuous_distns.gilbrat_gen object>\n",
      "    gompertz = <scipy.stats._continuous_distns.gompertz_gen object>\n",
      "    gumbel_l = <scipy.stats._continuous_distns.gumbel_l_gen object>\n",
      "    gumbel_r = <scipy.stats._continuous_distns.gumbel_r_gen object>\n",
      "    halfcauchy = <scipy.stats._continuous_distns.halfcauchy_gen object>\n",
      "    halfgennorm = <scipy.stats._continuous_distns.halfgennorm_gen object>\n",
      "    halflogistic = <scipy.stats._continuous_distns.halflogistic_gen object...\n",
      "    halfnorm = <scipy.stats._continuous_distns.halfnorm_gen object>\n",
      "    hypergeom = <scipy.stats._discrete_distns.hypergeom_gen object>\n",
      "    hypsecant = <scipy.stats._continuous_distns.hypsecant_gen object>\n",
      "    invgamma = <scipy.stats._continuous_distns.invgamma_gen object>\n",
      "    invgauss = <scipy.stats._continuous_distns.invgauss_gen object>\n",
      "    invweibull = <scipy.stats._continuous_distns.invweibull_gen object>\n",
      "    invwishart = <scipy.stats._multivariate.invwishart_gen object>\n",
      "    johnsonsb = <scipy.stats._continuous_distns.johnsonsb_gen object>\n",
      "    johnsonsu = <scipy.stats._continuous_distns.johnsonsu_gen object>\n",
      "    kappa3 = <scipy.stats._continuous_distns.kappa3_gen object>\n",
      "    kappa4 = <scipy.stats._continuous_distns.kappa4_gen object>\n",
      "    ksone = <scipy.stats._continuous_distns.ksone_gen object>\n",
      "    kstwo = <scipy.stats._continuous_distns.kstwo_gen object>\n",
      "    kstwobign = <scipy.stats._continuous_distns.kstwobign_gen object>\n",
      "    laplace = <scipy.stats._continuous_distns.laplace_gen object>\n",
      "    levy = <scipy.stats._continuous_distns.levy_gen object>\n",
      "    levy_l = <scipy.stats._continuous_distns.levy_l_gen object>\n",
      "    levy_stable = <scipy.stats._continuous_distns.levy_stable_gen object>\n",
      "    loggamma = <scipy.stats._continuous_distns.loggamma_gen object>\n",
      "    logistic = <scipy.stats._continuous_distns.logistic_gen object>\n",
      "    loglaplace = <scipy.stats._continuous_distns.loglaplace_gen object>\n",
      "    lognorm = <scipy.stats._continuous_distns.lognorm_gen object>\n",
      "    logser = <scipy.stats._discrete_distns.logser_gen object>\n",
      "    loguniform = <scipy.stats._continuous_distns.reciprocal_gen object>\n",
      "    lomax = <scipy.stats._continuous_distns.lomax_gen object>\n",
      "    matrix_normal = <scipy.stats._multivariate.matrix_normal_gen object>\n",
      "    maxwell = <scipy.stats._continuous_distns.maxwell_gen object>\n",
      "    mielke = <scipy.stats._continuous_distns.mielke_gen object>\n",
      "    moyal = <scipy.stats._continuous_distns.moyal_gen object>\n",
      "    multinomial = <scipy.stats._multivariate.multinomial_gen object>\n",
      "    multivariate_normal = <scipy.stats._multivariate.multivariate_normal_g...\n",
      "    nakagami = <scipy.stats._continuous_distns.nakagami_gen object>\n",
      "    nbinom = <scipy.stats._discrete_distns.nbinom_gen object>\n",
      "    ncf = <scipy.stats._continuous_distns.ncf_gen object>\n",
      "    nct = <scipy.stats._continuous_distns.nct_gen object>\n",
      "    ncx2 = <scipy.stats._continuous_distns.ncx2_gen object>\n",
      "    norm = <scipy.stats._continuous_distns.norm_gen object>\n",
      "    norminvgauss = <scipy.stats._continuous_distns.norminvgauss_gen object...\n",
      "    ortho_group = <scipy.stats._multivariate.ortho_group_gen object>\n",
      "    pareto = <scipy.stats._continuous_distns.pareto_gen object>\n",
      "    pearson3 = <scipy.stats._continuous_distns.pearson3_gen object>\n",
      "    planck = <scipy.stats._discrete_distns.planck_gen object>\n",
      "    poisson = <scipy.stats._discrete_distns.poisson_gen object>\n",
      "    powerlaw = <scipy.stats._continuous_distns.powerlaw_gen object>\n",
      "    powerlognorm = <scipy.stats._continuous_distns.powerlognorm_gen object...\n",
      "    powernorm = <scipy.stats._continuous_distns.powernorm_gen object>\n",
      "    randint = <scipy.stats._discrete_distns.randint_gen object>\n",
      "    random_correlation = <scipy.stats._multivariate.random_correlation_gen...\n",
      "    rayleigh = <scipy.stats._continuous_distns.rayleigh_gen object>\n",
      "    rdist = <scipy.stats._continuous_distns.rdist_gen object>\n",
      "    recipinvgauss = <scipy.stats._continuous_distns.recipinvgauss_gen obje...\n",
      "    reciprocal = <scipy.stats._continuous_distns.reciprocal_gen object>\n",
      "    rice = <scipy.stats._continuous_distns.rice_gen object>\n",
      "    semicircular = <scipy.stats._continuous_distns.semicircular_gen object...\n",
      "    skellam = <scipy.stats._discrete_distns.skellam_gen object>\n",
      "    skewnorm = <scipy.stats._continuous_distns.skew_norm_gen object>\n",
      "    special_ortho_group = <scipy.stats._multivariate.special_ortho_group_g...\n",
      "    t = <scipy.stats._continuous_distns.t_gen object>\n",
      "    trapz = <scipy.stats._continuous_distns.trapz_gen object>\n",
      "    triang = <scipy.stats._continuous_distns.triang_gen object>\n",
      "    truncexpon = <scipy.stats._continuous_distns.truncexpon_gen object>\n",
      "    truncnorm = <scipy.stats._continuous_distns.truncnorm_gen object>\n",
      "    tukeylambda = <scipy.stats._continuous_distns.tukeylambda_gen object>\n",
      "    uniform = <scipy.stats._continuous_distns.uniform_gen object>\n",
      "    unitary_group = <scipy.stats._multivariate.unitary_group_gen object>\n",
      "    vonmises = <scipy.stats._continuous_distns.vonmises_gen object>\n",
      "    vonmises_line = <scipy.stats._continuous_distns.vonmises_gen object>\n",
      "    wald = <scipy.stats._continuous_distns.wald_gen object>\n",
      "    weibull_max = <scipy.stats._continuous_distns.weibull_max_gen object>\n",
      "    weibull_min = <scipy.stats._continuous_distns.weibull_min_gen object>\n",
      "    wishart = <scipy.stats._multivariate.wishart_gen object>\n",
      "    wrapcauchy = <scipy.stats._continuous_distns.wrapcauchy_gen object>\n",
      "    yulesimon = <scipy.stats._discrete_distns.yulesimon_gen object>\n",
      "    zipf = <scipy.stats._discrete_distns.zipf_gen object>\n",
      "\n",
      "FILE\n",
      "    /Users/pavelulegin/anaconda3/envs/data/lib/python3.8/site-packages/scipy/stats/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (30, 10)) #Команда для вывода 3-х окон с графиками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 2160x720 with 3 Axes>,\n",
       " array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fcfc727db50>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfc90b67c0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fcfc9529130>],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(1, 3, figsize = (30, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize = (30, 10)) #Команда для вывода 3-х окон с графиками\n",
    "a = [5, 10, 50]\n",
    "for j in range(3):\n",
    "    ax[j].hist([dist.rvs(a[j]).mean() for i in range(1000)], density=True)\n",
    "    x = np.linspace(1,4,1000)\n",
    "    norm_dist = sts.norm(mean, (var/a[j]) ** (1/2))\n",
    "    pdf = norm_dist.pdf(x)\n",
    "    ax[j].plot(x, pdf, label='norm_dist 50 samples pdf', alpha=1, linewidth=4)\n",
    "    ax[j].legend()\n",
    "    ax[j].set_ylabel('$f(x)$')\n",
    "    ax[j].set_xlabel('$x$')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
