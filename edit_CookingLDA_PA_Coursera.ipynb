{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(5, 1), (9, 1), (15, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)], [(15, 1), (18, 1), (29, 1), (30, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(corpus[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda = models.LdaModel(corpus, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = lda.show_topics(num_topics=40, num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for i in [\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]:\n",
    "    d[i] = [dictionary.token2id[i], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in d.values():\n",
    "    for doc in top:\n",
    "            for word in doc[1]:\n",
    "                if word[0] == str(i[0]):\n",
    "                    i[1] += 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[15, 23], [52, 9], [29, 8], [82, 1], [729, 0], [9, 2]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers1(23, 9, 8, 1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"otvet1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chast = []\n",
    "for i in dictionary2.dfs.keys():\n",
    "    if dictionary2.dfs[i] > 4000:\n",
    "        chast.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2.filter_tokens(chast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sum([len(i) for i in corpus])\n",
    "s2 = sum([len(i) for i in corpus2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers2(len(dictionary), len(dictionary2), s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"otvet2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda2 = models.LdaModel(corpus2, num_topics=40, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cog = lda.top_topics(corpus=corpus, dictionary=dictionary)\n",
    "cog2 = lda2.top_topics(corpus=corpus2, dictionary=dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.290560728421211"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([i[1] for i in cog])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers3(np.mean([i[1] for i in cog]), np.mean([i[1] for i in cog2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"otvet3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 0.12812187), (31, 0.6175879), (33, 0.138662)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.get_document_topics(corpus2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
       "       0.025, 0.025, 0.025, 0.025], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda2.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "lda3 = models.LdaModel(corpus2, num_topics=40, passes=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.02139767),\n",
       " (1, 0.021295449),\n",
       " (2, 0.021276837),\n",
       " (3, 0.021365918),\n",
       " (4, 0.021295369),\n",
       " (5, 0.021311188),\n",
       " (6, 0.02130497),\n",
       " (7, 0.021280425),\n",
       " (8, 0.02140141),\n",
       " (9, 0.021379549),\n",
       " (10, 0.021837927),\n",
       " (11, 0.02149252),\n",
       " (12, 0.021276837),\n",
       " (13, 0.02218951),\n",
       " (14, 0.021718133),\n",
       " (15, 0.021506282),\n",
       " (16, 0.021404231),\n",
       " (17, 0.021964444),\n",
       " (18, 0.021329323),\n",
       " (19, 0.021678474),\n",
       " (20, 0.024654336),\n",
       " (21, 0.021277266),\n",
       " (22, 0.021276837),\n",
       " (23, 0.02128486),\n",
       " (24, 0.021771807),\n",
       " (25, 0.021494577),\n",
       " (26, 0.021462504),\n",
       " (27, 0.021634083),\n",
       " (28, 0.0214952),\n",
       " (29, 0.02130315),\n",
       " (30, 0.042615026),\n",
       " (31, 0.092192926),\n",
       " (32, 0.021500388),\n",
       " (33, 0.021278715),\n",
       " (34, 0.021446656),\n",
       " (35, 0.021365918),\n",
       " (36, 0.021331841),\n",
       " (37, 0.021289436),\n",
       " (38, 0.021277951),\n",
       " (39, 0.06834008)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda3.get_document_topics(corpus2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = sum([len(i) for i in lda2.get_document_topics(corpus2, minimum_probability=0.01)])\n",
    "s2 = sum([len(i) for i in lda3.get_document_topics(corpus2, minimum_probability=0.01)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_answers4(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"otvet4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.DataFrame(np.zeros((len(recipes), 40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(lda2.get_document_topics(corpus2)):\n",
    "    for tem in doc:\n",
    "        data[tem[0]][i] = tem[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = [i['cuisine'] for i in recipes]\n",
    "le = LabelEncoder()\n",
    "le.fit(data.target)\n",
    "data.target = le.transform(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13867</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.113888</td>\n",
       "      <td>0.326322</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224999</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125094</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39769</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.10251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.102517</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.102517</td>\n",
       "      <td>0.102362</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39770</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146398</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39771</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.166385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39772</th>\n",
       "      <td>0.054456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.053917</td>\n",
       "      <td>0.110990</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.056162</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.056238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39773</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248594</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39774 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1        2         3         4       5       6  \\\n",
       "0      0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  0.0000   \n",
       "1      0.000000  0.000000  0.00000  0.113888  0.326322  0.0000  0.0000   \n",
       "2      0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  0.0000   \n",
       "3      0.012500  0.012500  0.01250  0.012500  0.012500  0.0125  0.0125   \n",
       "4      0.000000  0.125094  0.00000  0.000000  0.000000  0.0000  0.0000   \n",
       "...         ...       ...      ...       ...       ...     ...     ...   \n",
       "39769  0.000000  0.000000  0.10251  0.000000  0.000000  0.0000  0.0000   \n",
       "39770  0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  0.0000   \n",
       "39771  0.000000  0.000000  0.00000  0.166385  0.000000  0.0000  0.0000   \n",
       "39772  0.054456  0.000000  0.00000  0.053917  0.110990  0.0000  0.0000   \n",
       "39773  0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  0.0000   \n",
       "\n",
       "            7       8       9  ...        31        32       33      34  \\\n",
       "0      0.0000  0.0000  0.0000  ...  0.617580  0.000000  0.13867  0.0000   \n",
       "1      0.0000  0.0000  0.0000  ...  0.000000  0.000000  0.00000  0.0000   \n",
       "2      0.0000  0.0000  0.0000  ...  0.000000  0.000000  0.00000  0.0000   \n",
       "3      0.0125  0.0125  0.0125  ...  0.012500  0.012500  0.01250  0.0125   \n",
       "4      0.0000  0.0000  0.0000  ...  0.000000  0.065678  0.00000  0.0000   \n",
       "...       ...     ...     ...  ...       ...       ...      ...     ...   \n",
       "39769  0.0000  0.0000  0.0000  ...  0.000000  0.000000  0.00000  0.0000   \n",
       "39770  0.0000  0.0000  0.0000  ...  0.000000  0.146398  0.00000  0.0000   \n",
       "39771  0.0000  0.0000  0.0000  ...  0.000000  0.000000  0.00000  0.0000   \n",
       "39772  0.0000  0.0000  0.0000  ...  0.000000  0.000000  0.00000  0.0000   \n",
       "39773  0.0000  0.0000  0.0000  ...  0.115162  0.000000  0.00000  0.0000   \n",
       "\n",
       "           35        36      37        38        39  target  \n",
       "0      0.0000  0.000000  0.0000  0.000000  0.000000       6  \n",
       "1      0.0000  0.000000  0.0000  0.000000  0.224999      16  \n",
       "2      0.0000  0.000000  0.0000  0.000000  0.000000       4  \n",
       "3      0.0125  0.012500  0.0125  0.012500  0.012500       7  \n",
       "4      0.0000  0.000000  0.0000  0.000000  0.000000       7  \n",
       "...       ...       ...     ...       ...       ...     ...  \n",
       "39769  0.0000  0.102517  0.0000  0.102517  0.102362       8  \n",
       "39770  0.0000  0.000000  0.0000  0.000000  0.000000       9  \n",
       "39771  0.0000  0.000000  0.0000  0.000000  0.000000       8  \n",
       "39772  0.0000  0.056162  0.0000  0.056238  0.000000       3  \n",
       "39773  0.0000  0.000000  0.0000  0.000000  0.248594      13  \n",
       "\n",
       "[39774 rows x 41 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "res = cross_val_score(rf, data.drop('target', axis=1), y=data.target, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers5(res.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"otvet5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-2f9067cea258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-122-1b559997ba91>\u001b[0m in \u001b[0;36mgenerate_recipe\u001b[0;34m(model, num_ingredients)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtopic_distr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopic_distr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "generate_recipe(lda2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>korean</th>\n",
       "      <th>irish</th>\n",
       "      <th>mexican</th>\n",
       "      <th>japanese</th>\n",
       "      <th>cajun_creole</th>\n",
       "      <th>brazilian</th>\n",
       "      <th>southern_us</th>\n",
       "      <th>french</th>\n",
       "      <th>spanish</th>\n",
       "      <th>vietnamese</th>\n",
       "      <th>italian</th>\n",
       "      <th>jamaican</th>\n",
       "      <th>chinese</th>\n",
       "      <th>british</th>\n",
       "      <th>thai</th>\n",
       "      <th>filipino</th>\n",
       "      <th>russian</th>\n",
       "      <th>indian</th>\n",
       "      <th>moroccan</th>\n",
       "      <th>greek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.024609</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.024249</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0.024717</td>\n",
       "      <td>0.024759</td>\n",
       "      <td>0.025194</td>\n",
       "      <td>0.023668</td>\n",
       "      <td>0.023854</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.024368</td>\n",
       "      <td>0.024105</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.023740</td>\n",
       "      <td>0.023088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022216</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>0.023376</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022983</td>\n",
       "      <td>0.023843</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>0.025449</td>\n",
       "      <td>0.024045</td>\n",
       "      <td>0.021182</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.031722</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.028479</td>\n",
       "      <td>0.021311</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>0.026055</td>\n",
       "      <td>0.023296</td>\n",
       "      <td>0.030238</td>\n",
       "      <td>0.024947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.022821</td>\n",
       "      <td>0.023553</td>\n",
       "      <td>0.023270</td>\n",
       "      <td>0.030025</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>0.024650</td>\n",
       "      <td>0.022522</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>0.023847</td>\n",
       "      <td>0.028077</td>\n",
       "      <td>0.022206</td>\n",
       "      <td>0.025756</td>\n",
       "      <td>0.024245</td>\n",
       "      <td>0.023599</td>\n",
       "      <td>0.026733</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.035057</td>\n",
       "      <td>0.023861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022933</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.026207</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>0.023579</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.024148</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.024320</td>\n",
       "      <td>0.022360</td>\n",
       "      <td>0.024276</td>\n",
       "      <td>0.024569</td>\n",
       "      <td>0.022845</td>\n",
       "      <td>0.024160</td>\n",
       "      <td>0.021672</td>\n",
       "      <td>0.024519</td>\n",
       "      <td>0.023221</td>\n",
       "      <td>0.022115</td>\n",
       "      <td>0.023051</td>\n",
       "      <td>0.023564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022253</td>\n",
       "      <td>0.023498</td>\n",
       "      <td>0.026150</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>0.022817</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>0.022541</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.021341</td>\n",
       "      <td>0.028951</td>\n",
       "      <td>0.025804</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.023751</td>\n",
       "      <td>0.021026</td>\n",
       "      <td>0.024472</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.024039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.023791</td>\n",
       "      <td>0.030496</td>\n",
       "      <td>0.023268</td>\n",
       "      <td>0.024887</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.030760</td>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.025432</td>\n",
       "      <td>0.022838</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.035180</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>0.029008</td>\n",
       "      <td>0.030781</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022299</td>\n",
       "      <td>0.027946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.026517</td>\n",
       "      <td>0.023054</td>\n",
       "      <td>0.022811</td>\n",
       "      <td>0.025005</td>\n",
       "      <td>0.029439</td>\n",
       "      <td>0.031581</td>\n",
       "      <td>0.028961</td>\n",
       "      <td>0.027889</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>0.025137</td>\n",
       "      <td>0.024817</td>\n",
       "      <td>0.022211</td>\n",
       "      <td>0.028790</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>0.027578</td>\n",
       "      <td>0.021801</td>\n",
       "      <td>0.023525</td>\n",
       "      <td>0.024392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>0.022026</td>\n",
       "      <td>0.022938</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>0.023076</td>\n",
       "      <td>0.021590</td>\n",
       "      <td>0.034317</td>\n",
       "      <td>0.022427</td>\n",
       "      <td>0.021338</td>\n",
       "      <td>0.023287</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.022993</td>\n",
       "      <td>0.020961</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.025265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.022878</td>\n",
       "      <td>0.023130</td>\n",
       "      <td>0.029855</td>\n",
       "      <td>0.023081</td>\n",
       "      <td>0.025235</td>\n",
       "      <td>0.024799</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.022867</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>0.023498</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.024248</td>\n",
       "      <td>0.023669</td>\n",
       "      <td>0.023383</td>\n",
       "      <td>0.023335</td>\n",
       "      <td>0.025752</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>0.024848</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.024641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.028265</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>0.022683</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>0.024605</td>\n",
       "      <td>0.025639</td>\n",
       "      <td>0.024964</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.023732</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.023930</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>0.023733</td>\n",
       "      <td>0.027430</td>\n",
       "      <td>0.024123</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.024491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.021666</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>0.026137</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>0.024305</td>\n",
       "      <td>0.024168</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>0.021143</td>\n",
       "      <td>0.025479</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.021139</td>\n",
       "      <td>0.023918</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.022799</td>\n",
       "      <td>0.023540</td>\n",
       "      <td>0.023701</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.025622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.025991</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.023873</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>0.024368</td>\n",
       "      <td>0.024882</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.023252</td>\n",
       "      <td>0.024819</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>0.025139</td>\n",
       "      <td>0.023524</td>\n",
       "      <td>0.023032</td>\n",
       "      <td>0.023898</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>0.023705</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.023061</td>\n",
       "      <td>0.022512</td>\n",
       "      <td>0.024859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.022263</td>\n",
       "      <td>0.037573</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>0.026537</td>\n",
       "      <td>0.028694</td>\n",
       "      <td>0.037854</td>\n",
       "      <td>0.041013</td>\n",
       "      <td>0.026354</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.022896</td>\n",
       "      <td>0.042807</td>\n",
       "      <td>0.021158</td>\n",
       "      <td>0.026304</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>0.023253</td>\n",
       "      <td>0.023319</td>\n",
       "      <td>0.024910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.022269</td>\n",
       "      <td>0.023685</td>\n",
       "      <td>0.025740</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.023753</td>\n",
       "      <td>0.024528</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>0.024753</td>\n",
       "      <td>0.025115</td>\n",
       "      <td>0.022849</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.025628</td>\n",
       "      <td>0.022660</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>0.023424</td>\n",
       "      <td>0.023558</td>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.024232</td>\n",
       "      <td>0.024095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.024112</td>\n",
       "      <td>0.022340</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.024680</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>0.024838</td>\n",
       "      <td>0.023218</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.023855</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>0.022422</td>\n",
       "      <td>0.026758</td>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.022574</td>\n",
       "      <td>0.032441</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>0.022140</td>\n",
       "      <td>0.029237</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>0.023930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.029943</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.023174</td>\n",
       "      <td>0.024937</td>\n",
       "      <td>0.023206</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>0.024260</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.024619</td>\n",
       "      <td>0.026158</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.024707</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.022975</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>0.024430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.023002</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.022071</td>\n",
       "      <td>0.026546</td>\n",
       "      <td>0.023573</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>0.023199</td>\n",
       "      <td>0.024153</td>\n",
       "      <td>0.029095</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.024527</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.022730</td>\n",
       "      <td>0.025182</td>\n",
       "      <td>0.025579</td>\n",
       "      <td>0.026880</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>0.026473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.023168</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.025361</td>\n",
       "      <td>0.023109</td>\n",
       "      <td>0.026369</td>\n",
       "      <td>0.023524</td>\n",
       "      <td>0.024060</td>\n",
       "      <td>0.023309</td>\n",
       "      <td>0.024827</td>\n",
       "      <td>0.023619</td>\n",
       "      <td>0.024069</td>\n",
       "      <td>0.024184</td>\n",
       "      <td>0.023248</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.023373</td>\n",
       "      <td>0.023049</td>\n",
       "      <td>0.023156</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.024132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.021855</td>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.023246</td>\n",
       "      <td>0.022588</td>\n",
       "      <td>0.045932</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.026346</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>0.026254</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>0.023869</td>\n",
       "      <td>0.027863</td>\n",
       "      <td>0.022640</td>\n",
       "      <td>0.023937</td>\n",
       "      <td>0.022418</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.024149</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.023995</td>\n",
       "      <td>0.023934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.023864</td>\n",
       "      <td>0.023251</td>\n",
       "      <td>0.022868</td>\n",
       "      <td>0.023626</td>\n",
       "      <td>0.023183</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.022680</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>0.022725</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.022063</td>\n",
       "      <td>0.025020</td>\n",
       "      <td>0.027955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.023163</td>\n",
       "      <td>0.028116</td>\n",
       "      <td>0.023791</td>\n",
       "      <td>0.024522</td>\n",
       "      <td>0.022752</td>\n",
       "      <td>0.025722</td>\n",
       "      <td>0.023759</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.025763</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>0.026663</td>\n",
       "      <td>0.022390</td>\n",
       "      <td>0.025612</td>\n",
       "      <td>0.022618</td>\n",
       "      <td>0.025109</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.024747</td>\n",
       "      <td>0.024868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.042565</td>\n",
       "      <td>0.026535</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>0.037403</td>\n",
       "      <td>0.022693</td>\n",
       "      <td>0.023248</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>0.032491</td>\n",
       "      <td>0.022906</td>\n",
       "      <td>0.024273</td>\n",
       "      <td>0.036544</td>\n",
       "      <td>0.023501</td>\n",
       "      <td>0.029129</td>\n",
       "      <td>0.028985</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>0.023235</td>\n",
       "      <td>0.024314</td>\n",
       "      <td>0.022472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.055228</td>\n",
       "      <td>0.022646</td>\n",
       "      <td>0.022836</td>\n",
       "      <td>0.038392</td>\n",
       "      <td>0.022209</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.023032</td>\n",
       "      <td>0.022052</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>0.032662</td>\n",
       "      <td>0.022455</td>\n",
       "      <td>0.027017</td>\n",
       "      <td>0.054074</td>\n",
       "      <td>0.023072</td>\n",
       "      <td>0.031102</td>\n",
       "      <td>0.036877</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.021993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.031655</td>\n",
       "      <td>0.023946</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>0.027385</td>\n",
       "      <td>0.022060</td>\n",
       "      <td>0.023718</td>\n",
       "      <td>0.023367</td>\n",
       "      <td>0.023172</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>0.029632</td>\n",
       "      <td>0.022476</td>\n",
       "      <td>0.026839</td>\n",
       "      <td>0.040687</td>\n",
       "      <td>0.024848</td>\n",
       "      <td>0.030464</td>\n",
       "      <td>0.027483</td>\n",
       "      <td>0.024450</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.023690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>0.038671</td>\n",
       "      <td>0.023541</td>\n",
       "      <td>0.021640</td>\n",
       "      <td>0.031816</td>\n",
       "      <td>0.022905</td>\n",
       "      <td>0.021955</td>\n",
       "      <td>0.023649</td>\n",
       "      <td>0.038111</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>0.022681</td>\n",
       "      <td>0.022123</td>\n",
       "      <td>0.036624</td>\n",
       "      <td>0.023634</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.022380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.022963</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>0.023479</td>\n",
       "      <td>0.029019</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.025061</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.024049</td>\n",
       "      <td>0.023093</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.024626</td>\n",
       "      <td>0.024029</td>\n",
       "      <td>0.024522</td>\n",
       "      <td>0.023641</td>\n",
       "      <td>0.024349</td>\n",
       "      <td>0.023314</td>\n",
       "      <td>0.021838</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.022540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.022031</td>\n",
       "      <td>0.023959</td>\n",
       "      <td>0.027927</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>0.026133</td>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.023310</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.021508</td>\n",
       "      <td>0.023108</td>\n",
       "      <td>0.024211</td>\n",
       "      <td>0.021747</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.022611</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.024134</td>\n",
       "      <td>0.031818</td>\n",
       "      <td>0.035030</td>\n",
       "      <td>0.024959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.027270</td>\n",
       "      <td>0.022622</td>\n",
       "      <td>0.022753</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.027761</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>0.029587</td>\n",
       "      <td>0.030361</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>0.021598</td>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.026485</td>\n",
       "      <td>0.021651</td>\n",
       "      <td>0.022215</td>\n",
       "      <td>0.026691</td>\n",
       "      <td>0.023626</td>\n",
       "      <td>0.027850</td>\n",
       "      <td>0.033548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.023737</td>\n",
       "      <td>0.024320</td>\n",
       "      <td>0.022523</td>\n",
       "      <td>0.025708</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.023344</td>\n",
       "      <td>0.024174</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.022289</td>\n",
       "      <td>0.022935</td>\n",
       "      <td>0.022640</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.023342</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.024434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.022858</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>0.026118</td>\n",
       "      <td>0.025164</td>\n",
       "      <td>0.024247</td>\n",
       "      <td>0.028685</td>\n",
       "      <td>0.026898</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>0.021749</td>\n",
       "      <td>0.023524</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>0.025285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.021842</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>0.053256</td>\n",
       "      <td>0.021849</td>\n",
       "      <td>0.022131</td>\n",
       "      <td>0.024804</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.022699</td>\n",
       "      <td>0.020974</td>\n",
       "      <td>0.022535</td>\n",
       "      <td>0.022787</td>\n",
       "      <td>0.021138</td>\n",
       "      <td>0.024040</td>\n",
       "      <td>0.021070</td>\n",
       "      <td>0.022426</td>\n",
       "      <td>0.027421</td>\n",
       "      <td>0.021912</td>\n",
       "      <td>0.021980</td>\n",
       "      <td>0.022993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.021602</td>\n",
       "      <td>0.023676</td>\n",
       "      <td>0.023122</td>\n",
       "      <td>0.022187</td>\n",
       "      <td>0.023599</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>0.024188</td>\n",
       "      <td>0.025692</td>\n",
       "      <td>0.025896</td>\n",
       "      <td>0.021487</td>\n",
       "      <td>0.027335</td>\n",
       "      <td>0.022255</td>\n",
       "      <td>0.021452</td>\n",
       "      <td>0.023360</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.024278</td>\n",
       "      <td>0.021886</td>\n",
       "      <td>0.024366</td>\n",
       "      <td>0.027730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.022103</td>\n",
       "      <td>0.024123</td>\n",
       "      <td>0.023016</td>\n",
       "      <td>0.022084</td>\n",
       "      <td>0.027896</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.023567</td>\n",
       "      <td>0.024685</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>0.021184</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.022045</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>0.024174</td>\n",
       "      <td>0.021234</td>\n",
       "      <td>0.023281</td>\n",
       "      <td>0.023646</td>\n",
       "      <td>0.021832</td>\n",
       "      <td>0.022896</td>\n",
       "      <td>0.025201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.031658</td>\n",
       "      <td>0.023346</td>\n",
       "      <td>0.022188</td>\n",
       "      <td>0.044987</td>\n",
       "      <td>0.022607</td>\n",
       "      <td>0.023257</td>\n",
       "      <td>0.023034</td>\n",
       "      <td>0.022906</td>\n",
       "      <td>0.022172</td>\n",
       "      <td>0.028099</td>\n",
       "      <td>0.022603</td>\n",
       "      <td>0.025286</td>\n",
       "      <td>0.043553</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>0.026297</td>\n",
       "      <td>0.029652</td>\n",
       "      <td>0.023246</td>\n",
       "      <td>0.023355</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.022137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.024295</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.022766</td>\n",
       "      <td>0.024102</td>\n",
       "      <td>0.024264</td>\n",
       "      <td>0.024240</td>\n",
       "      <td>0.029846</td>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.028175</td>\n",
       "      <td>0.024105</td>\n",
       "      <td>0.027802</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.021844</td>\n",
       "      <td>0.023611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.026128</td>\n",
       "      <td>0.022130</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.024867</td>\n",
       "      <td>0.024393</td>\n",
       "      <td>0.023405</td>\n",
       "      <td>0.029426</td>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>0.031320</td>\n",
       "      <td>0.025640</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.023446</td>\n",
       "      <td>0.024640</td>\n",
       "      <td>0.022058</td>\n",
       "      <td>0.025375</td>\n",
       "      <td>0.026634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.025863</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>0.026086</td>\n",
       "      <td>0.023591</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.026462</td>\n",
       "      <td>0.022470</td>\n",
       "      <td>0.028023</td>\n",
       "      <td>0.023017</td>\n",
       "      <td>0.022268</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.022397</td>\n",
       "      <td>0.022512</td>\n",
       "      <td>0.022874</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.023137</td>\n",
       "      <td>0.025723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.025814</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.023897</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.032050</td>\n",
       "      <td>0.027175</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.024269</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>0.025523</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>0.024746</td>\n",
       "      <td>0.025618</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.024278</td>\n",
       "      <td>0.026736</td>\n",
       "      <td>0.025823</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.022936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.026985</td>\n",
       "      <td>0.022713</td>\n",
       "      <td>0.021839</td>\n",
       "      <td>0.029658</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.026653</td>\n",
       "      <td>0.022754</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.023260</td>\n",
       "      <td>0.055310</td>\n",
       "      <td>0.022896</td>\n",
       "      <td>0.025125</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.023104</td>\n",
       "      <td>0.068009</td>\n",
       "      <td>0.030839</td>\n",
       "      <td>0.022205</td>\n",
       "      <td>0.025230</td>\n",
       "      <td>0.022609</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.023754</td>\n",
       "      <td>0.023508</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>0.023937</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.025297</td>\n",
       "      <td>0.026034</td>\n",
       "      <td>0.023881</td>\n",
       "      <td>0.026517</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.023797</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>0.023334</td>\n",
       "      <td>0.022716</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>0.022796</td>\n",
       "      <td>0.036732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      korean     irish   mexican  japanese  cajun_creole  brazilian  \\\n",
       "0   0.024601  0.023043  0.024609  0.023854      0.023460   0.024249   \n",
       "1   0.022216  0.027178  0.023376  0.022446      0.022983   0.023843   \n",
       "2   0.022821  0.023553  0.023270  0.030025      0.021478   0.024650   \n",
       "3   0.022933  0.023940  0.026207  0.022677      0.023579   0.025350   \n",
       "4   0.022253  0.023498  0.026150  0.022358      0.026005   0.022817   \n",
       "5   0.023791  0.030496  0.023268  0.024887      0.023956   0.028569   \n",
       "6   0.021667  0.026517  0.023054  0.022811      0.025005   0.029439   \n",
       "7   0.021492  0.024453  0.022543  0.022026      0.022938   0.023712   \n",
       "8   0.022878  0.023130  0.029855  0.023081      0.025235   0.024799   \n",
       "9   0.028265  0.024711  0.022683  0.024000      0.022539   0.024605   \n",
       "10  0.021666  0.024004  0.024023  0.022175      0.026137   0.023878   \n",
       "11  0.025991  0.024348  0.023873  0.022351      0.024368   0.024882   \n",
       "12  0.022263  0.037573  0.023598  0.025141      0.026537   0.028694   \n",
       "13  0.022269  0.023685  0.025740  0.022771      0.023753   0.024528   \n",
       "14  0.024112  0.022340  0.027809  0.024680      0.022240   0.024838   \n",
       "15  0.029943  0.024357  0.023174  0.024937      0.023206   0.024363   \n",
       "16  0.023002  0.023782  0.022071  0.026546      0.023573   0.024283   \n",
       "17  0.023168  0.023716  0.025361  0.023109      0.026369   0.023524   \n",
       "18  0.021855  0.025668  0.023246  0.022588      0.045932   0.026144   \n",
       "19  0.023299  0.023864  0.023251  0.022868      0.023626   0.023183   \n",
       "20  0.023163  0.028116  0.023791  0.024522      0.022752   0.025722   \n",
       "21  0.042565  0.026535  0.022224  0.037403      0.022693   0.023248   \n",
       "22  0.055228  0.022646  0.022836  0.038392      0.022209   0.022354   \n",
       "23  0.031655  0.023946  0.022325  0.027385      0.022060   0.023718   \n",
       "24  0.022785  0.022271  0.038671  0.023541      0.021640   0.031816   \n",
       "25  0.022963  0.023949  0.023470  0.023479      0.029019   0.024418   \n",
       "26  0.022031  0.023959  0.027927  0.022869      0.029273   0.026133   \n",
       "27  0.021468  0.027270  0.022622  0.022753      0.022594   0.027761   \n",
       "28  0.022464  0.023737  0.024320  0.022523      0.025708   0.024348   \n",
       "29  0.023264  0.026365  0.022858  0.022733      0.026118   0.025164   \n",
       "30  0.021842  0.023431  0.053256  0.021849      0.022131   0.024804   \n",
       "31  0.021602  0.023676  0.023122  0.022187      0.023599   0.022657   \n",
       "32  0.022103  0.024123  0.023016  0.022084      0.027896   0.022900   \n",
       "33  0.031658  0.023346  0.022188  0.044987      0.022607   0.023257   \n",
       "34  0.024295  0.027711  0.022766  0.024102      0.024264   0.024240   \n",
       "35  0.021989  0.026128  0.022130  0.022161      0.024867   0.024393   \n",
       "36  0.021888  0.025863  0.025177  0.022705      0.026086   0.023591   \n",
       "37  0.025814  0.026851  0.023897  0.025399      0.032050   0.027175   \n",
       "38  0.026985  0.022713  0.021839  0.029658      0.021212   0.026653   \n",
       "39  0.023754  0.023508  0.024407  0.023937      0.028302   0.025297   \n",
       "\n",
       "    southern_us    french   spanish  vietnamese   italian  jamaican   chinese  \\\n",
       "0      0.023695  0.024717  0.024759    0.025194  0.023668  0.023854  0.024426   \n",
       "1      0.027162  0.025449  0.024045    0.021182  0.023954  0.031722  0.021828   \n",
       "2      0.022522  0.023028  0.023847    0.028077  0.022206  0.025756  0.024245   \n",
       "3      0.024148  0.023716  0.024320    0.022360  0.024276  0.024569  0.022845   \n",
       "4      0.025818  0.022541  0.022491    0.021341  0.028951  0.025804  0.022161   \n",
       "5      0.030760  0.026756  0.025432    0.022838  0.025275  0.024932  0.024254   \n",
       "6      0.031581  0.028961  0.027889    0.021943  0.025137  0.024817  0.022211   \n",
       "7      0.024053  0.024703  0.023076    0.021590  0.034317  0.022427  0.021338   \n",
       "8      0.024176  0.022867  0.023729    0.023498  0.023623  0.024248  0.023669   \n",
       "9      0.025639  0.024964  0.023746    0.023611  0.023732  0.024902  0.023930   \n",
       "10     0.024305  0.024168  0.027029    0.021143  0.025479  0.022947  0.021139   \n",
       "11     0.024326  0.023252  0.024819    0.023328  0.025139  0.023524  0.023032   \n",
       "12     0.037854  0.041013  0.026354    0.021782  0.028571  0.025836  0.022896   \n",
       "13     0.023814  0.024753  0.025115    0.022849  0.024088  0.025628  0.022660   \n",
       "14     0.023218  0.022750  0.023855    0.029392  0.022422  0.026758  0.024412   \n",
       "15     0.024315  0.024260  0.024361    0.026255  0.024038  0.024619  0.026158   \n",
       "16     0.023199  0.024153  0.029095    0.022093  0.024527  0.023408  0.022504   \n",
       "17     0.024060  0.023309  0.024827    0.023619  0.024069  0.024184  0.023248   \n",
       "18     0.026346  0.025921  0.026254    0.022837  0.023869  0.027863  0.022640   \n",
       "19     0.023154  0.025759  0.025519    0.022680  0.029614  0.022367  0.022873   \n",
       "20     0.023759  0.025012  0.025763    0.022880  0.023901  0.026663  0.022390   \n",
       "21     0.023305  0.023280  0.022532    0.032491  0.022906  0.024273  0.036544   \n",
       "22     0.023032  0.022052  0.022153    0.032662  0.022455  0.027017  0.054074   \n",
       "23     0.023367  0.023172  0.022692    0.029632  0.022476  0.026839  0.040687   \n",
       "24     0.022905  0.021955  0.023649    0.038111  0.021740  0.027029  0.022681   \n",
       "25     0.025061  0.023900  0.024049    0.023093  0.024058  0.024626  0.024029   \n",
       "26     0.023953  0.023310  0.025561    0.021508  0.023108  0.024211  0.021747   \n",
       "27     0.024316  0.029587  0.030361    0.021472  0.027981  0.021598  0.021283   \n",
       "28     0.023300  0.023914  0.025393    0.023344  0.024174  0.029730  0.022289   \n",
       "29     0.024247  0.028685  0.026898    0.022600  0.027765  0.024974  0.022003   \n",
       "30     0.023730  0.022486  0.022699    0.020974  0.022535  0.022787  0.021138   \n",
       "31     0.024188  0.025692  0.025896    0.021487  0.027335  0.022255  0.021452   \n",
       "32     0.023567  0.024685  0.025870    0.021184  0.026708  0.022045  0.021375   \n",
       "33     0.023034  0.022906  0.022172    0.028099  0.022603  0.025286  0.043553   \n",
       "34     0.029846  0.024867  0.023657    0.022547  0.023484  0.028175  0.024105   \n",
       "35     0.023405  0.029426  0.029057    0.022022  0.031320  0.025640  0.021700   \n",
       "36     0.025797  0.025258  0.026462    0.022470  0.028023  0.023017  0.022268   \n",
       "37     0.026255  0.024269  0.024797    0.025523  0.023866  0.024746  0.025618   \n",
       "38     0.022754  0.024623  0.023260    0.055310  0.022896  0.025125  0.025737   \n",
       "39     0.026034  0.023881  0.026517    0.024974  0.023712  0.023797  0.022857   \n",
       "\n",
       "     british      thai  filipino   russian    indian  moroccan     greek  \n",
       "0   0.023590  0.024368  0.024105  0.023464  0.023389  0.023740  0.023088  \n",
       "1   0.028479  0.021311  0.023466  0.026055  0.023296  0.030238  0.024947  \n",
       "2   0.023599  0.026733  0.024803  0.023485  0.081872  0.035057  0.023861  \n",
       "3   0.024160  0.021672  0.024519  0.023221  0.022115  0.023051  0.023564  \n",
       "4   0.023751  0.021026  0.024472  0.023352  0.021682  0.021485  0.024039  \n",
       "5   0.035180  0.022232  0.029008  0.030781  0.022727  0.022299  0.027946  \n",
       "6   0.028790  0.021684  0.023401  0.027578  0.021801  0.023525  0.024392  \n",
       "7   0.023287  0.022072  0.023187  0.022993  0.020961  0.022040  0.025265  \n",
       "8   0.023383  0.023335  0.025752  0.023684  0.024848  0.023436  0.024641  \n",
       "9   0.026503  0.023415  0.023733  0.027430  0.024123  0.028490  0.024491  \n",
       "10  0.023918  0.021164  0.022799  0.023540  0.023701  0.035499  0.025622  \n",
       "11  0.023898  0.023043  0.023705  0.023506  0.023061  0.022512  0.024859  \n",
       "12  0.042807  0.021158  0.026304  0.037711  0.023253  0.023319  0.024910  \n",
       "13  0.023762  0.023302  0.023424  0.023558  0.023472  0.024232  0.024095  \n",
       "14  0.022574  0.032441  0.022880  0.022140  0.029237  0.026515  0.023930  \n",
       "15  0.023851  0.024707  0.025204  0.023710  0.022975  0.023176  0.024430  \n",
       "16  0.026245  0.022730  0.025182  0.025579  0.026880  0.031391  0.026473  \n",
       "17  0.022689  0.023671  0.023373  0.023049  0.023156  0.023903  0.024132  \n",
       "18  0.023937  0.022418  0.027497  0.024149  0.024715  0.023995  0.023934  \n",
       "19  0.022725  0.024004  0.023954  0.023808  0.022063  0.025020  0.027955  \n",
       "20  0.025612  0.022618  0.025109  0.029164  0.026303  0.024747  0.024868  \n",
       "21  0.023501  0.029129  0.028985  0.025955  0.023235  0.024314  0.022472  \n",
       "22  0.023072  0.031102  0.036877  0.023196  0.021398  0.020924  0.021993  \n",
       "23  0.024848  0.030464  0.027483  0.024450  0.026116  0.028300  0.023690  \n",
       "24  0.022123  0.036624  0.023634  0.022539  0.025679  0.025359  0.022380  \n",
       "25  0.024522  0.023641  0.024349  0.023314  0.021838  0.022551  0.022540  \n",
       "26  0.023192  0.022611  0.023153  0.024134  0.031818  0.035030  0.024959  \n",
       "27  0.026485  0.021651  0.022215  0.026691  0.023626  0.027850  0.033548  \n",
       "28  0.022935  0.022640  0.022918  0.023139  0.023342  0.024100  0.024434  \n",
       "29  0.025251  0.021749  0.023524  0.026367  0.021774  0.024103  0.025285  \n",
       "30  0.024040  0.021070  0.022426  0.027421  0.021912  0.021980  0.022993  \n",
       "31  0.023360  0.021785  0.022599  0.024278  0.021886  0.024366  0.027730  \n",
       "32  0.024174  0.021234  0.023281  0.023646  0.021832  0.022896  0.025201  \n",
       "33  0.023239  0.026297  0.029652  0.023246  0.023355  0.022008  0.022137  \n",
       "34  0.027802  0.022578  0.024846  0.027275  0.022529  0.021844  0.023611  \n",
       "35  0.025307  0.020920  0.023446  0.024640  0.022058  0.025375  0.026634  \n",
       "36  0.022594  0.022397  0.022512  0.022874  0.022356  0.023137  0.025723  \n",
       "37  0.024375  0.024278  0.026736  0.025823  0.021928  0.022786  0.022936  \n",
       "38  0.023104  0.068009  0.030839  0.022205  0.025230  0.022609  0.023563  \n",
       "39  0.023334  0.022716  0.024647  0.026851  0.022460  0.022796  0.036732  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_topic_cuisine_matrix(lda3, corpus2, recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matrix(compute_topic_cuisine_matrix(lda3, corpus2, recipes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
